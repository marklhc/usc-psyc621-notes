<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Model Comparison and Regularization | Course Handouts for Bayesian Data Analysis Class</title>
  <meta name="description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Fall semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Model Comparison and Regularization | Course Handouts for Bayesian Data Analysis Class" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Fall semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Model Comparison and Regularization | Course Handouts for Bayesian Data Analysis Class" />
  
  <meta name="twitter:description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Fall semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  

<meta name="author" content="Mark Lai" />


<meta name="date" content="2020-06-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="model-diagnostics.html"/>
<link rel="next" href="hierarchical-multilevel-models.html"/>
<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PSYC 621 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#history-of-bayesian-statistics"><i class="fa fa-check"></i><b>1.1</b> History of Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#thomas-bayes-17011762"><i class="fa fa-check"></i><b>1.1.1</b> Thomas Bayes (1701–1762)</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#pierre-simon-laplace-17491827"><i class="fa fa-check"></i><b>1.1.2</b> Pierre-Simon Laplace (1749–1827)</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction.html"><a href="introduction.html#th-century"><i class="fa fa-check"></i><b>1.1.3</b> 20th Century</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#motivations-for-using-bayesian-methods"><i class="fa fa-check"></i><b>1.2</b> Motivations for Using Bayesian Methods</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#problem-with-classical-frequentist-statistics"><i class="fa fa-check"></i><b>1.2.1</b> Problem with classical (frequentist) statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#probability"><i class="fa fa-check"></i><b>1.3</b> Probability</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#classical-interpretation"><i class="fa fa-check"></i><b>1.3.1</b> Classical Interpretation</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#frequentist-interpretation"><i class="fa fa-check"></i><b>1.3.2</b> Frequentist Interpretation</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#problem-of-the-single-case"><i class="fa fa-check"></i><b>1.3.3</b> Problem of the single case</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#subjectivist-interpretation"><i class="fa fa-check"></i><b>1.3.4</b> Subjectivist Interpretation</a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#basics-of-probability"><i class="fa fa-check"></i><b>1.3.5</b> Basics of Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#bayess-theorem"><i class="fa fa-check"></i><b>1.4</b> Bayes’s Theorem</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#example-1-base-rate-fallacy-from-wikipedia"><i class="fa fa-check"></i><b>1.4.1</b> Example 1: Base rate fallacy (From Wikipedia)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#bayesian-statistics"><i class="fa fa-check"></i><b>1.5</b> Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#example-2-locating-a-plane"><i class="fa fa-check"></i><b>1.5.1</b> Example 2: Locating a Plane</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#comparing-bayesian-and-frequentist-statistics"><i class="fa fa-check"></i><b>1.6</b> Comparing Bayesian and Frequentist Statistics</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#software-for-bayesian-statistics"><i class="fa fa-check"></i><b>1.7</b> Software for Bayesian Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>2</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Steps of Bayesian Data Analysis</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#real-data-example"><i class="fa fa-check"></i><b>2.2</b> Real Data Example</a></li>
<li class="chapter" data-level="2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#choosing-a-model"><i class="fa fa-check"></i><b>2.3</b> Choosing a Model</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#exchangeability"><i class="fa fa-check"></i><b>2.3.1</b> Exchangeability*</a></li>
<li class="chapter" data-level="2.3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#probability-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="2.3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-likelihood"><i class="fa fa-check"></i><b>2.3.3</b> The Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#specifying-priors"><i class="fa fa-check"></i><b>2.4</b> Specifying Priors</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#beta-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#obtain-the-posterior-distributions"><i class="fa fa-check"></i><b>2.5</b> Obtain the Posterior Distributions</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#grid-approximation"><i class="fa fa-check"></i><b>2.5.1</b> Grid Approximation</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#using-conjugate-priors"><i class="fa fa-check"></i><b>2.5.2</b> Using Conjugate Priors</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#laplace-approximation-with-maximum-a-posteriori-estimation"><i class="fa fa-check"></i><b>2.5.3</b> Laplace Approximation with Maximum A Posteriori Estimation</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#markov-chain-monte-carlo-mcmc"><i class="fa fa-check"></i><b>2.5.4</b> Markov Chain Monte Carlo (MCMC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summarizing-the-posterior-distribution"><i class="fa fa-check"></i><b>2.6</b> Summarizing the Posterior Distribution</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-mean-median-and-mode"><i class="fa fa-check"></i><b>2.6.1</b> Posterior Mean, Median, and Mode</a></li>
<li class="chapter" data-level="2.6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#uncertainty-estimates"><i class="fa fa-check"></i><b>2.6.2</b> Uncertainty Estimates</a></li>
<li class="chapter" data-level="2.6.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>2.6.3</b> Credible Intervals</a></li>
<li class="chapter" data-level="2.6.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#probability-of-theta-higherlower-than-a-certain-value"><i class="fa fa-check"></i><b>2.6.4</b> Probability of <span class="math inline">\(\theta\)</span> Higher/Lower Than a Certain Value</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#model-checking"><i class="fa fa-check"></i><b>2.7</b> Model Checking</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>2.7.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.8</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="2.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summary"><i class="fa fa-check"></i><b>2.9</b> Summary</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#key-concepts"><i class="fa fa-check"></i><b>2.9.1</b> Key Concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="one-parameter-models.html"><a href="one-parameter-models.html"><i class="fa fa-check"></i><b>3</b> One-Parameter Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#binomialbernoulli-data"><i class="fa fa-check"></i><b>3.1</b> Binomial/Bernoulli data</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#reparameterization"><i class="fa fa-check"></i><b>3.1.1</b> Reparameterization*</a></li>
<li class="chapter" data-level="3.1.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#posterior-predictive-check-1"><i class="fa fa-check"></i><b>3.1.2</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="3.1.3" data-path="one-parameter-models.html"><a href="one-parameter-models.html#comparison-to-frequentist-results"><i class="fa fa-check"></i><b>3.1.3</b> Comparison to frequentist results</a></li>
<li class="chapter" data-level="3.1.4" data-path="one-parameter-models.html"><a href="one-parameter-models.html#sensitivity-to-different-priors"><i class="fa fa-check"></i><b>3.1.4</b> Sensitivity to different priors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#poisson-data"><i class="fa fa-check"></i><b>3.2</b> Poisson Data</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#example-2"><i class="fa fa-check"></i><b>3.2.1</b> Example 2</a></li>
<li class="chapter" data-level="3.2.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#choosing-a-model-1"><i class="fa fa-check"></i><b>3.2.2</b> Choosing a model</a></li>
<li class="chapter" data-level="3.2.3" data-path="one-parameter-models.html"><a href="one-parameter-models.html#choosing-a-prior"><i class="fa fa-check"></i><b>3.2.3</b> Choosing a prior</a></li>
<li class="chapter" data-level="3.2.4" data-path="one-parameter-models.html"><a href="one-parameter-models.html#model-equations-and-diagram"><i class="fa fa-check"></i><b>3.2.4</b> Model Equations and Diagram</a></li>
<li class="chapter" data-level="3.2.5" data-path="one-parameter-models.html"><a href="one-parameter-models.html#getting-the-posterior"><i class="fa fa-check"></i><b>3.2.5</b> Getting the posterior</a></li>
<li class="chapter" data-level="3.2.6" data-path="one-parameter-models.html"><a href="one-parameter-models.html#posterior-predictive-check-2"><i class="fa fa-check"></i><b>3.2.6</b> Posterior Predictive Check</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html"><i class="fa fa-check"></i><b>4</b> Brief Introduction to STAN</a>
<ul>
<li class="chapter" data-level="4.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#stan"><i class="fa fa-check"></i><b>4.1</b> <code>STAN</code></a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#stan-code"><i class="fa fa-check"></i><b>4.1.1</b> <code>STAN</code> code</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#rstan"><i class="fa fa-check"></i><b>4.2</b> <code>RStan</code></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#assembling-data-list-in-r"><i class="fa fa-check"></i><b>4.2.1</b> Assembling data list in R</a></li>
<li class="chapter" data-level="4.2.2" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#call-rstan"><i class="fa fa-check"></i><b>4.2.2</b> Call <code>rstan</code></a></li>
<li class="chapter" data-level="4.2.3" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#summarize-the-results"><i class="fa fa-check"></i><b>4.2.3</b> Summarize the results</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#resources"><i class="fa fa-check"></i><b>4.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="group-comparisons.html"><a href="group-comparisons.html"><i class="fa fa-check"></i><b>5</b> Group Comparisons</a>
<ul>
<li class="chapter" data-level="5.1" data-path="group-comparisons.html"><a href="group-comparisons.html#data"><i class="fa fa-check"></i><b>5.1</b> Data</a></li>
<li class="chapter" data-level="5.2" data-path="group-comparisons.html"><a href="group-comparisons.html#between-subject-comparisons"><i class="fa fa-check"></i><b>5.2</b> Between-Subject Comparisons</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="group-comparisons.html"><a href="group-comparisons.html#plots"><i class="fa fa-check"></i><b>5.2.1</b> Plots</a></li>
<li class="chapter" data-level="5.2.2" data-path="group-comparisons.html"><a href="group-comparisons.html#independent-sample-t-test"><i class="fa fa-check"></i><b>5.2.2</b> Independent sample t-test</a></li>
<li class="chapter" data-level="5.2.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesian-normal-model"><i class="fa fa-check"></i><b>5.2.3</b> Bayesian Normal Model</a></li>
<li class="chapter" data-level="5.2.4" data-path="group-comparisons.html"><a href="group-comparisons.html#robust-model"><i class="fa fa-check"></i><b>5.2.4</b> Robust Model</a></li>
<li class="chapter" data-level="5.2.5" data-path="group-comparisons.html"><a href="group-comparisons.html#shifted-lognormal-model"><i class="fa fa-check"></i><b>5.2.5</b> Shifted Lognormal Model*</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="group-comparisons.html"><a href="group-comparisons.html#notes-on-model-comparison"><i class="fa fa-check"></i><b>5.3</b> Notes on Model Comparison</a></li>
<li class="chapter" data-level="5.4" data-path="group-comparisons.html"><a href="group-comparisons.html#within-subject-comparisons"><i class="fa fa-check"></i><b>5.4</b> Within-Subject Comparisons</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="group-comparisons.html"><a href="group-comparisons.html#plots-1"><i class="fa fa-check"></i><b>5.4.1</b> Plots</a></li>
<li class="chapter" data-level="5.4.2" data-path="group-comparisons.html"><a href="group-comparisons.html#independent-sample-t-test-1"><i class="fa fa-check"></i><b>5.4.2</b> Independent sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="5.4.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesian-normal-model-1"><i class="fa fa-check"></i><b>5.4.3</b> Bayesian Normal Model</a></li>
<li class="chapter" data-level="5.4.4" data-path="group-comparisons.html"><a href="group-comparisons.html#using-brms"><i class="fa fa-check"></i><b>5.4.4</b> Using <code>brms</code>*</a></li>
<li class="chapter" data-level="5.4.5" data-path="group-comparisons.html"><a href="group-comparisons.html#region-of-practical-equivalence-rope"><i class="fa fa-check"></i><b>5.4.5</b> Region of Practical Equivalence (ROPE)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="6.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#monte-carlo-simulation-with-one-unknown"><i class="fa fa-check"></i><b>6.1</b> Monte Carlo Simulation With One Unknown</a></li>
<li class="chapter" data-level="6.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo-mcmc-with-one-parameter"><i class="fa fa-check"></i><b>6.2</b> Markov Chain Monte Carlo (MCMC) With One Parameter</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> The Metropolis algorithm</a></li>
<li class="chapter" data-level="6.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-metropolis-hastings-algorithm"><i class="fa fa-check"></i><b>6.2.2</b> The Metropolis-Hastings Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain"><i class="fa fa-check"></i><b>6.3</b> Markov Chain</a></li>
<li class="chapter" data-level="6.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#effective-sample-size-n_texteff"><i class="fa fa-check"></i><b>6.4</b> Effective Sample Size (<span class="math inline">\(n_\text{eff}\)</span>)</a></li>
<li class="chapter" data-level="6.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mc-error"><i class="fa fa-check"></i><b>6.5</b> MC Error</a></li>
<li class="chapter" data-level="6.6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#burn-inwarmup"><i class="fa fa-check"></i><b>6.6</b> Burn-in/Warmup</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#thinning"><i class="fa fa-check"></i><b>6.6.1</b> Thinning</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#diagnostics-of-mcmc"><i class="fa fa-check"></i><b>6.7</b> Diagnostics of MCMC</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mixing"><i class="fa fa-check"></i><b>6.7.1</b> Mixing</a></li>
<li class="chapter" data-level="6.7.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#acceptance-rate"><i class="fa fa-check"></i><b>6.7.2</b> Acceptance Rate</a></li>
<li class="chapter" data-level="6.7.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#diagnostics-using-multiple-chains"><i class="fa fa-check"></i><b>6.7.3</b> Diagnostics Using Multiple Chains</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#multiple-parameters"><i class="fa fa-check"></i><b>6.8</b> Multiple Parameters</a></li>
<li class="chapter" data-level="6.9" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>6.9</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>7</b> Linear Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="linear-models.html"><a href="linear-models.html#what-is-regression"><i class="fa fa-check"></i><b>7.1</b> What is Regression?</a></li>
<li class="chapter" data-level="7.2" data-path="linear-models.html"><a href="linear-models.html#one-predictor"><i class="fa fa-check"></i><b>7.2</b> One Predictor</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="linear-models.html"><a href="linear-models.html#a-continuous-predictor"><i class="fa fa-check"></i><b>7.2.1</b> A continuous predictor</a></li>
<li class="chapter" data-level="7.2.2" data-path="linear-models.html"><a href="linear-models.html#centering"><i class="fa fa-check"></i><b>7.2.2</b> Centering</a></li>
<li class="chapter" data-level="7.2.3" data-path="linear-models.html"><a href="linear-models.html#a-categorical-predictor"><i class="fa fa-check"></i><b>7.2.3</b> A categorical predictor</a></li>
<li class="chapter" data-level="7.2.4" data-path="linear-models.html"><a href="linear-models.html#predictors-with-multiple-categories"><i class="fa fa-check"></i><b>7.2.4</b> Predictors with multiple categories</a></li>
<li class="chapter" data-level="7.2.5" data-path="linear-models.html"><a href="linear-models.html#stan-4"><i class="fa fa-check"></i><b>7.2.5</b> STAN</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="linear-models.html"><a href="linear-models.html#multiple-regression"><i class="fa fa-check"></i><b>7.3</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="linear-models.html"><a href="linear-models.html#two-predictor-example"><i class="fa fa-check"></i><b>7.3.1</b> Two Predictor Example</a></li>
<li class="chapter" data-level="7.3.2" data-path="linear-models.html"><a href="linear-models.html#interactions"><i class="fa fa-check"></i><b>7.3.2</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="linear-models.html"><a href="linear-models.html#tabulating-the-models"><i class="fa fa-check"></i><b>7.4</b> Tabulating the Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Model Diagnostics</a>
<ul>
<li class="chapter" data-level="8.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#assumptions-of-linear-models"><i class="fa fa-check"></i><b>8.1</b> Assumptions of Linear Models</a></li>
<li class="chapter" data-level="8.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#diagnostic-tools"><i class="fa fa-check"></i><b>8.2</b> Diagnostic Tools</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#posterior-predictive-check-7"><i class="fa fa-check"></i><b>8.2.1</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="8.2.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#marginal-model-plots"><i class="fa fa-check"></i><b>8.2.2</b> Marginal model plots</a></li>
<li class="chapter" data-level="8.2.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#residual-plots"><i class="fa fa-check"></i><b>8.2.3</b> Residual plots</a></li>
<li class="chapter" data-level="8.2.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#multicollinearity"><i class="fa fa-check"></i><b>8.2.4</b> Multicollinearity</a></li>
<li class="chapter" data-level="8.2.5" data-path="model-diagnostics.html"><a href="model-diagnostics.html#robust-models"><i class="fa fa-check"></i><b>8.2.5</b> Robust Models</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#other-topics"><i class="fa fa-check"></i><b>8.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html"><i class="fa fa-check"></i><b>9</b> Model Comparison and Regularization</a>
<ul>
<li class="chapter" data-level="9.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>9.1</b> Overfitting and Underfitting</a></li>
<li class="chapter" data-level="9.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>9.2</b> Kullback-Leibler Divergence</a></li>
<li class="chapter" data-level="9.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#information-criteria"><i class="fa fa-check"></i><b>9.3</b> Information Criteria</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#experiment-on-deviance"><i class="fa fa-check"></i><b>9.3.1</b> Experiment on Deviance</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#information-criteria-1"><i class="fa fa-check"></i><b>9.4</b> Information Criteria</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#akaike-information-criteria-aic"><i class="fa fa-check"></i><b>9.4.1</b> Akaike Information Criteria (AIC)</a></li>
<li class="chapter" data-level="9.4.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#deviance-information-criteria-dic"><i class="fa fa-check"></i><b>9.4.2</b> Deviance Information Criteria (DIC)</a></li>
<li class="chapter" data-level="9.4.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#watanabe-akaike-information-criteria-waic"><i class="fa fa-check"></i><b>9.4.3</b> Watanabe-Akaike Information Criteria (WAIC)</a></li>
<li class="chapter" data-level="9.4.4" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>9.4.4</b> Leave-One-Out Cross Validation</a></li>
<li class="chapter" data-level="9.4.5" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#example"><i class="fa fa-check"></i><b>9.4.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#stackingmodel-averaging"><i class="fa fa-check"></i><b>9.5</b> Stacking/Model Averaging</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#model-weights"><i class="fa fa-check"></i><b>9.5.1</b> Model Weights</a></li>
<li class="chapter" data-level="9.5.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#model-averaging"><i class="fa fa-check"></i><b>9.5.2</b> Model Averaging</a></li>
<li class="chapter" data-level="9.5.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#stacking"><i class="fa fa-check"></i><b>9.5.3</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#shrinkage-priors"><i class="fa fa-check"></i><b>9.6</b> Shrinkage Priors</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#number-of-parameters"><i class="fa fa-check"></i><b>9.6.1</b> Number of parameters</a></li>
<li class="chapter" data-level="9.6.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#sparsity-inducing-priors"><i class="fa fa-check"></i><b>9.6.2</b> Sparsity-Inducing Priors</a></li>
<li class="chapter" data-level="9.6.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#finnish-horseshoe"><i class="fa fa-check"></i><b>9.6.3</b> Finnish Horseshoe</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#variable-selection"><i class="fa fa-check"></i><b>9.7</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#projection-based-method"><i class="fa fa-check"></i><b>9.7.1</b> Projection-Based Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html"><i class="fa fa-check"></i><b>10</b> Hierarchical &amp; Multilevel Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#anova"><i class="fa fa-check"></i><b>10.1</b> ANOVA</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#frequentist-anova"><i class="fa fa-check"></i><b>10.1.1</b> “Frequentist” ANOVA</a></li>
<li class="chapter" data-level="10.1.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#bayesian-anova"><i class="fa fa-check"></i><b>10.1.2</b> Bayesian ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#multilevel-modeling-mlm"><i class="fa fa-check"></i><b>10.2</b> Multilevel Modeling (MLM)</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#examples-of-clustering"><i class="fa fa-check"></i><b>10.2.1</b> Examples of clustering</a></li>
<li class="chapter" data-level="10.2.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#data-1"><i class="fa fa-check"></i><b>10.2.2</b> Data</a></li>
<li class="chapter" data-level="10.2.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#intraclass-correlation"><i class="fa fa-check"></i><b>10.2.3</b> Intraclass correlation</a></li>
<li class="chapter" data-level="10.2.4" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#is-mlm-needed"><i class="fa fa-check"></i><b>10.2.4</b> Is MLM needed?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-coefficients"><i class="fa fa-check"></i><b>10.3</b> Varying Coefficients</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-intercepts"><i class="fa fa-check"></i><b>10.3.1</b> Varying Intercepts</a></li>
<li class="chapter" data-level="10.3.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-slopes"><i class="fa fa-check"></i><b>10.3.2</b> Varying Slopes</a></li>
<li class="chapter" data-level="10.3.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-sigma"><i class="fa fa-check"></i><b>10.3.3</b> Varying <span class="math inline">\(\sigma\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#model-comparisons"><i class="fa fa-check"></i><b>10.4</b> Model Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>11</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#basics-of-generalized-linear-models"><i class="fa fa-check"></i><b>11.1</b> Basics of Generalized Linear Models</a></li>
<li class="chapter" data-level="11.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>11.2</b> Binary Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#the-logit-link"><i class="fa fa-check"></i><b>11.2.1</b> The logit link</a></li>
<li class="chapter" data-level="11.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#choice-of-priors"><i class="fa fa-check"></i><b>11.2.2</b> Choice of Priors</a></li>
<li class="chapter" data-level="11.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interpreting-the-coefficients"><i class="fa fa-check"></i><b>11.2.3</b> Interpreting the coefficients</a></li>
<li class="chapter" data-level="11.2.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#model-checking-1"><i class="fa fa-check"></i><b>11.2.4</b> Model Checking</a></li>
<li class="chapter" data-level="11.2.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#complete-separation"><i class="fa fa-check"></i><b>11.2.5</b> Complete Separation</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binomial-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Binomial Logistic Regression</a></li>
<li class="chapter" data-level="11.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#probit-regression"><i class="fa fa-check"></i><b>11.4</b> Probit Regression</a></li>
<li class="chapter" data-level="11.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>11.5</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interpretations-2"><i class="fa fa-check"></i><b>11.5.1</b> Interpretations</a></li>
<li class="chapter" data-level="11.5.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#model-checking-2"><i class="fa fa-check"></i><b>11.5.2</b> Model Checking</a></li>
<li class="chapter" data-level="11.5.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#other-models-in-glm"><i class="fa fa-check"></i><b>11.5.3</b> Other models in GLM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="missing-data.html"><a href="missing-data.html"><i class="fa fa-check"></i><b>12</b> Missing Data</a>
<ul>
<li class="chapter" data-level="12.1" data-path="missing-data.html"><a href="missing-data.html#missing-data-mechanisms"><i class="fa fa-check"></i><b>12.1</b> Missing Data Mechanisms</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="missing-data.html"><a href="missing-data.html#mcar-missing-completely-at-random"><i class="fa fa-check"></i><b>12.1.1</b> MCAR (Missing Completely at Random)</a></li>
<li class="chapter" data-level="12.1.2" data-path="missing-data.html"><a href="missing-data.html#mar-missing-at-random"><i class="fa fa-check"></i><b>12.1.2</b> MAR (Missing At Random)</a></li>
<li class="chapter" data-level="12.1.3" data-path="missing-data.html"><a href="missing-data.html#nmar-not-missing-at-random"><i class="fa fa-check"></i><b>12.1.3</b> NMAR (Not Missing At Random)</a></li>
<li class="chapter" data-level="12.1.4" data-path="missing-data.html"><a href="missing-data.html#ignorable-missingness"><i class="fa fa-check"></i><b>12.1.4</b> Ignorable Missingness*</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="missing-data.html"><a href="missing-data.html#bayesian-approaches-for-missing-data"><i class="fa fa-check"></i><b>12.2</b> Bayesian Approaches for Missing Data</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="missing-data.html"><a href="missing-data.html#complete-case-analysislistwise-deletion"><i class="fa fa-check"></i><b>12.2.1</b> Complete Case Analysis/Listwise Deletion</a></li>
<li class="chapter" data-level="12.2.2" data-path="missing-data.html"><a href="missing-data.html#treat-missing-data-as-parameters"><i class="fa fa-check"></i><b>12.2.2</b> Treat Missing Data as Parameters</a></li>
<li class="chapter" data-level="12.2.3" data-path="missing-data.html"><a href="missing-data.html#multiple-imputation"><i class="fa fa-check"></i><b>12.2.3</b> Multiple Imputation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Handouts for Bayesian Data Analysis Class</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-comparison-and-regularization" class="section level1" number="9">
<h1><span class="header-section-number">Chapter 9</span> Model Comparison and Regularization</h1>
<div id="overfitting-and-underfitting" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Overfitting and Underfitting</h2>
<p>In statistical modeling, a more complex model almost always results in a better
fit to the data. Roughly speaking, a more complex model means one with more
parameters, although as you will see later, in Bayesian analyses, number of
parameters is sometimes not straight forward to find out. On the extreme side,
if one has 10 observations, one can has a model with 10 parameters that can
perfectly predict every single data point (by just having a parameter to predict
each data point). However, there are two problems with too complex a model.
First, an increasingly complex model also makes it increasingly hard to extract
useful information from the data. Instead of describing the relationship between
two variables, like <code>mom_iq</code> and <code>kid_score</code>, by a straight line, one ends up
with a crazy model that is difficult to make sense. Second, as you will also
see, the more complex a model, the more is the risk that it <em>overfit</em> the
current data such that it does not work for future observations.</p>
<p>For example, let’s randomly sample 10 cases in the <code>kidiq</code> data set, and build
some model from it.</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="model-comparison-and-regularization.html#cb239-1"></a>kidiq &lt;-<span class="st"> </span>haven<span class="op">::</span><span class="kw">read_dta</span>(<span class="st">&quot;../data/kidiq.dta&quot;</span>)</span>
<span id="cb239-2"><a href="model-comparison-and-regularization.html#cb239-2"></a>kidiq100 &lt;-<span class="st"> </span>kidiq <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb239-3"><a href="model-comparison-and-regularization.html#cb239-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mom_iq =</span> mom_iq <span class="op">/</span><span class="st"> </span><span class="dv">100</span>,  <span class="co"># divid mom_iq by 100</span></span>
<span id="cb239-4"><a href="model-comparison-and-regularization.html#cb239-4"></a>         <span class="dt">kid_score =</span> kid_score <span class="op">/</span><span class="st"> </span><span class="dv">100</span>,   <span class="co"># divide kid_score by 100</span></span>
<span id="cb239-5"><a href="model-comparison-and-regularization.html#cb239-5"></a>         <span class="dt">mom_iq_c =</span> mom_iq <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, </span>
<span id="cb239-6"><a href="model-comparison-and-regularization.html#cb239-6"></a>         <span class="dt">mom_hs =</span> <span class="kw">factor</span>(mom_hs, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>)))</span></code></pre></div>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="model-comparison-and-regularization.html#cb240-1"></a><span class="kw">set.seed</span>(<span class="dv">1533</span>)  <span class="co"># set the seed for reproducibility</span></span>
<span id="cb240-2"><a href="model-comparison-and-regularization.html#cb240-2"></a><span class="co"># Sample 10 observations</span></span>
<span id="cb240-3"><a href="model-comparison-and-regularization.html#cb240-3"></a>train &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="kw">nrow</span>(kidiq), 10L)</span>
<span id="cb240-4"><a href="model-comparison-and-regularization.html#cb240-4"></a>kidiq_sub &lt;-<span class="st"> </span>kidiq[train, ]</span>
<span id="cb240-5"><a href="model-comparison-and-regularization.html#cb240-5"></a>base &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> mom_iq, <span class="dt">y =</span> kid_score), </span>
<span id="cb240-6"><a href="model-comparison-and-regularization.html#cb240-6"></a>               <span class="dt">data =</span> kidiq_sub) <span class="op">+</span><span class="st"> </span></span>
<span id="cb240-7"><a href="model-comparison-and-regularization.html#cb240-7"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="fl">.9</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb240-8"><a href="model-comparison-and-regularization.html#cb240-8"></a><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">120</span>, <span class="dv">180</span>)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb240-9"><a href="model-comparison-and-regularization.html#cb240-9"></a><span class="st">  </span><span class="kw">xlim</span>(<span class="kw">range</span>(kidiq<span class="op">$</span>mom_iq))</span>
<span id="cb240-10"><a href="model-comparison-and-regularization.html#cb240-10"></a><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> mom_iq, <span class="dt">y =</span> kid_score), <span class="dt">data =</span> kidiq) <span class="op">+</span><span class="st"> </span></span>
<span id="cb240-11"><a href="model-comparison-and-regularization.html#cb240-11"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="fl">.7</span>, <span class="dt">col =</span> <span class="st">&quot;lightblue&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb240-12"><a href="model-comparison-and-regularization.html#cb240-12"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="fl">1.1</span>, <span class="dt">data =</span> kidiq_sub, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb240-13"><a href="model-comparison-and-regularization.html#cb240-13"></a><span class="st">  </span><span class="kw">xlim</span>(<span class="kw">range</span>(kidiq<span class="op">$</span>mom_iq))</span></code></pre></div>
<p><img src="09_model_comparison_files/figure-html/plot-kidiq-sub-1.png" width="336" /></p>
<p>When using <code>mom_iq</code> to predict <code>kid_score</code>, we can use beyond a linear
regression line by using higher order <em>polynomials</em>. For example, a second-order
polynomial assumes a quadratic effect (with one turning point), and it goes to
cubic, quartic, and more. The figure below shows the fit from a linear effect of
<code>mom_iq</code>, a quadratic effect, and increasingly complex to a six degree
polynomial. As you can see, as the model gets more complex, the fitted line
tries to capture all the 10 points really well, with an increasing <span class="math inline">\(R^2\)</span>.
However, also note that the standard error around the fitted line gets, meaning
that there are more uncertainty when the model gets more and more complex.</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="model-comparison-and-regularization.html#cb241-1"></a>r2 &lt;-<span class="st"> </span><span class="cf">function</span>(object) {</span>
<span id="cb241-2"><a href="model-comparison-and-regularization.html#cb241-2"></a>  <span class="co"># Function for computing R^2</span></span>
<span id="cb241-3"><a href="model-comparison-and-regularization.html#cb241-3"></a>  z &lt;-<span class="st"> </span>object</span>
<span id="cb241-4"><a href="model-comparison-and-regularization.html#cb241-4"></a>  f &lt;-<span class="st"> </span>z<span class="op">$</span>fitted.values</span>
<span id="cb241-5"><a href="model-comparison-and-regularization.html#cb241-5"></a>  r &lt;-<span class="st"> </span>z<span class="op">$</span>residuals</span>
<span id="cb241-6"><a href="model-comparison-and-regularization.html#cb241-6"></a>  mss &lt;-<span class="st"> </span><span class="cf">if</span> (<span class="kw">attr</span>(z<span class="op">$</span>terms, <span class="st">&quot;intercept&quot;</span>)) </span>
<span id="cb241-7"><a href="model-comparison-and-regularization.html#cb241-7"></a>    <span class="kw">sum</span>((f <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(f))<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb241-8"><a href="model-comparison-and-regularization.html#cb241-8"></a>  <span class="cf">else</span> <span class="kw">sum</span>(f<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb241-9"><a href="model-comparison-and-regularization.html#cb241-9"></a>  rss &lt;-<span class="st"> </span><span class="kw">sum</span>(r<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb241-10"><a href="model-comparison-and-regularization.html#cb241-10"></a>  mss <span class="op">/</span><span class="st"> </span>(mss <span class="op">+</span><span class="st"> </span>rss)</span>
<span id="cb241-11"><a href="model-comparison-and-regularization.html#cb241-11"></a>}</span>
<span id="cb241-12"><a href="model-comparison-and-regularization.html#cb241-12"></a></span>
<span id="cb241-13"><a href="model-comparison-and-regularization.html#cb241-13"></a>p_list &lt;-<span class="st"> </span><span class="kw">map</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="cf">function</span>(i) {</span>
<span id="cb241-14"><a href="model-comparison-and-regularization.html#cb241-14"></a>  mod &lt;-<span class="st"> </span><span class="kw">lm</span>(kid_score <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(mom_iq, <span class="dt">degree =</span> i), <span class="dt">data =</span> kidiq_sub)</span>
<span id="cb241-15"><a href="model-comparison-and-regularization.html#cb241-15"></a>  base <span class="op">+</span><span class="st"> </span></span>
<span id="cb241-16"><a href="model-comparison-and-regularization.html#cb241-16"></a><span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, i), <span class="dt">level =</span> <span class="fl">.80</span>, </span>
<span id="cb241-17"><a href="model-comparison-and-regularization.html#cb241-17"></a>                <span class="dt">fullrange =</span> <span class="ot">TRUE</span>) <span class="op">+</span></span>
<span id="cb241-18"><a href="model-comparison-and-regularization.html#cb241-18"></a><span class="st">    </span><span class="kw">geom_text</span>(<span class="dt">x =</span> <span class="dv">90</span>, <span class="dt">y =</span> <span class="dv">170</span>, </span>
<span id="cb241-19"><a href="model-comparison-and-regularization.html#cb241-19"></a>              <span class="dt">label =</span> <span class="kw">paste0</span>(<span class="st">&quot;italic(R)^2 == &quot;</span>, <span class="kw">round</span>(<span class="kw">r2</span>(mod), <span class="dv">1</span>)), </span>
<span id="cb241-20"><a href="model-comparison-and-regularization.html#cb241-20"></a>              <span class="dt">parse =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb241-21"><a href="model-comparison-and-regularization.html#cb241-21"></a><span class="st">    </span><span class="kw">geom_text</span>(<span class="dt">x =</span> <span class="dv">100</span>, <span class="dt">y =</span> <span class="dv">-100</span>, </span>
<span id="cb241-22"><a href="model-comparison-and-regularization.html#cb241-22"></a>              <span class="dt">label =</span> <span class="kw">paste0</span>(<span class="st">&quot;RMSE == &quot;</span>, <span class="kw">round</span>(<span class="kw">sqrt</span>(<span class="kw">mean</span>(<span class="kw">residuals</span>(mod)<span class="op">^</span><span class="dv">2</span>)), <span class="dv">1</span>)), </span>
<span id="cb241-23"><a href="model-comparison-and-regularization.html#cb241-23"></a>              <span class="dt">parse =</span> <span class="ot">TRUE</span>)</span>
<span id="cb241-24"><a href="model-comparison-and-regularization.html#cb241-24"></a>})</span>
<span id="cb241-25"><a href="model-comparison-and-regularization.html#cb241-25"></a><span class="kw">do.call</span>(grid.arrange, <span class="kw">c</span>(p_list, <span class="dt">nrow =</span> <span class="dv">2</span>))</span></code></pre></div>
<div class="figure"><span id="fig:overfit-data"></span>
<img src="09_model_comparison_files/figure-html/overfit-data-1.png" alt="Fit of models on the 10 random cases. Top panel: linear, quadratic, and cubic; bottom panel: 4th, 5th, and 6th degree polynomials" width="720" />
<p class="caption">
Figure 9.1: Fit of models on the 10 random cases. Top panel: linear, quadratic, and cubic; bottom panel: 4th, 5th, and 6th degree polynomials
</p>
</div>
<p>Another way to look at the accuracy of the model is to look at the <em>Root Mean
Squared Error</em> (RMSE), which is defined as the square root of the average
squared prediction error. This is a measure of prediction error. The smaller the
RMSE, the better the prediction is. As you can see in the above figure, more
complex models always reduce the RMSE in the data we use to fit the model
(also called training data).</p>
<p>However, if I take the estimated regression line/curve based on the subsample of
10 observations, and predict the remaining cases in the data set, things will be
different. As you can see in the figure below, whereas prediction error is
comparable for the linear and the quadratic model, polynomials of higher degrees
predict the data really badly. This is because when you use a complex model in a
data set, it tailors the coefficients to any sampling errors and noise in the
data such that it will not generalize to new observations. Therefore, our goal
in model comparison is to choose a model that is complex enough to capture the
essence of the data generation process (and thus avoid <em>underfitting</em>), but
avoid <em>overfitting</em> as to make the model useless for predicting new
observations.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="model-comparison-and-regularization.html#cb242-1"></a>base2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> mom_iq, <span class="dt">y =</span> kid_score), <span class="dt">data =</span> kidiq[<span class="op">-</span>train, ]) <span class="op">+</span><span class="st"> </span></span>
<span id="cb242-2"><a href="model-comparison-and-regularization.html#cb242-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="fl">.6</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb242-3"><a href="model-comparison-and-regularization.html#cb242-3"></a><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">120</span>, <span class="dv">180</span>)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb242-4"><a href="model-comparison-and-regularization.html#cb242-4"></a><span class="st">  </span><span class="kw">xlim</span>(<span class="kw">range</span>(kidiq<span class="op">$</span>mom_iq))</span>
<span id="cb242-5"><a href="model-comparison-and-regularization.html#cb242-5"></a>p_list2 &lt;-<span class="st"> </span><span class="kw">map</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="cf">function</span>(i) {</span>
<span id="cb242-6"><a href="model-comparison-and-regularization.html#cb242-6"></a>  mod &lt;-<span class="st"> </span><span class="kw">lm</span>(kid_score <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(mom_iq, <span class="dt">degree =</span> i), <span class="dt">data =</span> kidiq_sub)</span>
<span id="cb242-7"><a href="model-comparison-and-regularization.html#cb242-7"></a>  f &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> kidiq[<span class="op">-</span>train, ])</span>
<span id="cb242-8"><a href="model-comparison-and-regularization.html#cb242-8"></a>  y &lt;-<span class="st"> </span>kidiq<span class="op">$</span>kid_score[<span class="op">-</span>train]</span>
<span id="cb242-9"><a href="model-comparison-and-regularization.html#cb242-9"></a>  r &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>f</span>
<span id="cb242-10"><a href="model-comparison-and-regularization.html#cb242-10"></a>  rmse_mod &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>(r<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb242-11"><a href="model-comparison-and-regularization.html#cb242-11"></a>  base2 <span class="op">+</span><span class="st"> </span></span>
<span id="cb242-12"><a href="model-comparison-and-regularization.html#cb242-12"></a><span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">data =</span> kidiq_sub, <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, i), </span>
<span id="cb242-13"><a href="model-comparison-and-regularization.html#cb242-13"></a>                <span class="dt">fullrange =</span> <span class="ot">TRUE</span>, <span class="dt">level =</span> <span class="fl">.80</span>) <span class="op">+</span></span>
<span id="cb242-14"><a href="model-comparison-and-regularization.html#cb242-14"></a><span class="st">    </span><span class="kw">geom_text</span>(<span class="dt">x =</span> <span class="dv">100</span>, <span class="dt">y =</span> <span class="dv">-100</span>, </span>
<span id="cb242-15"><a href="model-comparison-and-regularization.html#cb242-15"></a>              <span class="dt">label =</span> <span class="kw">paste0</span>(<span class="st">&quot;RMSE == &quot;</span>, <span class="kw">round</span>(rmse_mod, <span class="dv">1</span>)), </span>
<span id="cb242-16"><a href="model-comparison-and-regularization.html#cb242-16"></a>              <span class="dt">parse =</span> <span class="ot">TRUE</span>)</span>
<span id="cb242-17"><a href="model-comparison-and-regularization.html#cb242-17"></a>})</span>
<span id="cb242-18"><a href="model-comparison-and-regularization.html#cb242-18"></a><span class="kw">do.call</span>(grid.arrange, <span class="kw">c</span>(p_list2, <span class="dt">nrow =</span> <span class="dv">2</span>))</span></code></pre></div>
<div class="figure"><span id="fig:overfit-generalize"></span>
<img src="09_model_comparison_files/figure-html/overfit-generalize-1.png" alt="Using the regression lines based on 10 random cases to predict the remaining 424 cases. Top panel: linear, quadratic, and cubic; bottom panel: 4th, 5th, and 6th degree polynomials" width="720" />
<p class="caption">
Figure 9.2: Using the regression lines based on 10 random cases to predict the remaining 424 cases. Top panel: linear, quadratic, and cubic; bottom panel: 4th, 5th, and 6th degree polynomials
</p>
</div>
<p>The goal of statistical modeling is to choose a model that is optimal
between the overfitting/underfitting dichotomy. In machine learning, this is
also commonly referred to as the bias-variance trade-off, as a model that is too
simple tends to produce biased predictions because it does not capture the
essence of the data generating process, whereas a model that is overly complex
is unbiased but results in a lot of uncertainty in the prediction, because
there are too many unnecessary components that can affect predictions, as
indicated in the confidence bands around the 6th degree polynomial line.</p>
<p>Polynomials of varying degrees are merely one example of comparing simple to
complex models. You can think about:</p>
<ul>
<li>models with and without interactions,</li>
<li>models with a few predictors versus hundreds of predictors,</li>
<li>regression analyses versus multilevel models, etc.</li>
</ul>
<p>This lecture is about finding an optimal model that avoids overfitting and
avoids underfitting. Whereas one can always avoid underfitting by fitting a more
and more complex model, a more practical problem is to have some tools to
refrain oneself from choosing a model that is too complex and predict future
observations badly. In this note, you will learn to perform model comparison
with information criteria to find a model that has better balance between
overfitting and underfitting, while in the next note you will learn additional
tools that synthesize multiple models and perform variable selection.</p>
</div>
<div id="kullback-leibler-divergence" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Kullback-Leibler Divergence</h2>
<p>When comparing models (e.g., linear vs. quadratic), we prefer models that are
closer to the “true” data-generating process. A model that are closer to the
“true” model is better than a model that are not as close. Therefore, we need
some ways to quantify the degree of “closeness” to the true model. Note that in
this context models refer to the distributional family as well as the parameter
values. For example, the model <span class="math inline">\(y_i \sim \mathcal{N}(5, 2)\)</span> is a different model than
<span class="math inline">\(y_i \sim \mathcal{N}(3, 2)\)</span>, which is a different model than <span class="math inline">\(y_i \sim \mathrm{Gamma}(2, 2)\)</span>. The first two have the same family but different
parameter values (different means, same <span class="math inline">\(\mathit{SD}\)</span>), whereas the last two have
different distributional families (Normal vs. Gamma).</p>
<p>To measure the degree of “closeness” between two models, <span class="math inline">\(M_0\)</span> and <span class="math inline">\(M_1\)</span>, by far
the most popular metric in statistics is the <em>Kullback-Liebler Divergence</em> (or
Kullback-Liebler discrepancy; <span class="math inline">\(D_\textrm{KL}\)</span>). By definition,</p>
<p><span class="math display">\[\begin{align*}
D_\textrm{KL}(M_0 | M_1) &amp; = \int_{-\infty}^\infty p_{M_0} (\boldsymbol{\mathbf{y}}) 
                    \log \frac{p_{M_0}(\boldsymbol{\mathbf{y}})}{p_{M_1}(\boldsymbol{\mathbf{y}})} \; \mathrm{d}\boldsymbol{\mathbf{y}} \\
                &amp; = \int_{-\infty}^\infty p_{M_0} (\boldsymbol{\mathbf{y}}) 
                          \log p_{M_0}(\boldsymbol{\mathbf{y}}) \; \mathrm{d}\boldsymbol{\mathbf{y}} - 
                    \int_{-\infty}^\infty p_{M_0} (\boldsymbol{\mathbf{y}}) 
                          \log p_{M_1}(\boldsymbol{\mathbf{y}}) \; \mathrm{d}\boldsymbol{\mathbf{y}}. 
\end{align*}\]</span></p>
<p>Note that strictly speaking, <span class="math inline">\(D_\textrm{KL}\)</span> cannot be called a “distance” between two
models because in general, <span class="math inline">\(D_\textrm{KL}(M_0 | M_1) \neq D_\textrm{KL}(M_1 | M_0)\)</span>. As an
example, assume that the data are generated by a true model <span class="math inline">\(M_0\)</span>, and we have
two candidate models <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span>, where</p>
<ul>
<li><span class="math inline">\(M_0: y \sim \mathcal{N}(3, 2)\)</span></li>
<li><span class="math inline">\(M_1: y \sim \mathcal{N}(3.5, 2.5)\)</span></li>
<li><span class="math inline">\(M_2: y \sim \mathrm{Cauchy}(3, 2)\)</span></li>
</ul>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="model-comparison-and-regularization.html#cb243-1"></a><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">9</span>)), <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb243-2"><a href="model-comparison-and-regularization.html#cb243-2"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">3</span>, <span class="dt">sd =</span> <span class="dv">2</span>), </span>
<span id="cb243-3"><a href="model-comparison-and-regularization.html#cb243-3"></a>                <span class="kw">aes</span>(<span class="dt">col =</span> <span class="st">&quot;M0&quot;</span>), <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb243-4"><a href="model-comparison-and-regularization.html#cb243-4"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="fl">3.5</span>, <span class="dt">sd =</span> <span class="fl">2.5</span>), </span>
<span id="cb243-5"><a href="model-comparison-and-regularization.html#cb243-5"></a>                <span class="kw">aes</span>(<span class="dt">col =</span> <span class="st">&quot;M1&quot;</span>), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb243-6"><a href="model-comparison-and-regularization.html#cb243-6"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dcauchy, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">location =</span> <span class="dv">3</span>, <span class="dt">scale =</span> <span class="dv">2</span>), </span>
<span id="cb243-7"><a href="model-comparison-and-regularization.html#cb243-7"></a>                <span class="kw">aes</span>(<span class="dt">col =</span> <span class="st">&quot;M2&quot;</span>), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb243-8"><a href="model-comparison-and-regularization.html#cb243-8"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="st">&quot;&quot;</span>, <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>), </span>
<span id="cb243-9"><a href="model-comparison-and-regularization.html#cb243-9"></a>                     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;M0&quot;</span>, <span class="st">&quot;M1&quot;</span>, <span class="st">&quot;M2&quot;</span>)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb243-10"><a href="model-comparison-and-regularization.html#cb243-10"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;y&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;density&quot;</span>)</span></code></pre></div>
<pre><code>&gt;# Warning: `mapping` is not used by stat_function()

&gt;# Warning: `mapping` is not used by stat_function()

&gt;# Warning: `mapping` is not used by stat_function()</code></pre>
<div class="figure"><span id="fig:divergence"></span>
<img src="09_model_comparison_files/figure-html/divergence-1.png" alt="Density for $M_0$, $M_1$, and $M_2$" width="384" />
<p class="caption">
Figure 9.3: Density for <span class="math inline">\(M_0\)</span>, <span class="math inline">\(M_1\)</span>, and <span class="math inline">\(M_2\)</span>
</p>
</div>
<p>One can compute that <span class="math inline">\(D_\textrm{KL}(M_0 | M_1) = 0.063\)</span> and
<span class="math inline">\(D_\textrm{KL}(M_0 | M_1) = 0.259\)</span>, and so <span class="math inline">\(M_1\)</span> is a better
model than <span class="math inline">\(M_2\)</span>.</p>
<p>Note that in the expression of <span class="math inline">\(D_\textrm{KL}\)</span>, when talking about the same target model,
the first term is always the same and describes the “true” model, <span class="math inline">\(M_0\)</span>.
Therefore, it is sufficient to compare models on the second term,
<span class="math inline">\(\int_{-\infty}^\infty p_{M_0} (\boldsymbol{\mathbf{y}}) \log p_{M_1}(\boldsymbol{\mathbf{y}}) \; \mathrm{d}\boldsymbol{\mathbf{y}}\)</span>, which can
also be written as <span class="math inline">\(\mathrm{E}=[\log p_{M_1} (\boldsymbol{\mathbf{y}})]\)</span>, i.e., the <em>expected log
predictive density</em> (<em>elpd</em>). In other words, a model with a larger elpd is
preferred over a model with a smaller elpd.</p>
<p>However, in real data analysis, we don’t know what <span class="math inline">\(M_0\)</span> is. If we knew, then we
would just need to choose <span class="math inline">\(M_0\)</span> as our model and there will be no problem about
model comparisons. In addition, even if we know that the true model is, e.g., a
normal model (which never happens in real data analysis), we still need to
estimate the parameter values, and the estimates will not be exactly the same as
the true parameter values. However, elpd is defined as the expected value over
the true predictive distribution, <span class="math inline">\(p_{M_0}(y)\)</span>, which cannot be obtained without
knowing what <span class="math inline">\(M_0\)</span> is.</p>
<p>So instead, we need to estimate the elpd. A naive way to estimate it would be to
assume that the distribution of the data is the true model, but that will lead
to an overly optimistic estimate, and computing elpd this way will always favor
a more complex model. The best way to estimate elpd is to collect data on a new
independent sample that is believed to share the same data generating process as
the current sample, and estimate elpd on the new sample. This is called
<em>out-of-sample validation</em>. The problem, of course, is we usually do not have
the resources to collect a new sample.</p>
<p>Therefore, statisticians had worked hard to find ways to estimate elpd from the
current sample, and there are two broad approaches:</p>
<ul>
<li>Information criteria: AIC, DIC, and WAIC, which estimate the elpd in
the current sample, minus a correction factor</li>
<li>Cross validation, which splits the current sample into <span class="math inline">\(k\)</span> parts, estimate the
parameters in <span class="math inline">\(k - 1\)</span> parts, and estimate the elpd in the remaining 1 part. A
special case is when <span class="math inline">\(k\)</span> = <span class="math inline">\(N\)</span> so that each time one uses <span class="math inline">\(N\)</span> - 1 data points to
estimate the model parameters, and estimate the elpd for the observation that
was left out. This is called <em>leave-one-out</em> cross-validation (LOO-CV).</li>
</ul>
</div>
<div id="information-criteria" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Information Criteria</h2>
<p>We will illustrate the computation of information criteria with <code>mom_iq</code>
predicting <code>kid_score</code> (with centering):</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="model-comparison-and-regularization.html#cb245-1"></a><span class="co"># mom_iq with centering</span></span>
<span id="cb245-2"><a href="model-comparison-and-regularization.html#cb245-2"></a>m1 &lt;-<span class="st"> </span><span class="kw">brm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_iq_c, <span class="dt">data =</span> kidiq100, </span>
<span id="cb245-3"><a href="model-comparison-and-regularization.html#cb245-3"></a>          <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>), </span>
<span id="cb245-4"><a href="model-comparison-and-regularization.html#cb245-4"></a>                    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">coef =</span> <span class="st">&quot;mom_iq_c&quot;</span>), </span>
<span id="cb245-5"><a href="model-comparison-and-regularization.html#cb245-5"></a>                    <span class="kw">prior</span>(<span class="kw">student_t</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;sigma&quot;</span>)), </span>
<span id="cb245-6"><a href="model-comparison-and-regularization.html#cb245-6"></a>          <span class="dt">seed =</span> <span class="dv">2302</span></span>
<span id="cb245-7"><a href="model-comparison-and-regularization.html#cb245-7"></a>)</span></code></pre></div>
<p>Without going too deep into the underlying math, it can be shown that a good
estimate of elpd is</p>
<p><span class="math display">\[\sum_{i = 1}^n \log p_{M_1}(y_i) - p,\]</span></p>
<p>where <span class="math inline">\(p\)</span> is some measure of the number of parameters in <span class="math inline">\(M_1\)</span>. The first term
is the likelihood of the model in the current sample. The second term is an
adjustment factor so that the quantity above represents the average likelihood
of the model <em>in a new sample</em>. It is more common to work with <em>deviance</em> by
multiplying the log-likelihood by <span class="math inline">\(-2\)</span>, i.e.,</p>
<p><span class="math display">\[D = -2 \sum_{i = 1}^n \log p_{M_1}(y_i).\]</span></p>
<div id="experiment-on-deviance" class="section level3" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Experiment on Deviance</h3>
<p>Now, let’s check the in-sample deviance and out-of-sample deviance of our
<code>kidiq</code> data with different polynomial functions. Here is a sample function for
computing elpd (with frequentist, just for speed purpose) for different degrees
of polynomial:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="model-comparison-and-regularization.html#cb246-1"></a><span class="co"># Function for computing deviance with different polynomial</span></span>
<span id="cb246-2"><a href="model-comparison-and-regularization.html#cb246-2"></a>deviance_kidiq &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">degree =</span> <span class="dv">1</span>, </span>
<span id="cb246-3"><a href="model-comparison-and-regularization.html#cb246-3"></a>                           <span class="dt">train =</span> <span class="dv">10</span>, </span>
<span id="cb246-4"><a href="model-comparison-and-regularization.html#cb246-4"></a>                           <span class="dt">y =</span> kidiq<span class="op">$</span>kid_score, </span>
<span id="cb246-5"><a href="model-comparison-and-regularization.html#cb246-5"></a>                           <span class="dt">x =</span> kidiq<span class="op">$</span>mom_iq) {</span>
<span id="cb246-6"><a href="model-comparison-and-regularization.html#cb246-6"></a>  N &lt;-<span class="st"> </span><span class="kw">length</span>(y)</span>
<span id="cb246-7"><a href="model-comparison-and-regularization.html#cb246-7"></a>  <span class="co"># get training sample</span></span>
<span id="cb246-8"><a href="model-comparison-and-regularization.html#cb246-8"></a>  <span class="cf">if</span> (<span class="kw">length</span>(train) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</span>
<span id="cb246-9"><a href="model-comparison-and-regularization.html#cb246-9"></a>    train &lt;-<span class="st"> </span><span class="kw">sample.int</span>(N, train)</span>
<span id="cb246-10"><a href="model-comparison-and-regularization.html#cb246-10"></a>  }</span>
<span id="cb246-11"><a href="model-comparison-and-regularization.html#cb246-11"></a>  ntrain &lt;-<span class="st"> </span><span class="kw">length</span>(train)</span>
<span id="cb246-12"><a href="model-comparison-and-regularization.html#cb246-12"></a>  <span class="co"># Obtain design matrix</span></span>
<span id="cb246-13"><a href="model-comparison-and-regularization.html#cb246-13"></a>  X &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">poly</span>(x, degree, <span class="dt">simple =</span> <span class="ot">TRUE</span>))</span>
<span id="cb246-14"><a href="model-comparison-and-regularization.html#cb246-14"></a>  <span class="co"># Get elpd for training sample</span></span>
<span id="cb246-15"><a href="model-comparison-and-regularization.html#cb246-15"></a>  Xtrain &lt;-<span class="st"> </span>X[train, ]</span>
<span id="cb246-16"><a href="model-comparison-and-regularization.html#cb246-16"></a>  ytrain &lt;-<span class="st"> </span>y[train]</span>
<span id="cb246-17"><a href="model-comparison-and-regularization.html#cb246-17"></a>  betahat &lt;-<span class="st"> </span><span class="kw">qr.solve</span>(Xtrain, ytrain)  <span class="co"># estimated betas</span></span>
<span id="cb246-18"><a href="model-comparison-and-regularization.html#cb246-18"></a>  res_train &lt;-<span class="st"> </span>ytrain <span class="op">-</span><span class="st"> </span>Xtrain <span class="op">%*%</span><span class="st"> </span>betahat</span>
<span id="cb246-19"><a href="model-comparison-and-regularization.html#cb246-19"></a>  sigmahat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(res_train<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span></span>
<span id="cb246-20"><a href="model-comparison-and-regularization.html#cb246-20"></a><span class="st">                     </span>(ntrain <span class="op">-</span><span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>degree))  <span class="co"># estimated sigma</span></span>
<span id="cb246-21"><a href="model-comparison-and-regularization.html#cb246-21"></a>  deviance_train &lt;-<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(res_train, <span class="dt">sd =</span> sigmahat, <span class="dt">log =</span> <span class="ot">TRUE</span>))</span>
<span id="cb246-22"><a href="model-comparison-and-regularization.html#cb246-22"></a>  res_test &lt;-<span class="st"> </span>y[<span class="op">-</span>train] <span class="op">-</span><span class="st"> </span>X[<span class="op">-</span>train, ] <span class="op">%*%</span><span class="st"> </span>betahat</span>
<span id="cb246-23"><a href="model-comparison-and-regularization.html#cb246-23"></a>  deviance_test &lt;-<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(res_test, <span class="dt">sd =</span> sigmahat, <span class="dt">log =</span> <span class="ot">TRUE</span>))</span>
<span id="cb246-24"><a href="model-comparison-and-regularization.html#cb246-24"></a>  <span class="kw">tibble</span>(<span class="dt">degree =</span> degree, </span>
<span id="cb246-25"><a href="model-comparison-and-regularization.html#cb246-25"></a>         <span class="dt">sample =</span> <span class="kw">c</span>(<span class="st">&#39;in-sample&#39;</span>, <span class="st">&#39;out-of-sample&#39;</span>), </span>
<span id="cb246-26"><a href="model-comparison-and-regularization.html#cb246-26"></a>         <span class="dt">deviance =</span> <span class="kw">c</span>(deviance_train <span class="op">/</span><span class="st"> </span>ntrain,</span>
<span id="cb246-27"><a href="model-comparison-and-regularization.html#cb246-27"></a>                      deviance_test <span class="op">/</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span>ntrain))</span>
<span id="cb246-28"><a href="model-comparison-and-regularization.html#cb246-28"></a>  )</span>
<span id="cb246-29"><a href="model-comparison-and-regularization.html#cb246-29"></a>}</span></code></pre></div>
<p>Below shows the in-sample and out-of-sample elpd for linear model:</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="model-comparison-and-regularization.html#cb247-1"></a><span class="kw">deviance_kidiq</span>(<span class="dt">degree =</span> <span class="dv">1</span>, <span class="dt">train =</span> train)</span></code></pre></div>
<pre><code>&gt;# # A tibble: 2 x 3
&gt;#   degree sample        deviance
&gt;#    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;
&gt;# 1      1 in-sample         7.92
&gt;# 2      1 out-of-sample     8.88</code></pre>
<p>And for quadratic:</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="model-comparison-and-regularization.html#cb249-1"></a><span class="kw">deviance_kidiq</span>(<span class="dt">degree =</span> <span class="dv">2</span>, <span class="dt">train =</span> train)</span></code></pre></div>
<pre><code>&gt;# # A tibble: 2 x 3
&gt;#   degree sample        deviance
&gt;#    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;
&gt;# 1      2 in-sample         7.94
&gt;# 2      2 out-of-sample     8.84</code></pre>
<p>As you can see, in general, the deviance is smaller for the current data than
for the hold-out data. Note also because the data sets have different size, I
divide the deviance by the sample size so that they can be compared.</p>
<p>Now let’s run an experiment to check the elpd with different degrees polynomial,
with a training sample size of 60:</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="model-comparison-and-regularization.html#cb251-1"></a><span class="kw">set.seed</span>(<span class="dv">1733</span>)</span>
<span id="cb251-2"><a href="model-comparison-and-regularization.html#cb251-2"></a><span class="co"># Use the `map` function to run different polynomials, and use the `rerun`</span></span>
<span id="cb251-3"><a href="model-comparison-and-regularization.html#cb251-3"></a><span class="co"># function run the deviance 100 times. The code below runs `deviance_kidiq` by</span></span>
<span id="cb251-4"><a href="model-comparison-and-regularization.html#cb251-4"></a><span class="co"># randomly sampling 30 training samples 100 times, and compute the in-sample and</span></span>
<span id="cb251-5"><a href="model-comparison-and-regularization.html#cb251-5"></a><span class="co"># out-of-sample deviance for each.</span></span>
<span id="cb251-6"><a href="model-comparison-and-regularization.html#cb251-6"></a><span class="co"># rerun(100, deviance_kidiq(degree = 1, train = 30L)) %&gt;% </span></span>
<span id="cb251-7"><a href="model-comparison-and-regularization.html#cb251-7"></a><span class="co">#   bind_rows()</span></span>
<span id="cb251-8"><a href="model-comparison-and-regularization.html#cb251-8"></a><span class="co"># Now run 1 to 8 degree polynomial, each 1000 times:</span></span>
<span id="cb251-9"><a href="model-comparison-and-regularization.html#cb251-9"></a>dev_df &lt;-<span class="st"> </span><span class="kw">map_df</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, </span>
<span id="cb251-10"><a href="model-comparison-and-regularization.html#cb251-10"></a>                 <span class="op">~</span><span class="st"> </span><span class="kw">rerun</span>(<span class="dv">1000</span>, <span class="kw">deviance_kidiq</span>(<span class="dt">degree =</span> .x, <span class="dt">train =</span> 60L)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb251-11"><a href="model-comparison-and-regularization.html#cb251-11"></a><span class="st">                   </span>bind_rows)</span>
<span id="cb251-12"><a href="model-comparison-and-regularization.html#cb251-12"></a><span class="co"># Plot the results</span></span>
<span id="cb251-13"><a href="model-comparison-and-regularization.html#cb251-13"></a>dev_df <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb251-14"><a href="model-comparison-and-regularization.html#cb251-14"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> degree, <span class="dt">y =</span> deviance, <span class="dt">col =</span> sample)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb251-15"><a href="model-comparison-and-regularization.html#cb251-15"></a><span class="st">  </span><span class="kw">stat_summary</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb251-16"><a href="model-comparison-and-regularization.html#cb251-16"></a><span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>)</span></code></pre></div>
<pre><code>&gt;# No summary function supplied, defaulting to `mean_se()`
&gt;# No summary function supplied, defaulting to `mean_se()`</code></pre>
<p><img src="09_model_comparison_files/figure-html/dev_df-plot-1.png" width="480" /></p>
<p>As you can see, the in-sample deviance (red line) keeps decreasing, indicating
that a more complex model fit the data better, which is always the case. So if
one were to use deviance to determine what model is optimal, one will always
choose a model that is most complex, just like using <span class="math inline">\(R^2\)</span> (indeed, for linear
models deviance is basically the same as <span class="math inline">\(R^2\)</span>).</p>
<p>Now, look at the blue line, which represents the deviance computed using the
coefficients obtained from the training set but applied to the remaining data.
As you can see, the deviance achieves its minimum around the linear and the
quadratic model, and starts to increase, meaning that more complex model does
not fit the hold out data.</p>
<p>A statistical model is used to learn something from a data set that can
generalize to other observations. Therefore, we should care about the blue line,
instead of the red one. The indices you will see in the remaining of this note
are all attempts to approximate the blue line.</p>
<blockquote>
<p>More complex models always fit the current data better, but may not generalize
to other data. In other words, models that are too complex are not
generalizable.</p>
</blockquote>
</div>
</div>
<div id="information-criteria-1" class="section level2" number="9.4">
<h2><span class="header-section-number">9.4</span> Information Criteria</h2>
<div id="akaike-information-criteria-aic" class="section level3" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Akaike Information Criteria (AIC)</h3>
<p>Multiplying the quantity of elpd - <span class="math inline">\(p\)</span> by <span class="math inline">\(-2\)</span>, or deviance + 2<span class="math inline">\(p\)</span>, with the
deviance obtained using the maximum likelihood estimates (MLEs) for the
parameters, gives you exactly the formula for AIC:</p>
<p><span class="math display">\[\textrm{AIC} = D(\hat \theta) + 2p,\]</span></p>
<p>and <span class="math inline">\(p\)</span> in AIC is taken to be just the number of parameters. As we have
multiplied by a negative number, maximizing the estimate of elpd is equivalent
to minimizing the AIC, so one would prefer a model with the smallest AIC.</p>
<p>The approximation of AIC works best when the probability distribution under
the <span class="math inline">\(M_1\)</span> is normal, and that the sample size is much larger than the number of
parameters. It is nothing Bayesian because there is no posterior distributions
used, as <span class="math inline">\(D\)</span> is computed only based on the MLE. Also, it does not take into
account any prior information.</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="model-comparison-and-regularization.html#cb253-1"></a><span class="co"># Frequentist model</span></span>
<span id="cb253-2"><a href="model-comparison-and-regularization.html#cb253-2"></a>m1_freq &lt;-<span class="st"> </span><span class="kw">lm</span>(m1<span class="op">$</span>formula, <span class="dt">data =</span> m1<span class="op">$</span>data)</span>
<span id="cb253-3"><a href="model-comparison-and-regularization.html#cb253-3"></a><span class="kw">AIC</span>(m1_freq)</span></code></pre></div>
<pre><code>&gt;# [1] -240</code></pre>
</div>
<div id="deviance-information-criteria-dic" class="section level3" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> Deviance Information Criteria (DIC)</h3>
<p>The definition of AIC assumes that the parameter estimates are known or are
maximum likelihood estimates. The DIC, instead, replaces those with the
posterior distribution of the parameters. The general formula for DIC is</p>
<p><span class="math display">\[\textrm{DIC} = \mathrm{E}(D | \boldsymbol{\mathbf{y}}) + 2 p_D,\]</span></p>
<p>where <span class="math inline">\(p_D\)</span> is the effective number of parameters estimated in the Markov chain.
Although DIC does take into account the prior distributions, and <span class="math inline">\(\mathrm{E}(D | \boldsymbol{\mathbf{y}})\)</span> is based on a posterior distribution, it still works best when the posterior
distributions are multivariate normal, and that <span class="math inline">\(N \gg p\)</span>.</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="model-comparison-and-regularization.html#cb255-1"></a><span class="co"># Function to compute DIC</span></span>
<span id="cb255-2"><a href="model-comparison-and-regularization.html#cb255-2"></a>dic_brmsfit &lt;-<span class="st"> </span><span class="cf">function</span>(object) {</span>
<span id="cb255-3"><a href="model-comparison-and-regularization.html#cb255-3"></a>  Dbar &lt;-<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(<span class="kw">rowSums</span>(<span class="kw">log_lik</span>(object)))</span>
<span id="cb255-4"><a href="model-comparison-and-regularization.html#cb255-4"></a>  coef_pmean &lt;-<span class="st"> </span><span class="kw">unname</span>(<span class="kw">fixef</span>(m1)[ , <span class="st">&quot;Estimate&quot;</span>])</span>
<span id="cb255-5"><a href="model-comparison-and-regularization.html#cb255-5"></a>  X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="kw">as.formula</span>(object<span class="op">$</span>formula), object<span class="op">$</span>data)</span>
<span id="cb255-6"><a href="model-comparison-and-regularization.html#cb255-6"></a>  res &lt;-<span class="st"> </span>res &lt;-<span class="st"> </span><span class="kw">residuals</span>(m1)[ , <span class="st">&quot;Estimate&quot;</span>]</span>
<span id="cb255-7"><a href="model-comparison-and-regularization.html#cb255-7"></a>  N &lt;-<span class="st"> </span><span class="kw">length</span>(res)</span>
<span id="cb255-8"><a href="model-comparison-and-regularization.html#cb255-8"></a>  sigma &lt;-<span class="st"> </span><span class="kw">posterior_summary</span>(m1, <span class="dt">pars =</span> <span class="st">&quot;sigma&quot;</span>)[ , <span class="st">&quot;Estimate&quot;</span>]</span>
<span id="cb255-9"><a href="model-comparison-and-regularization.html#cb255-9"></a>  Dhat &lt;-<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(res, <span class="dt">sd =</span> sigma, <span class="dt">log =</span> <span class="ot">TRUE</span>))</span>
<span id="cb255-10"><a href="model-comparison-and-regularization.html#cb255-10"></a>  p &lt;-<span class="st"> </span>Dbar <span class="op">-</span><span class="st"> </span>Dhat</span>
<span id="cb255-11"><a href="model-comparison-and-regularization.html#cb255-11"></a>  elpd &lt;-<span class="st"> </span>Dhat <span class="op">/</span><span class="st"> </span><span class="dv">-2</span> <span class="op">-</span><span class="st"> </span>p</span>
<span id="cb255-12"><a href="model-comparison-and-regularization.html#cb255-12"></a>  <span class="kw">data.frame</span>(<span class="dt">elpd_dic =</span> elpd, <span class="dt">p_dic =</span> p, <span class="dt">dic =</span> Dhat <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p, </span>
<span id="cb255-13"><a href="model-comparison-and-regularization.html#cb255-13"></a>             <span class="dt">row.names =</span> <span class="st">&quot;Estimate&quot;</span>)</span>
<span id="cb255-14"><a href="model-comparison-and-regularization.html#cb255-14"></a>}</span>
<span id="cb255-15"><a href="model-comparison-and-regularization.html#cb255-15"></a><span class="kw">dic_brmsfit</span>(m1)</span></code></pre></div>
<pre><code>&gt;#          elpd_dic p_dic  dic
&gt;# Estimate      120  3.01 -240</code></pre>
</div>
<div id="watanabe-akaike-information-criteria-waic" class="section level3" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> Watanabe-Akaike Information Criteria (WAIC)</h3>
<p>A further modification has been proposed to use the
<em>log pointwise posterior predictive density</em>, with the effective number of
parameters computed using the posterior variance of the likelihood.</p>
<p><span class="math display">\[\textrm{WAIC} = -2 \sum_{i = 1}^n \log \mathrm{E}[p(y_i | \boldsymbol{\mathbf{\theta}}, \boldsymbol{\mathbf{y}})] + 
                  2 p_\textrm{WAIC},\]</span></p>
<p>where <span class="math inline">\(\mathrm{E}[p(y_i | \boldsymbol{\mathbf{\theta}}, \boldsymbol{\mathbf{y}})]\)</span> is the posterior mean of the likelihood
of the <span class="math inline">\(i\)</span>th observation. The WAIC incorporates prior information, and the use
of pointwise likelihood makes it more robust when the posterior distributions
deviate from normality. In general, WAIC is a better estimate of the
out-of-sample deviance than AIC and DIC.</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="model-comparison-and-regularization.html#cb257-1"></a><span class="kw">waic</span>(m1)  <span class="co"># built-in function in brms</span></span></code></pre></div>
<pre><code>&gt;# 
&gt;# Computed from 4000 by 434 log-likelihood matrix
&gt;# 
&gt;#           Estimate   SE
&gt;# elpd_waic    120.0 14.5
&gt;# p_waic         2.9  0.3
&gt;# waic        -240.1 28.9</code></pre>
</div>
<div id="leave-one-out-cross-validation" class="section level3" number="9.4.4">
<h3><span class="header-section-number">9.4.4</span> Leave-One-Out Cross Validation</h3>
<p>The idea of cross-validation is to split the sample so that it imitates the
scenario of estimating the parameters in part of the data and predicting the
remaining part. The part that is used for estimation is called the <em>training
set</em>, and the part that is used for prediction is called the <em>validation set</em>.
Leave-one-out information criteria (LOO-IC) means that one uses <span class="math inline">\(N - 1\)</span>
observations as the training set and 1 observation as the validation sample,
repeat the process <span class="math inline">\(N\)</span> times so that each time a different observation is being
predicted, and adding up the prediction results will give an estimate of elpd
that closely approximates the results that would be obtained by collecting new
data and doing the validation. To make it more concrete, we can go back to the
<code>kidiq</code> data with <code>mom_iq</code> predicting <code>kid_score</code>. We can do this for case #286,
as an example:</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="model-comparison-and-regularization.html#cb259-1"></a><span class="co"># Estimate the model without case #286</span></span>
<span id="cb259-2"><a href="model-comparison-and-regularization.html#cb259-2"></a>m1_no286 &lt;-<span class="st"> </span><span class="kw">update</span>(m1, <span class="dt">newdata =</span> kidiq100[<span class="op">-</span><span class="dv">286</span>, ])</span></code></pre></div>
<pre><code>&gt;# Start sampling</code></pre>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb261-1"><a href="model-comparison-and-regularization.html#cb261-1"></a><span class="co"># The log predictive density for case #286</span></span>
<span id="cb261-2"><a href="model-comparison-and-regularization.html#cb261-2"></a><span class="kw">mean</span>(<span class="kw">log_lik</span>(m1_no286, <span class="dt">newdata =</span> kidiq100[<span class="dv">286</span>, ]))</span></code></pre></div>
<pre><code>&gt;# [1] -4.2</code></pre>
<p>Because LOO-IC requires fitting the model <span class="math inline">\(N\)</span> times, it is generally very
computational intensive. There are, however, shortcuts for some common models
that make it computed faster. Otherwise, WAIC can be treated as a fast
approximation of LOO-IC, although LOO-IC is more robust and will be a better
estimate of out-of-sample deviance. In STAN, it uses the so called Pareto
smoothed importance sampling (PSIS) to make the process faster, without having
to repeat the process <span class="math inline">\(N\)</span> times.</p>
<p>Here is the LOO-IC for the model:</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="model-comparison-and-regularization.html#cb263-1"></a><span class="kw">loo</span>(m1)</span></code></pre></div>
<pre><code>&gt;# 
&gt;# Computed from 4000 by 434 log-likelihood matrix
&gt;# 
&gt;#          Estimate   SE
&gt;# elpd_loo    120.0 14.5
&gt;# p_loo         2.9  0.3
&gt;# looic      -240.1 28.9
&gt;# ------
&gt;# Monte Carlo SE of elpd_loo is 0.0.
&gt;# 
&gt;# All Pareto k estimates are good (k &lt; 0.5).
&gt;# See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>You can save the WAIC and the LOO-IC information to the fitted result:</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="model-comparison-and-regularization.html#cb265-1"></a>m1 &lt;-<span class="st"> </span><span class="kw">add_criterion</span>(m1, <span class="kw">c</span>(<span class="st">&quot;loo&quot;</span>, <span class="st">&quot;waic&quot;</span>))</span></code></pre></div>
<p>See <span class="citation">Vehtari, Gelman, and Gabry (<a href="#ref-Vehtari2016" role="doc-biblioref">2016</a>)</span> for more discussions on WAIC and LOO-IC.</p>
<hr />
</div>
<div id="example" class="section level3" number="9.4.5">
<h3><span class="header-section-number">9.4.5</span> Example</h3>
<p>Consider four potential models in predicting <code>kid_score</code>:</p>
<p><span class="math display">\[\texttt{kidscore}_i \sim \mathcal{N}(\mu_i, \sigma)\]</span></p>
<p><span class="math display">\[\begin{align*}
  \mu_i &amp; = \beta_0 + \beta_1 (\texttt{mom_iq}_i)  \\
  \mu_i &amp; = \beta_0 + \beta_1 (\texttt{mom_iq}_i) +
            \beta_2 (\texttt{mom_hs}_i) \\
  \mu_i &amp; = \beta_0 + \beta_1 (\texttt{mom_iq}_i) + 
            \beta_2 (\texttt{mom_hs}_i) + \beta_3 (\texttt{mom_iq}_i \times 
                                                    \texttt{mom_hs}_i)  \\
  \mu_i &amp; = \beta_0 + \beta_1 (\texttt{mom_iq}_i) + 
            \beta_2 (\texttt{mom_hs}_i) + 
            \beta_3 (\texttt{mom_iq}_i \times \texttt{mom_hs}_i) + 
            \beta_4 (\texttt{mom_age}_i)
\end{align*}\]</span></p>
<p>The first model only has <code>mom_iq</code> as a predictor, which is equivalent to
saying that the coefficients for <code>mom_hs</code> and <code>mom_age</code> are zero. The second
model added <code>mom_hs</code> as a predictor. The third model includes an additional
interaction term, whereas the fourth model also include <code>mom_age</code>. Now,
we can compare the four models:</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="model-comparison-and-regularization.html#cb266-1"></a><span class="kw">loo_compare</span>(m1, m2, m3, m4)</span></code></pre></div>
<pre><code>&gt;#    elpd_diff se_diff
&gt;# m3  0.0       0.0   
&gt;# m4 -0.4       1.1   
&gt;# m2 -3.4       2.5   
&gt;# m1 -6.0       3.9</code></pre>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="model-comparison-and-regularization.html#cb268-1"></a><span class="co"># m3 is the best</span></span></code></pre></div>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="model-comparison-and-regularization.html#cb269-1"></a><span class="co"># show details</span></span>
<span id="cb269-2"><a href="model-comparison-and-regularization.html#cb269-2"></a><span class="kw">loo</span>(m1, m2, m3, m4)</span></code></pre></div>
<pre><code>&gt;# Output of model &#39;m1&#39;:
&gt;# 
&gt;# Computed from 4000 by 434 log-likelihood matrix
&gt;# 
&gt;#          Estimate   SE
&gt;# elpd_loo    120.0 14.5
&gt;# p_loo         2.9  0.3
&gt;# looic      -240.1 28.9
&gt;# ------
&gt;# Monte Carlo SE of elpd_loo is 0.0.
&gt;# 
&gt;# All Pareto k estimates are good (k &lt; 0.5).
&gt;# See help(&#39;pareto-k-diagnostic&#39;) for details.
&gt;# 
&gt;# Output of model &#39;m2&#39;:
&gt;# 
&gt;# Computed from 4000 by 434 log-likelihood matrix
&gt;# 
&gt;#          Estimate   SE
&gt;# elpd_loo    122.6 14.2
&gt;# p_loo         4.0  0.4
&gt;# looic      -245.2 28.4
&gt;# ------
&gt;# Monte Carlo SE of elpd_loo is 0.0.
&gt;# 
&gt;# All Pareto k estimates are good (k &lt; 0.5).
&gt;# See help(&#39;pareto-k-diagnostic&#39;) for details.
&gt;# 
&gt;# Output of model &#39;m3&#39;:
&gt;# 
&gt;# Computed from 4000 by 434 log-likelihood matrix
&gt;# 
&gt;#          Estimate   SE
&gt;# elpd_loo    126.0 14.3
&gt;# p_loo         4.9  0.5
&gt;# looic      -252.0 28.7
&gt;# ------
&gt;# Monte Carlo SE of elpd_loo is 0.0.
&gt;# 
&gt;# All Pareto k estimates are good (k &lt; 0.5).
&gt;# See help(&#39;pareto-k-diagnostic&#39;) for details.
&gt;# 
&gt;# Output of model &#39;m4&#39;:
&gt;# 
&gt;# Computed from 4000 by 434 log-likelihood matrix
&gt;# 
&gt;#          Estimate   SE
&gt;# elpd_loo    125.7 14.4
&gt;# p_loo         5.9  0.6
&gt;# looic      -251.3 28.8
&gt;# ------
&gt;# Monte Carlo SE of elpd_loo is 0.0.
&gt;# 
&gt;# All Pareto k estimates are good (k &lt; 0.5).
&gt;# See help(&#39;pareto-k-diagnostic&#39;) for details.
&gt;# 
&gt;# Model comparisons:
&gt;#    elpd_diff se_diff
&gt;# m3  0.0       0.0   
&gt;# m4 -0.4       1.1   
&gt;# m2 -3.4       2.5   
&gt;# m1 -6.0       3.9</code></pre>
<p>Model 3 has the lowest LOO-IC, although if you compare the difference in LOO-IC
between Model 3 and Model 4 and the corresponding standard errors (in the column
<code>se_diff</code>), the difference is relatively small. Given that Model 3 achieves the
smallest LOO-IC and is simpler than Model 4, one may conclude that Model 3 is
the best model among the four.</p>
<hr />
</div>
</div>
<div id="stackingmodel-averaging" class="section level2" number="9.5">
<h2><span class="header-section-number">9.5</span> Stacking/Model Averaging</h2>
<p>Sometimes it may not be a good practice to only choose one model with low WAIC
or LOO-IC, especially when several models have very similar WAIC/LOO-IC, but
they make somewhat different predictions. Instead, we can perform <em>stacking</em> or
<em>model averaging</em> by weighting the <em>predictions</em> from multiple models, using
weights that are based on their information criteria performance. Stacking
approaches this by optimizing the leave-one-out mean squared error in the
resulting prediction, whereas model averaging preserves the uncertainty and was
not optimized for that task. The technical details can be found in <span class="citation">Yao et al. (<a href="#ref-Yao2018" role="doc-biblioref">2018</a>)</span>.</p>
<p>Note that the conventional Bayesian model averaging used the posterior model
probability <span class="citation">(Hoeting et al. <a href="#ref-hoeting1999bayesian" role="doc-biblioref">1999</a>)</span>, which are approximated by the BIC. The
discussion in this note is based on more recent discussion in, e.g., <span class="citation">Yao et al. (<a href="#ref-Yao2018" role="doc-biblioref">2018</a>)</span>.</p>
<p>Let’s run four models on some training data by randomly splitting the data into
half. First rescale some of the variables:</p>
<!-- ```{r kidiq100, echo=TRUE} -->
<!-- kidiq <- haven::read_dta("../data/kidiq.dta") -->
<!-- kidiq100 <- kidiq %>%  -->
<!--   mutate(mom_iq = mom_iq / 100,  # divid mom_iq by 100 -->
<!--          kid_score = kid_score / 100,   # divide kid_score by 100 -->
<!--          mom_iq_c = mom_iq - 1,  -->
<!--          mom_hs = factor(mom_hs, labels = c("no", "yes")),  -->
<!--          mom_age_c = (mom_age - 18) / 10) -->
<!-- ``` -->
<p>I will run four models, which is from the last note</p>
<p><span class="math display">\[\texttt{kidscore}_i \sim \mathcal{N}(\mu_i, \sigma)\]</span></p>
<p><span class="math display">\[\begin{align*}
  \mu_i &amp; = \beta_0 + \beta_1 (\texttt{mom_iq}_i)  \\
  \mu_i &amp; = \beta_0 + \beta_1 (\texttt{mom_iq}_i) +
            \beta_2 (\texttt{mom_hs}_i) \\
  \mu_i &amp; = \beta_0 + \beta_1 (\texttt{mom_iq}_i) + 
            \beta_2 (\texttt{mom_hs}_i) + \beta_3 (\texttt{mom_iq}_i \times 
                                                    \texttt{mom_hs}_i)  \\
  \mu_i &amp; = \beta_0 + \beta_1 (\texttt{mom_iq}_i) + 
            \beta_2 (\texttt{mom_hs}_i) + 
            \beta_3 (\texttt{mom_iq}_i \times \texttt{mom_hs}_i) + 
            \beta_4 (\texttt{mom_age}_i)
\end{align*}\]</span></p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="model-comparison-and-regularization.html#cb271-1"></a><span class="co"># mom_iq with centering</span></span>
<span id="cb271-2"><a href="model-comparison-and-regularization.html#cb271-2"></a>m1 &lt;-<span class="st"> </span><span class="kw">brm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_iq_c, <span class="dt">data =</span> kidiq100, </span>
<span id="cb271-3"><a href="model-comparison-and-regularization.html#cb271-3"></a>          <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>), </span>
<span id="cb271-4"><a href="model-comparison-and-regularization.html#cb271-4"></a>                    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>), </span>
<span id="cb271-5"><a href="model-comparison-and-regularization.html#cb271-5"></a>                    <span class="kw">prior</span>(<span class="kw">student_t</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;sigma&quot;</span>)), </span>
<span id="cb271-6"><a href="model-comparison-and-regularization.html#cb271-6"></a>          <span class="dt">seed =</span> <span class="dv">2302</span>, </span>
<span id="cb271-7"><a href="model-comparison-and-regularization.html#cb271-7"></a>          <span class="dt">chains =</span> 2L, <span class="dt">cores =</span> 2L</span>
<span id="cb271-8"><a href="model-comparison-and-regularization.html#cb271-8"></a>)</span>
<span id="cb271-9"><a href="model-comparison-and-regularization.html#cb271-9"></a>m1 &lt;-<span class="st"> </span><span class="kw">add_criterion</span>(m1, <span class="kw">c</span>(<span class="st">&quot;loo&quot;</span>, <span class="st">&quot;waic&quot;</span>))</span>
<span id="cb271-10"><a href="model-comparison-and-regularization.html#cb271-10"></a><span class="co"># Use `update` will sometimes avoid recompiling</span></span>
<span id="cb271-11"><a href="model-comparison-and-regularization.html#cb271-11"></a>m2 &lt;-<span class="st"> </span><span class="kw">update</span>(m1, kid_score <span class="op">~</span><span class="st"> </span>mom_iq_c <span class="op">+</span><span class="st"> </span>mom_hs, <span class="dt">newdata =</span> kidiq100)</span>
<span id="cb271-12"><a href="model-comparison-and-regularization.html#cb271-12"></a>m2 &lt;-<span class="st"> </span><span class="kw">add_criterion</span>(m2, <span class="kw">c</span>(<span class="st">&quot;loo&quot;</span>, <span class="st">&quot;waic&quot;</span>))</span>
<span id="cb271-13"><a href="model-comparison-and-regularization.html#cb271-13"></a>m3 &lt;-<span class="st"> </span><span class="kw">update</span>(m2, kid_score <span class="op">~</span><span class="st"> </span>mom_iq_c <span class="op">*</span><span class="st"> </span>mom_hs, </span>
<span id="cb271-14"><a href="model-comparison-and-regularization.html#cb271-14"></a>             <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>, </span>
<span id="cb271-15"><a href="model-comparison-and-regularization.html#cb271-15"></a>                             <span class="dt">coef =</span> <span class="st">&quot;mom_iq_c:mom_hsyes&quot;</span>))</span>
<span id="cb271-16"><a href="model-comparison-and-regularization.html#cb271-16"></a>)</span>
<span id="cb271-17"><a href="model-comparison-and-regularization.html#cb271-17"></a>m3 &lt;-<span class="st"> </span><span class="kw">add_criterion</span>(m3, <span class="kw">c</span>(<span class="st">&quot;loo&quot;</span>, <span class="st">&quot;waic&quot;</span>))</span>
<span id="cb271-18"><a href="model-comparison-and-regularization.html#cb271-18"></a>m4 &lt;-<span class="st"> </span><span class="kw">update</span>(m3, kid_score <span class="op">~</span><span class="st"> </span>mom_iq_c <span class="op">*</span><span class="st"> </span>mom_hs <span class="op">+</span><span class="st"> </span>mom_age_c, <span class="dt">newdata =</span> kidiq100)</span>
<span id="cb271-19"><a href="model-comparison-and-regularization.html#cb271-19"></a>m4 &lt;-<span class="st"> </span><span class="kw">add_criterion</span>(m4, <span class="kw">c</span>(<span class="st">&quot;loo&quot;</span>, <span class="st">&quot;waic&quot;</span>))</span></code></pre></div>
<div id="model-weights" class="section level3" number="9.5.1">
<h3><span class="header-section-number">9.5.1</span> Model Weights</h3>
<p>We have seen that <code>m3</code> and <code>m4</code> gave the best LOO-IC and WAIC:</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="model-comparison-and-regularization.html#cb272-1"></a><span class="kw">loo_compare</span>(m1, m2, m3, m4)</span></code></pre></div>
<pre><code>&gt;#    elpd_diff se_diff
&gt;# m3  0.0       0.0   
&gt;# m4 -0.6       1.2   
&gt;# m1 -6.2       3.9   
&gt;# m2 -6.2       3.9</code></pre>
<p>So it makes sense that if we’re to assign weights, <code>m3</code> should get most weights.
Let’s check the following:</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="model-comparison-and-regularization.html#cb274-1"></a><span class="co"># Weights based on WAIC</span></span>
<span id="cb274-2"><a href="model-comparison-and-regularization.html#cb274-2"></a>waic_wts &lt;-<span class="st"> </span><span class="kw">model_weights</span>(m1, m2, m3, m4, <span class="dt">weights =</span> <span class="st">&quot;waic&quot;</span>)</span>
<span id="cb274-3"><a href="model-comparison-and-regularization.html#cb274-3"></a><span class="co"># Weights based on Pseudo-BMA (with Bayesian bootstrap)</span></span>
<span id="cb274-4"><a href="model-comparison-and-regularization.html#cb274-4"></a>pbma_wts &lt;-<span class="st"> </span><span class="kw">loo_model_weights</span>(m1, m2, m3, m4, <span class="dt">method =</span> <span class="st">&quot;pseudobma&quot;</span>)</span>
<span id="cb274-5"><a href="model-comparison-and-regularization.html#cb274-5"></a><span class="co"># Print out the weights</span></span>
<span id="cb274-6"><a href="model-comparison-and-regularization.html#cb274-6"></a><span class="kw">round</span>(<span class="kw">cbind</span>(waic_wts, pbma_wts), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>&gt;#    waic_wts pbma_wts
&gt;# m1    0.001    0.049
&gt;# m2    0.001    0.073
&gt;# m3    0.641    0.522
&gt;# m4    0.356    0.356</code></pre>
<p>You can see <code>m3</code> would get the highest weight, but it’s only 0.641 and
thus less than half of the weights when all four models are considered together.</p>
<p>In Bayesian, we want to preserve all the uncertainty in our analyses. Therefore,
if we’re not certain which models to use and have tried multiple ones, it would
make sense to use all of them to get the best information. So unlike what is
commonly done in practice where a researcher would test multiple models and
present the best model <em>as if</em> they intended only to test this model, Bayesian
analysts should do the honest thing and use all models. The reward is usually
better prediction!</p>
</div>
<div id="model-averaging" class="section level3" number="9.5.2">
<h3><span class="header-section-number">9.5.2</span> Model Averaging</h3>
<p>I will not go deep into averaging, as there are many ways to do it, but at this
moment it requires some programming to perform averaging with <code>STAN</code> and <code>brms</code>.
Averaging highlight the Bayesian spirit of incorporating all information for
prediction and propagating the uncertainty, which is a key element that unifies
a lot of Bayesian methods.</p>
<p>In STAN, currently it implements the pseudo-Bayesian Model Averaging (BMA) with
Bayesian bootstrap</p>
<p>I’ve written a very basic averaging function, <code>bma_brm_lm</code> for <code>brms</code> with
linear models. Whereas averaging of predictions can be done for any models,
generally it only makes sense to average the coefficients
Here is an example of using it for the four models and output posterior draws
of the parameters that are weighted averages from the original models.</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="model-comparison-and-regularization.html#cb276-1"></a><span class="kw">source</span>(<span class="st">&quot;pbma_brm_lm.R&quot;</span>)</span>
<span id="cb276-2"><a href="model-comparison-and-regularization.html#cb276-2"></a>pbma_draws &lt;-<span class="st"> </span><span class="kw">pbma_brm_lm</span>(m1, m2, m3, m4)</span></code></pre></div>
<pre><code>&gt;# Method: pseudo-BMA+ with Bayesian bootstrap
&gt;# ------
&gt;#    weight
&gt;# m1 0.048 
&gt;# m2 0.074 
&gt;# m3 0.528 
&gt;# m4 0.349</code></pre>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="model-comparison-and-regularization.html#cb278-1"></a><span class="co"># Coefficients for pseudo-BMA</span></span>
<span id="cb278-2"><a href="model-comparison-and-regularization.html#cb278-2"></a><span class="kw">posterior_summary</span>(pbma_draws)</span></code></pre></div>
<pre><code>&gt;#                    Estimate Est.Error     Q2.5   Q97.5
&gt;# Intercept            0.8442    0.0145  0.81585  0.8718
&gt;# mom_iq_c             0.8791    0.0878  0.71526  1.0589
&gt;# mom_hsyes            0.0311    0.0153  0.00146  0.0612
&gt;# mom_iq_c:mom_hsyes  -0.3783    0.0952 -0.56835 -0.1972
&gt;# mom_age_c            0.0120    0.0114 -0.00945  0.0344</code></pre>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="model-comparison-and-regularization.html#cb280-1"></a><span class="co"># Coefficients for M3</span></span>
<span id="cb280-2"><a href="model-comparison-and-regularization.html#cb280-2"></a><span class="kw">fixef</span>(m3)</span></code></pre></div>
<pre><code>&gt;#                    Estimate Est.Error    Q2.5   Q97.5
&gt;# Intercept             0.849    0.0213  0.8092  0.8912
&gt;# mom_iq_c              0.914    0.1386  0.6580  1.1873
&gt;# mom_hsyes             0.033    0.0235 -0.0119  0.0763
&gt;# mom_iq_c:mom_hsyes   -0.422    0.1524 -0.7179 -0.1390</code></pre>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="model-comparison-and-regularization.html#cb282-1"></a><span class="co"># Coefficients for M4</span></span>
<span id="cb282-2"><a href="model-comparison-and-regularization.html#cb282-2"></a><span class="kw">fixef</span>(m4)</span></code></pre></div>
<pre><code>&gt;#                    Estimate Est.Error   Q2.5   Q97.5
&gt;# Intercept            0.8383    0.0246  0.790  0.8860
&gt;# mom_iq_c             0.9312    0.1430  0.662  1.2208
&gt;# mom_hsyes            0.0265    0.0243 -0.022  0.0733
&gt;# mom_age_c            0.0344    0.0325 -0.027  0.0985
&gt;# mom_iq_c:mom_hsyes  -0.4448    0.1541 -0.766 -0.1502</code></pre>
<p>As you can see, the coefficients from the pseudo-BMA is smaller (i.e., being
shrunk to closer to zero) as compared to <code>m3</code> and <code>m4</code>. However, we also had
a smaller posterior <em>SD</em> of the estimates. Simulation studies have generally
shown that the prediction based on BMA tends to outperform many other methods,
especially when overfitting is suspected to be a problem.</p>
<!-- ### Predictive Errors -->
<!-- Let's check the prediction error for the individual models: -->
<!-- ```{r rmse-m1-m2-m3-m4} -->
<!-- # Define function to compute RMSE -->
<!-- rmse_validate <- function(brmsfit, newdata = kidiq100[-train, ]) { -->
<!--   res_brm <- residuals(brmsfit, newdata = newdata) -->
<!--   sqrt(mean(res_brm[ , "Estimate"]^2)) -->
<!-- } -->
<!-- tibble(model = c("m1", "m2", "m3", "m4"),  -->
<!--        RMSE = map_dbl(list(m1, m2, m3, m4), rmse_validate)) -->
<!-- ``` -->
<!-- In this case, `m3` had the best prediction error on the test data. -->
</div>
<div id="stacking" class="section level3" number="9.5.3">
<h3><span class="header-section-number">9.5.3</span> Stacking</h3>
<p>Stacking is another way to combine the predictions of different models. The
technical details can be found in <span class="citation">Yao et al. (<a href="#ref-Yao2018" role="doc-biblioref">2018</a>)</span>, but you can obtain the predictions
using the <code>pp_average</code> function:</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="model-comparison-and-regularization.html#cb284-1"></a><span class="co"># Prediction from stacking by Yao et al. (2018)</span></span>
<span id="cb284-2"><a href="model-comparison-and-regularization.html#cb284-2"></a>pred_stacking &lt;-<span class="st"> </span><span class="kw">pp_average</span>(m1, m2, m3, m4, <span class="dt">method =</span> <span class="st">&quot;predict&quot;</span>)</span>
<span id="cb284-3"><a href="model-comparison-and-regularization.html#cb284-3"></a><span class="co"># Prediction from pseudo BMA</span></span>
<span id="cb284-4"><a href="model-comparison-and-regularization.html#cb284-4"></a><span class="co"># 1. Obtain predictions from each model</span></span>
<span id="cb284-5"><a href="model-comparison-and-regularization.html#cb284-5"></a>pred_m1234 &lt;-<span class="st"> </span><span class="kw">map</span>(<span class="kw">list</span>(m1, m2, m3, m4), posterior_predict)</span>
<span id="cb284-6"><a href="model-comparison-and-regularization.html#cb284-6"></a><span class="co"># 2. Obtain model weights (pbma_wts as previously obtained)</span></span>
<span id="cb284-7"><a href="model-comparison-and-regularization.html#cb284-7"></a><span class="co"># 3. Obtain weighted predictions</span></span>
<span id="cb284-8"><a href="model-comparison-and-regularization.html#cb284-8"></a>pred_pbma &lt;-<span class="st"> </span><span class="kw">map2</span>(pred_m1234, pbma_wts, <span class="st">`</span><span class="dt">*</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb284-9"><a href="model-comparison-and-regularization.html#cb284-9"></a><span class="st">  </span><span class="kw">reduce</span>(<span class="st">`</span><span class="dt">+</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb284-10"><a href="model-comparison-and-regularization.html#cb284-10"></a><span class="st">  </span><span class="kw">posterior_summary</span>()</span>
<span id="cb284-11"><a href="model-comparison-and-regularization.html#cb284-11"></a><span class="co"># Compare the weights</span></span>
<span id="cb284-12"><a href="model-comparison-and-regularization.html#cb284-12"></a><span class="kw">ggplot</span>(<span class="kw">tibble</span>(<span class="dt">stacking =</span> pred_stacking[ , <span class="st">&quot;Estimate&quot;</span>], </span>
<span id="cb284-13"><a href="model-comparison-and-regularization.html#cb284-13"></a>              <span class="dt">pbma =</span> pred_pbma[ , <span class="st">&quot;Estimate&quot;</span>]), <span class="kw">aes</span>(<span class="dt">x =</span> pbma, <span class="dt">y =</span> stacking)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb284-14"><a href="model-comparison-and-regularization.html#cb284-14"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb284-15"><a href="model-comparison-and-regularization.html#cb284-15"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="09_model_comparison_files/figure-html/pred-stacking-pbma-1.png" width="672" /></p>
<p>As can be seen, in this example the two methods give very similar predictions.</p>
<div id="prediction-example" class="section level4" number="9.5.3.1">
<h4><span class="header-section-number">9.5.3.1</span> Prediction example</h4>
<p>Consider a kid whose mother’s IQ is 120 (<code>mom_iq</code> = .2), mother’s age is 40,
(<code>mom_age_c</code> = 2.2), mother does not have a high school degree, and mother
did not work in first three years of child’s life (<code>mom_work</code> = 1). Then the
prediction based on the various models are:</p>
<p><img src="09_model_comparison_files/figure-html/newkid-predict-1.png" width="672" /></p>
<p>Check out this blog post <a href="https://mc-stan.org/loo/articles/loo2-weights.html" class="uri">https://mc-stan.org/loo/articles/loo2-weights.html</a> for
more information on stacking and BMA.</p>
</div>
</div>
</div>
<div id="shrinkage-priors" class="section level2" number="9.6">
<h2><span class="header-section-number">9.6</span> Shrinkage Priors</h2>
<p>When the number of parameters to be estimated is large relative to the amount of
data available, ordinary least square (in frequentist) and estimation using
non-informative or weakly informative priors tend to overfit. For example,
fitting a 6th degree polynomial (with 8 parameters) on a data set with only 10
observations will severely overfit the data, making the results not
generalizable. One way to avoid overfitting is to perform <em>regularization</em>, that
is, to shrink some of the parameters to closer to zero. This makes the model fit
less well to the existing data, but will be much more generalizable to an
independent data set.</p>
<div id="number-of-parameters" class="section level3" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> Number of parameters</h3>
<p>In Bayesian analyses, the concept of number of parameters is a little vague.
This is because the posterior distribution is a function of both the prior and
the data. For non-informative priors, it would make sense to simply count the
number of parameters. However, say one put a very strong prior on one of the
regression coefficients, which has about 9 times the weights of the information
contributed by the data:</p>
<pre><code>&gt;# Warning: `mapping` is not used by stat_function()

&gt;# Warning: `mapping` is not used by stat_function()</code></pre>
<p><img src="09_model_comparison_files/figure-html/prior-data-weight-1.png" width="672" /></p>
<p>Then the posterior for the parameter only uses 1/10 of the information from the
data! Therefore, it would make more sense to count this as 0.1 parameter,
instead of 1 full parameter.</p>
<p>The concept of regularization is essentially to introduce a stronger prior so
that the posterior is less likely to overfit the data, and the resulting model
will have lower <em>effective number of parameters</em>, which, when done
appropriately, would find a model that is more likely to generalize to external
data sets.</p>
<p>In Bayesian methods, regularization can be done by choosing a prior on the
coefficient that has a sharp peak at 0, but also has a heavy tail. One such
prior is what is called the <em>horseshoe</em> prior. The discussion here is based on
the blog pot by Michael Betancourt: <a href="https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html" class="uri">https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html</a></p>
<p>It should first be pointed out that these priors were based on the assumption
that the predictors and the outcome has been scaled to have a standard deviation
of one. So we will do this here:</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="model-comparison-and-regularization.html#cb286-1"></a><span class="co"># For variable selection, scale the predictor and outcome to have unit variance</span></span>
<span id="cb286-2"><a href="model-comparison-and-regularization.html#cb286-2"></a>kidiq_std &lt;-<span class="st"> </span><span class="kw">scale</span>(kidiq)</span>
<span id="cb286-3"><a href="model-comparison-and-regularization.html#cb286-3"></a><span class="kw">head</span>(kidiq_std)</span></code></pre></div>
<pre><code>&gt;#      kid_score mom_hs  mom_iq mom_work mom_age
&gt;# [1,]   -1.0679  0.522  1.4078   0.9342    1.56
&gt;# [2,]    0.5489  0.522 -0.7092   0.9342    0.82
&gt;# [3,]   -0.0881  0.522  1.0295   0.9342    1.56
&gt;# [4,]   -0.1860  0.522 -0.0367   0.0878    0.82
&gt;# [5,]    1.3818  0.522 -0.4836   0.9342    1.56
&gt;# [6,]    0.5489 -1.913  0.5268  -1.6051   -1.77</code></pre>
</div>
<div id="sparsity-inducing-priors" class="section level3" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> Sparsity-Inducing Priors</h3>
<p>The <em>horseshoe</em> prior <span class="citation">(Carvalho, Polson, and Scott <a href="#ref-carvalho2009" role="doc-biblioref">2009</a>)</span> is a type of hierarchical prior for
regression models by introducing a global scale, <span class="math inline">\(\tau\)</span>, and local scale,
<span class="math inline">\(\lambda_m\)</span>, parameters on the priors for the regression coefficients.
Specifically, with <span class="math inline">\(p\)</span> predictors,
<span class="math display">\[\begin{align*}
  Y_i &amp; \sim \mathcal{N}(\mu_i, \sigma^2) \\
  \mu_i &amp; = \beta_0 + \sum_{m = 1}^p \beta_m X_m \\
  \beta_0 &amp; \sim \mathcal{N}(0, 1) \\
  \beta_m &amp; \sim \mathcal{N}(0, \tau \lambda_m) \\
  \lambda_m &amp; \sim \textrm{Cauchy}^+(0, 1)  \\
  \tau &amp; \sim \textrm{Cauchy}^+(0, \tau_0)
\end{align*}\]</span></p>
<p>The local scale, <span class="math inline">\(\lambda_m\)</span>, can flexibly shrink the coefficient to close to
zero. Below is the implication of the prior on the shrinkage of <span class="math inline">\(\beta\)</span>:</p>
<pre><code>&gt;# Warning: Removed 1 row(s) containing missing values (geom_path).</code></pre>
<p><img src="09_model_comparison_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>The U-shape here means that, for coefficients that are weakly supported by the
data, the horseshoe will shrink it to very close to zero, whereas for
coefficients that are more strongly supported by the data, the horseshoe will
not shrink it much.</p>
<p>The red curve in the following is one example for the resulting prior
distribution on <span class="math inline">\(\beta\)</span>:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="model-comparison-and-regularization.html#cb289-1"></a>dhs &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(</span>
<span id="cb289-2"><a href="model-comparison-and-regularization.html#cb289-2"></a>  <span class="cf">function</span>(y, <span class="dt">df =</span> <span class="dv">1</span>) {</span>
<span id="cb289-3"><a href="model-comparison-and-regularization.html#cb289-3"></a>  ff &lt;-<span class="st"> </span><span class="cf">function</span>(lam) <span class="kw">dnorm</span>(y, <span class="dv">0</span>, <span class="dt">sd =</span> lam) <span class="op">*</span><span class="st"> </span><span class="kw">dt</span>(lam, df) <span class="op">*</span><span class="st"> </span><span class="dv">2</span></span>
<span id="cb289-4"><a href="model-comparison-and-regularization.html#cb289-4"></a>  <span class="cf">if</span> (y <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>) <span class="kw">integrate</span>(ff, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="ot">Inf</span>)<span class="op">$</span>value</span>
<span id="cb289-5"><a href="model-comparison-and-regularization.html#cb289-5"></a>  <span class="cf">else</span> <span class="ot">Inf</span></span>
<span id="cb289-6"><a href="model-comparison-and-regularization.html#cb289-6"></a>  }</span>
<span id="cb289-7"><a href="model-comparison-and-regularization.html#cb289-7"></a>)</span>
<span id="cb289-8"><a href="model-comparison-and-regularization.html#cb289-8"></a><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>)), <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span></span>
<span id="cb289-9"><a href="model-comparison-and-regularization.html#cb289-9"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dhs, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">df =</span> <span class="dv">3</span>), <span class="dt">n =</span> <span class="dv">501</span>,</span>
<span id="cb289-10"><a href="model-comparison-and-regularization.html#cb289-10"></a>                <span class="kw">aes</span>(<span class="dt">col =</span> <span class="st">&quot;HS&quot;</span>), <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb289-11"><a href="model-comparison-and-regularization.html#cb289-11"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">n =</span> <span class="dv">501</span>, </span>
<span id="cb289-12"><a href="model-comparison-and-regularization.html#cb289-12"></a>                <span class="kw">aes</span>(<span class="dt">col =</span> <span class="st">&quot;norm&quot;</span>), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb289-13"><a href="model-comparison-and-regularization.html#cb289-13"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="st">&quot;&quot;</span>, <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>),</span>
<span id="cb289-14"><a href="model-comparison-and-regularization.html#cb289-14"></a>                     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;horseshoe(3)&quot;</span>, <span class="st">&quot;N(0, 1)&quot;</span>)) <span class="op">+</span></span>
<span id="cb289-15"><a href="model-comparison-and-regularization.html#cb289-15"></a><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;y&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;density&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylim</span>(<span class="dv">0</span>, <span class="fl">0.75</span>)</span></code></pre></div>
<pre><code>&gt;# Warning: `mapping` is not used by stat_function()

&gt;# Warning: `mapping` is not used by stat_function()</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="09_model_comparison_files/figure-html/unnamed-chunk-2-1.png" alt="Density for the Finnish horseshoe prior with 3 degrees of freedom" width="672" />
<p class="caption">
Figure 9.4: Density for the Finnish horseshoe prior with 3 degrees of freedom
</p>
</div>
<p>Such a prior has more density at 0, but also more density for extreme values, as
compared to a normal distribution. Thus, for coefficients with very weak
evidence, the regularizing prior will shrink it to zero, whereas for
coefficients with strong evidence, the shrinkage will be very small. This is
called a horseshoe prior. In <code>brms</code>, one can specify it with <code>horseshoe()</code>,
which is a stabilized version of the original horseshoe prior <span class="citation">(Carvalho, Polson, and Scott <a href="#ref-carvalho2009" role="doc-biblioref">2009</a>)</span>.</p>
</div>
<div id="finnish-horseshoe" class="section level3" number="9.6.3">
<h3><span class="header-section-number">9.6.3</span> Finnish Horseshoe</h3>
<p>The Finnish horseshoe (<a href="https://projecteuclid.org/euclid.ejs/1513306866" class="uri">https://projecteuclid.org/euclid.ejs/1513306866</a>) prior is
<span class="math display">\[\begin{align*}
  \beta_m &amp; \sim \mathcal{N}(0, \tau \tilde \lambda_m) \\
  \tilde \lambda_m &amp; = \frac{c \lambda_m}{\sqrt{c^2 + \tau^2 \lambda^2_m}} \\
  \lambda_m &amp; \sim \textrm{Cauchy}^+(0, 1)  \\
  c^2 &amp; \sim \textrm{Inv-Gamma}(\nu / 2, nu / 2 s^2) \\
  \tau &amp; \sim \textrm{Cauchy}^+(0, \tau_0)
\end{align*}\]</span></p>
<p>The additional parameters are chosen in the code below. First, fit a model
without shrinkage:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="model-comparison-and-regularization.html#cb291-1"></a><span class="co"># A model with all main and interaction effects</span></span>
<span id="cb291-2"><a href="model-comparison-and-regularization.html#cb291-2"></a>m5 &lt;-<span class="st"> </span><span class="kw">brm</span>(kid_score <span class="op">~</span><span class="st"> </span>(.)<span class="op">^</span><span class="dv">2</span>, <span class="dt">data =</span> kidiq_std,</span>
<span id="cb291-3"><a href="model-comparison-and-regularization.html#cb291-3"></a>          <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>), </span>
<span id="cb291-4"><a href="model-comparison-and-regularization.html#cb291-4"></a>                    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>), </span>
<span id="cb291-5"><a href="model-comparison-and-regularization.html#cb291-5"></a>                    <span class="kw">prior</span>(<span class="kw">student_t</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;sigma&quot;</span>)), </span>
<span id="cb291-6"><a href="model-comparison-and-regularization.html#cb291-6"></a>          <span class="dt">iter =</span> <span class="dv">1000</span>,  <span class="co"># just to save time</span></span>
<span id="cb291-7"><a href="model-comparison-and-regularization.html#cb291-7"></a>          <span class="dt">chains =</span> 2L,</span>
<span id="cb291-8"><a href="model-comparison-and-regularization.html#cb291-8"></a>          <span class="dt">cores =</span> 2L,</span>
<span id="cb291-9"><a href="model-comparison-and-regularization.html#cb291-9"></a>          <span class="dt">seed =</span> <span class="dv">2217</span>)</span></code></pre></div>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="model-comparison-and-regularization.html#cb292-1"></a><span class="co"># A model with all main and interaction effects</span></span>
<span id="cb292-2"><a href="model-comparison-and-regularization.html#cb292-2"></a>m_hs &lt;-<span class="st"> </span><span class="kw">brm</span>(kid_score <span class="op">~</span><span class="st"> </span>(.)<span class="op">^</span><span class="dv">2</span>, <span class="dt">data =</span> kidiq_std,</span>
<span id="cb292-3"><a href="model-comparison-and-regularization.html#cb292-3"></a>            <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>), </span>
<span id="cb292-4"><a href="model-comparison-and-regularization.html#cb292-4"></a>                      <span class="co"># Prior guess of 20% of the terms are non-zero</span></span>
<span id="cb292-5"><a href="model-comparison-and-regularization.html#cb292-5"></a>                      <span class="kw">prior</span>(<span class="kw">horseshoe</span>(<span class="dt">par_ratio =</span> <span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">8</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb292-6"><a href="model-comparison-and-regularization.html#cb292-6"></a>                      <span class="kw">prior</span>(<span class="kw">student_t</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;sigma&quot;</span>)), </span>
<span id="cb292-7"><a href="model-comparison-and-regularization.html#cb292-7"></a>            <span class="dt">iter =</span> <span class="dv">1000</span>,  <span class="co"># just to save time</span></span>
<span id="cb292-8"><a href="model-comparison-and-regularization.html#cb292-8"></a>            <span class="dt">chains =</span> 2L,</span>
<span id="cb292-9"><a href="model-comparison-and-regularization.html#cb292-9"></a>            <span class="dt">cores =</span> 2L,</span>
<span id="cb292-10"><a href="model-comparison-and-regularization.html#cb292-10"></a>            <span class="co"># Need higher adapt_delta</span></span>
<span id="cb292-11"><a href="model-comparison-and-regularization.html#cb292-11"></a>            <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">.99</span>),</span>
<span id="cb292-12"><a href="model-comparison-and-regularization.html#cb292-12"></a>            <span class="dt">seed =</span> <span class="dv">2217</span>)</span></code></pre></div>
<p>We can plot the coefficients:</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="model-comparison-and-regularization.html#cb293-1"></a><span class="kw">stanplot</span>(m_hs) <span class="op">+</span><span class="st"> </span></span>
<span id="cb293-2"><a href="model-comparison-and-regularization.html#cb293-2"></a><span class="st">  </span><span class="co"># Show the shrinkage as black, transparent dots</span></span>
<span id="cb293-3"><a href="model-comparison-and-regularization.html#cb293-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> <span class="kw">posterior_summary</span>(m5) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb293-4"><a href="model-comparison-and-regularization.html#cb293-4"></a><span class="st">               </span><span class="kw">as_tibble</span>(<span class="dt">rownames =</span> <span class="st">&quot;parameter&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb293-5"><a href="model-comparison-and-regularization.html#cb293-5"></a><span class="st">               </span><span class="kw">filter</span>(parameter <span class="op">!=</span><span class="st"> &quot;lp__&quot;</span>), </span>
<span id="cb293-6"><a href="model-comparison-and-regularization.html#cb293-6"></a>             <span class="kw">aes</span>(<span class="dt">x =</span> Estimate, <span class="dt">y =</span> parameter), <span class="dt">alpha =</span> <span class="fl">0.8</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb293-7"><a href="model-comparison-and-regularization.html#cb293-7"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">c</span>(<span class="op">-</span>.<span class="dv">05</span>, <span class="fl">.05</span>), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<pre><code>&gt;# Warning: Method &#39;stanplot&#39; is deprecated. Please use &#39;mcmc_plot&#39; instead.</code></pre>
<p><img src="09_model_comparison_files/figure-html/interval-m_hs-1.png" width="432" /></p>
<p>An arbitrary cutoff is to select only coefficients with posterior means
larger than .05, in which case only <code>mom_iq</code> and <code>mom_hs</code> and their interaction
were supported by the data.</p>
<p>You can also double check that the regularized version has better LOO-IC:</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="model-comparison-and-regularization.html#cb295-1"></a><span class="kw">loo</span>(m5, m_hs)</span></code></pre></div>
<pre><code>&gt;# Output of model &#39;m5&#39;:
&gt;# 
&gt;# Computed from 1000 by 434 log-likelihood matrix
&gt;# 
&gt;#          Estimate   SE
&gt;# elpd_loo   -567.6 14.4
&gt;# p_loo        13.1  1.4
&gt;# looic      1135.1 28.8
&gt;# ------
&gt;# Monte Carlo SE of elpd_loo is 0.1.
&gt;# 
&gt;# All Pareto k estimates are good (k &lt; 0.5).
&gt;# See help(&#39;pareto-k-diagnostic&#39;) for details.
&gt;# 
&gt;# Output of model &#39;m_hs&#39;:
&gt;# 
&gt;# Computed from 1000 by 434 log-likelihood matrix
&gt;# 
&gt;#          Estimate   SE
&gt;# elpd_loo   -565.1 14.5
&gt;# p_loo         8.4  1.0
&gt;# looic      1130.1 28.9
&gt;# ------
&gt;# Monte Carlo SE of elpd_loo is 0.1.
&gt;# 
&gt;# All Pareto k estimates are good (k &lt; 0.5).
&gt;# See help(&#39;pareto-k-diagnostic&#39;) for details.
&gt;# 
&gt;# Model comparisons:
&gt;#      elpd_diff se_diff
&gt;# m_hs  0.0       0.0   
&gt;# m5   -2.5       2.4</code></pre>
<p>And also that the effective number of parameters was smaller in <code>m_hs</code>.</p>
</div>
</div>
<div id="variable-selection" class="section level2" number="9.7">
<h2><span class="header-section-number">9.7</span> Variable Selection</h2>
<p>One way to identify variables that are relevant to predict a certain outcome is
to use the projection-based method, as discussed in <a href="https://cran.r-project.org/web/packages/projpred/vignettes/quickstart.html" class="uri">https://cran.r-project.org/web/packages/projpred/vignettes/quickstart.html</a> and
in <span class="citation">Piironen and Vehtari (<a href="#ref-Piironen2016" role="doc-biblioref">2016</a>)</span>.</p>
<p>Building from the full model with shrinkage priors, we first identify the
importance of various variables in terms of their importance for prediction:</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="model-comparison-and-regularization.html#cb297-1"></a><span class="kw">library</span>(projpred)</span></code></pre></div>
<pre><code>&gt;# This is projpred version 1.1.6.</code></pre>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="model-comparison-and-regularization.html#cb299-1"></a><span class="co"># Variable selection:</span></span>
<span id="cb299-2"><a href="model-comparison-and-regularization.html#cb299-2"></a>vs &lt;-<span class="st"> </span><span class="kw">varsel</span>(m_hs)</span></code></pre></div>
<pre><code>&gt;# Warning: Method &#39;parse_bf&#39; is deprecated. Please use &#39;brmsterms&#39; instead.</code></pre>
<pre><code>&gt;# Warning: posterior_linpred(transform = TRUE) is deprecated. Please use
&gt;# posterior_epred() instead, without the &#39;transform&#39; argument.</code></pre>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="model-comparison-and-regularization.html#cb302-1"></a>vs<span class="op">$</span>vind  <span class="co"># variables ordered as they enter during the search</span></span></code></pre></div>
<pre><code>&gt;#           mom_iq    mom_hs:mom_iq  mom_hs:mom_work mom_work:mom_age 
&gt;#                2                5                6               10 
&gt;#           mom_hs  mom_iq:mom_work   mom_hs:mom_age          mom_age 
&gt;#                1                8                7                4 
&gt;#   mom_iq:mom_age         mom_work 
&gt;#                9                3</code></pre>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="model-comparison-and-regularization.html#cb304-1"></a><span class="co"># plot predictive performance on training data</span></span>
<span id="cb304-2"><a href="model-comparison-and-regularization.html#cb304-2"></a><span class="kw">varsel_plot</span>(vs, <span class="dt">stats =</span> <span class="kw">c</span>(<span class="st">&quot;elpd&quot;</span>, <span class="st">&quot;rmse&quot;</span>))</span></code></pre></div>
<p><img src="09_model_comparison_files/figure-html/vs-1.png" width="576" /></p>
<p>We then use the <code>cv_varsel</code> method to perform cross-validation to see how many
variables should be included:</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="model-comparison-and-regularization.html#cb305-1"></a><span class="co"># With cross-validation</span></span>
<span id="cb305-2"><a href="model-comparison-and-regularization.html#cb305-2"></a>cvs &lt;-<span class="st"> </span><span class="kw">cv_varsel</span>(m_hs, </span>
<span id="cb305-3"><a href="model-comparison-and-regularization.html#cb305-3"></a>                 <span class="dt">verbose =</span> <span class="ot">FALSE</span>)  <span class="co"># not printing progress</span></span></code></pre></div>
<pre><code>&gt;# Warning: Method &#39;parse_bf&#39; is deprecated. Please use &#39;brmsterms&#39; instead.</code></pre>
<pre><code>&gt;# Warning: posterior_linpred(transform = TRUE) is deprecated. Please use
&gt;# posterior_epred() instead, without the &#39;transform&#39; argument.</code></pre>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="model-comparison-and-regularization.html#cb308-1"></a><span class="co"># model size suggested by the program</span></span>
<span id="cb308-2"><a href="model-comparison-and-regularization.html#cb308-2"></a><span class="kw">suggest_size</span>(cvs)</span></code></pre></div>
<pre><code>&gt;# [1] 2</code></pre>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="model-comparison-and-regularization.html#cb310-1"></a><span class="co"># plot the validation results, this time relative to the full model</span></span>
<span id="cb310-2"><a href="model-comparison-and-regularization.html#cb310-2"></a><span class="kw">varsel_plot</span>(cvs, <span class="dt">stats =</span> <span class="kw">c</span>(<span class="st">&quot;elpd&quot;</span>, <span class="st">&quot;rmse&quot;</span>), <span class="dt">deltas =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="09_model_comparison_files/figure-html/cvs-1.png" width="576" /></p>
<p>Here it suggests to include only <code>mom_iq</code> and its interaction with <code>mom_hs</code>.
However, we generally prefers to also include the main effect of <code>mom_hs</code>.</p>
<div id="projection-based-method" class="section level3" number="9.7.1">
<h3><span class="header-section-number">9.7.1</span> Projection-Based Method</h3>
<p>The projection-based method will obtain the posterior distributions based on
a projection from the full model on the simplified model. In other words, we’re
asking the question:</p>
<blockquote>
<p>If we want a model with only <code>mom_iq</code>, <code>mom_hs</code>, and their interactions in
the model, what coefficients should be obtained so that the resulting prediction
accuracy is as closed to the full model as possible?</p>
</blockquote>
<p>Note that the coefficients will be different from if you were to directly
estimate the model using the three predictors (i.e., <code>m3</code>). In this case,
simulation results showed that the projection-based method will yield a model
with better predictive performance.</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="model-comparison-and-regularization.html#cb311-1"></a><span class="co"># Fit m3 with the standardized data</span></span>
<span id="cb311-2"><a href="model-comparison-and-regularization.html#cb311-2"></a>m3_std &lt;-<span class="st"> </span><span class="kw">brm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_hs <span class="op">*</span><span class="st"> </span>mom_iq, <span class="dt">data =</span> kidiq_std, </span>
<span id="cb311-3"><a href="model-comparison-and-regularization.html#cb311-3"></a>              <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>), </span>
<span id="cb311-4"><a href="model-comparison-and-regularization.html#cb311-4"></a>                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>), </span>
<span id="cb311-5"><a href="model-comparison-and-regularization.html#cb311-5"></a>                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>, </span>
<span id="cb311-6"><a href="model-comparison-and-regularization.html#cb311-6"></a>                              <span class="dt">coef =</span> <span class="st">&quot;mom_hs:mom_iq&quot;</span>), </span>
<span id="cb311-7"><a href="model-comparison-and-regularization.html#cb311-7"></a>                        <span class="kw">prior</span>(<span class="kw">student_t</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;sigma&quot;</span>)), </span>
<span id="cb311-8"><a href="model-comparison-and-regularization.html#cb311-8"></a>              <span class="dt">seed =</span> <span class="dv">2302</span>, </span>
<span id="cb311-9"><a href="model-comparison-and-regularization.html#cb311-9"></a>              <span class="dt">chains =</span> 2L, <span class="dt">cores =</span> 2L</span>
<span id="cb311-10"><a href="model-comparison-and-regularization.html#cb311-10"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="model-comparison-and-regularization.html#cb312-1"></a><span class="co"># Visualise the projected three most relevant variables</span></span>
<span id="cb312-2"><a href="model-comparison-and-regularization.html#cb312-2"></a>proj &lt;-<span class="st"> </span><span class="kw">project</span>(vs, <span class="dt">vind =</span> vs<span class="op">$</span>vind[<span class="kw">c</span>(<span class="st">&quot;mom_iq&quot;</span>, <span class="st">&quot;mom_hs:mom_iq&quot;</span>, <span class="st">&quot;mom_hs&quot;</span>)])</span>
<span id="cb312-3"><a href="model-comparison-and-regularization.html#cb312-3"></a><span class="kw">mcmc_intervals</span>(<span class="kw">as.matrix</span>(proj)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb312-4"><a href="model-comparison-and-regularization.html#cb312-4"></a><span class="st">  </span><span class="co"># Show the non-projection version as black, transparent dots</span></span>
<span id="cb312-5"><a href="model-comparison-and-regularization.html#cb312-5"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> </span>
<span id="cb312-6"><a href="model-comparison-and-regularization.html#cb312-6"></a>               <span class="kw">fixef</span>(m3_std, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;Intercept&quot;</span>, <span class="st">&quot;mom_iq&quot;</span>, </span>
<span id="cb312-7"><a href="model-comparison-and-regularization.html#cb312-7"></a>                                      <span class="st">&quot;mom_hs&quot;</span>, <span class="st">&quot;mom_hs:mom_iq&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb312-8"><a href="model-comparison-and-regularization.html#cb312-8"></a><span class="st">               </span><span class="kw">as_tibble</span>(<span class="dt">rownames =</span> <span class="st">&quot;parameter&quot;</span>), </span>
<span id="cb312-9"><a href="model-comparison-and-regularization.html#cb312-9"></a>             <span class="kw">aes</span>(<span class="dt">x =</span> Estimate, <span class="dt">y =</span> parameter), <span class="dt">alpha =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<pre><code>&gt;# Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<p><img src="09_model_comparison_files/figure-html/proj-interval-1.png" width="672" /></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-carvalho2009">
<p>Carvalho, Carlos M, Nicholas G Polson, and James G Scott. 2009. “Handling Sparsity via the Horseshoe.” In <em>Artificial Intelligence and Statistics</em>, 73–80.</p>
</div>
<div id="ref-hoeting1999bayesian">
<p>Hoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” <em>Statistical Science</em>, 382–401.</p>
</div>
<div id="ref-Piironen2016">
<p>Piironen, Juho, and Aki Vehtari. 2016. “Comparison of Bayesian Predictive Methods for Model Selection.” <em>Statistics and Computing</em>.</p>
</div>
<div id="ref-Vehtari2016">
<p>Vehtari, Aki, Andrew Gelman, and Jonah Gabry. 2016. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and Waic.” <em>Statistics and Computing</em> 27 (5): 1413–32. <a href="https://doi.org/10.1007/s11222-016-9696-4">https://doi.org/10.1007/s11222-016-9696-4</a>.</p>
</div>
<div id="ref-Yao2018">
<p>Yao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. “Using stacking to average bayesian predictive distributions (with discussion).” <em>Bayesian Analysis</em> 13 (3): 917–1007. <a href="https://doi.org/10.1214/17-BA1091">https://doi.org/10.1214/17-BA1091</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="model-diagnostics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hierarchical-multilevel-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["notes_bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

[
["index.html", "Course Handouts for Bayesian Data Analysis Class Preface", " Course Handouts for Bayesian Data Analysis Class Mark Lai 2020-06-04 Preface This is a collection of my course handouts for PSYC 621 class. The materials are based on the book by McElreath (2016), the brms package (Bürkner 2017), and the STAN language. Please contact me for any errors (as I’m sure there are plenty of them). You can download the EPUB version of the notes with the “Download” button. In some chapters I sourced some extra R codes. They can be found here. The notes were last built with: sessioninfo::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 3.6.3 (2020-02-29) ## os Ubuntu 18.04.4 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/Los_Angeles ## date 2020-06-04 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) ## bookdown 0.19 2020-05-15 [1] CRAN (R 3.6.3) ## cli 2.0.2 2020-02-28 [1] CRAN (R 3.6.2) ## crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.3) ## digest 0.6.25 2020-02-23 [1] CRAN (R 3.6.3) ## evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.3) ## fansi 0.4.1 2020-01-08 [1] CRAN (R 3.6.2) ## glue 1.4.1 2020-05-13 [1] CRAN (R 3.6.3) ## htmltools 0.4.0 2019-10-04 [1] CRAN (R 3.6.1) ## knitr 1.28 2020-02-06 [1] CRAN (R 3.6.2) ## magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) ## Rcpp 1.0.4.6 2020-04-09 [1] CRAN (R 3.6.3) ## rlang 0.4.6 2020-05-02 [1] CRAN (R 3.6.3) ## rmarkdown 2.2 2020-05-31 [1] CRAN (R 3.6.3) ## sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.6.0) ## stringi 1.4.6 2020-02-17 [1] CRAN (R 3.6.2) ## stringr 1.4.0 2019-02-10 [1] CRAN (R 3.6.0) ## withr 2.2.0 2020-04-20 [1] CRAN (R 3.6.3) ## xfun 0.14 2020-05-20 [1] CRAN (R 3.6.3) ## yaml 2.2.1 2020-02-01 [1] CRAN (R 3.6.2) ## ## [1] /home/markl/R/x86_64-pc-linux-gnu-library/3.6 ## [2] /usr/local/lib/R/site-library ## [3] /usr/lib/R/site-library ## [4] /usr/lib/R/library References "],
["introduction.html", "Chapter 1 Introduction 1.1 History of Bayesian Statistics 1.2 Motivations for Using Bayesian Methods 1.3 Probability 1.4 Bayes’s Theorem 1.5 Bayesian Statistics 1.6 Comparing Bayesian and Frequentist Statistics 1.7 Software for Bayesian Statistics", " Chapter 1 Introduction There will be some math in this notes. Don’t worry if you feel the math is challenging; for applied focused students, it is much more important to understand the concepts of Bayesian methods than to understand the mathematical symbols, as they usually can be handled by the software. 1.1 History of Bayesian Statistics Here is a nice brief video that covers some of the 250+ years of history of Bayesian statistics: https://www.youtube.com/watch?v=BcvLAw-JRss. If you are interested in learning more about the story, check out the nice popular science book, “The theory that would not die,” by Sharon Bertsch McGrayne 1.1.1 Thomas Bayes (1701–1762) You may find a biography of Bayes from https://www.britannica.com/biography/Thomas-Bayes. There is also a nice story in the book by Lambert (2018). He was an English Presbyterian minister. The important work he wrote that founded Bayesian statistics was “An Essay towards solving a Problem in the Doctrine of Chances”, which he did not publish and was later discovered and edited by his friend, Richard Price, after Bayes’s death1 1.1.2 Pierre-Simon Laplace (1749–1827) Laplace, a French Mathematician, was an important figure in not just Bayesian statistics, but also in other areas of mathematics, astronomy, and physics. We actually know much more the work by Laplace than by Bayes, and Laplace has worked independently on the inverse probability problem (i.e., \\(P[\\text{Parameter} | \\text{Data}]\\)). Indeed, he was credited for largely formalizing Bayesian interpretation of probability and most of the machinery for Bayesian statistics, and making it a useful technique to be applied to different problems, despite the discipline being called “Bayesian.” His other contributions include the methods of least square and the central limit theorem. See a short biography of him at https://www.britannica.com/biography/Pierre-Simon-marquis-de-Laplace. 1.1.3 20th Century Until early 1920s, the inverse probability method, which is based on what is now called Bayes’s Theorem, is pretty much the predominant point of view of statistics. Then a point of view later known as frequentist statistics arrived, and quickly became the mainstream school of thinking for statistical inferences, and is still the major framework for quantitative research. In the early 1920s, frequentist scholar, most notably R. A. Fisher and Jerzy Neyman, criticized Bayesian inference for the use of subjective elements in an objective discipline. In Fisher’s word, The theory of inverse probability is founded upon an error, and must be wholly rejected—Fisher, 1925 Ironically, the term Bayesian was first used in one of Fisher’s work. And interestingly, Fisher actually thought he “have been doing almost exactly what Bayes had done in the 18th century.”2 Despite criticisms from frequentist scholars, Bayesian methods has been used by scholars in the Allies in World War II, such as Alan Turing, in an algorithm to break coded messages in the Enigma machine that the German Navy used to communicate. However, because of the more complex mathematics involved in Bayesian statistics, Bayesian statistics is limited to straight-forward problems and theoretical discussions until the early 1980s, when computing speed increases tremendously and makes Markov Chain Monte Carlo—the major algorithm for Bayesian estimation in modern Bayesian statistics—feasible. With the help of increased computing speed, Bayesian statistics has come back and been used as an alternative way of thinking, especially given growing dissatisfaction towards the misuse of frequentist statistics by some scholars across disciplines. Bayesian estimation methods have also been applied to many new research questions where frequentist approaches work less well, as well as in big data analytics and machine learning. 1.2 Motivations for Using Bayesian Methods Based on my personal experience, Bayesian methods is used quite often in statistics and related departments, as it is consistent and coherent, as contrast to frequentist where a new and probably ad hoc procedure needed to be developed to handle a new problem. For Bayesian, as long as you can formulate a model, you just run the analysis the same way as you would for simpler problems, or in Bayesian people’s word “turning the Bayesian crank,” and likely the difficulties would be more technical than theoretical, which is usually solved with better computational speed. Social and behavioral scientists are relatively slow to adopt the Bayesian method, but things have been changing. In a recently accepted paper by van de Schoot et al. (2017), the authors reviewed papers in psychology between 1990 to 2015 and found that whereas less than 10% of the papers in 1990 to 1996 mentioned “Bayesian”, the proportion increased steadily and was found in close to 45% of the psychology papers in 2015. Among studies using Bayesian methods, more than 1/4 cited computational problems (e.g., nonconvergence) in frequentist methods as a reason, and about 13% cited the need to incorporate prior knowledge into the estimation process. The other reasons included the flexibility of Bayesian methods for complex and nonstandard problems, and the use of techniques traditionally attached to Bayesian such as missing data and model comparisons. 1.2.1 Problem with classical (frequentist) statistics The rise of Bayesian methods is also related to the statistical reform movement in the past two decades. The problem is that applied researchers are obsessed with \\(p &lt; .05\\) and often misinterpreted a small \\(p\\)-value as something that it isn’t (read Gigerenzer 2004). Some scholars coined the term \\(p\\)-hacking to refer to the practice of obtaining statistical significance by choosing to test the data in a certain way, either consciously or subconsciously (e.g., dichotomizing using mean or median, trying the same hypothesis using different measures of the same variable, etc). This is closely related to the recent “replication crisis” in scientific research, with psychology being in the center under close scrutiny. Bayesian is no panacea to the problem. Indeed, if misused it can give rise to the same problems as statistical significance. My goal in this class is to help you appreciate the Bayesian tradition of embracing the uncertainty in your results, and adopt rigorous model checking and comprehensive reporting rather than relying merely on a \\(p\\)-value. I see this as the most important mission for someone teaching statistics. 1.3 Probability There are multiple perspectives for understanding probability.3 What you’ve learned in your statistics training are based on the frequentist interpretation of probability (and thus frequentist statistics), whereas what you will learn in this class have the foundation on the subjectivist interpretation of probability. Although, in my opinions, the impact of these differences in interpretations of probability on statistical practices is usually overstated, understanding the different perspectives on probability is helpful for understanding the Bayesian framework. You don’t need to commit to one interpretation of probability in order to conduct Bayesian data analysis. 1.3.1 Classical Interpretation This is an earlier perspective, and is based on counting rules. The idea is that probability is equally distributed among all “indifferent” outcomes. “Indifferent” outcomes are those where a person does not have any evidence to say that one outcome is more likely than another. For example, when one throws a die, one does not think that a certain number is more likely than another, unless one knows that the die is biased. In this case, there are six equally likely outcome, and so the probability of each outcome is 1 / 6. 1.3.2 Frequentist Interpretation The frequentist interpretation states that probability is essentially the long-term relative frequency of an outcome. For example, to find the probability of getting a “1” when throwing a die, one can repeat the experiment many times, as illustrated below: Trial Outcome 1 2 2 3 3 1 4 3 5 1 6 1 7 5 8 6 9 3 10 3 And we can plot the relative frequency of “1”s in the trials: As you can see, with more trials, the relative frequency approaches 1 / 6. It’s the reason why in introductory statistics, many of the concepts require you to think in terms of repeated sampling (e.g., sampling distribution, \\(p\\)-values, standard errors, confidence intervals), because probability in this framework is only possible when the outcome can be repeated. It’s also the reason why we don’t talk about something like: the probability of the null hypothesis being true, or the probability that the population mean is in the interval [75.5, 80.5], because the population is fixed and cannot be repeated. Only the samples can be repeated, so probability in frequentist statistics is only about samples. 1.3.3 Problem of the single case Because of the frequentist’s reference to long-term frequency, under this framework it does not make sense to talk about probability of an event that cannot be repeated. For example, it does not make sense to talk about the probability that the Democrats will win the 2020 US Presidential Election, or the probability that the LA Rams winning the 2019 Super Bowl (they didn’t), or the probability that it rained on Christmas Day in LA in 2018, because all these are specific events that cannot be repeated. The problem is that it is common for lay people to talk about probabilities or chances for these events, and so the frequentist interpretation is limited for these problems. 1.3.4 Subjectivist Interpretation The frequentist interpretation is sometimes called the “objectivist view”, as the reference of probability is based on empirical evidence of long-term relative frequency (albeit hypothetical in many cases). In contrast, the subjectivist view of probability is based on one’s belief. For example, when I say that the probability of getting a “1” from rolling a die is 1 / 6, it reflects the state of my mind about the die. My belief can arise from different sources: Maybe I make the die and know it is a fair one; maybe I saw someone throwing the die 1,000 times and the number of “1”s was close to 1,000 / 6, or maybe someone I trust and with authority says that the die has a 1-in-6 chance of showing a “1”. The “subjective” component has been criticized a lot by frequentist scholars, sometimes unfairly. To be clear, what “subjective” here means is that probability reflects the state of one’s mind instead of the state of the world, and so it is totally fine that two people can have different beliefs about the same event. However, it does not mean that probability is arbitrary, as the beliefs are subjected to the constraints of the axioms of probability as well as the condition that the person possessing such beliefs are rational.4 Therefore, if two persons are exposed to the same information, they should form similar, though likely not identical, beliefs about the event. The subjective interpretation works perfectly fine with single events, as one can have a belief about whether it rains on a particular day or a belief about a particular election result. 1.3.5 Basics of Probability Kolmogorov axioms: For an event \\(A_i\\) (e.g., getting a “1” from throwing a die) \\(P(A_i) \\geq 0\\) [All probabilities are non-negative] \\(P(A_1 \\cup A_2 \\cup \\cdots) = 1\\) [Union of all possibilities is 1] \\(P(A_1) + P(A_2) = P(A_1 \\text{ or } A_2)\\) [Mutually exclusive events] Consider two events, for example, on throwing a die, \\(A\\): The number is odd \\(B\\): The number is larger than or equal to 4 Assuming that die is (believed to be) fair, you can easily verify that the probability of \\(A\\) is \\(P(A)\\) = 3 / 6 = 1 / 2, and the probability of \\(B\\) is also \\(P(B)\\) = 3 / 6 = 1 / 2. 1.3.5.1 Conditional Probability Conditional probability is the probability of an event given some other information. In the real world, you can say that everything is conditional. For example, the probability of getting an odd number on throwing a die is 1/2 is conditional on the die being fair. We use \\(P(A | B)\\) to represent the the conditional probability of event \\(A\\) given event \\(B\\).. Continuing from the previous example, \\(P(A | B)\\) is the conditional probability of getting an odd number, knowing that the number is at least 4. By definition, the conditional probability is the probability that both \\(A\\) and \\(B\\) happen ( written as \\(A \\cap B\\) and pronounced as A–cap–B or A–intersection–B), divided by the probability that \\(B\\) happen. \\(P(A | B) = \\frac{P(A \\cap B)}{P(B)}\\) In the example, \\(P(A \\cap B)\\) = 1/6, because 5 is the only even number \\(\\geq\\) 4 when throwing a die. Thus, \\[\\begin{align} P(A | B) &amp; = 1 / 3 \\\\ &amp; = \\frac{P(A \\cap B)}{P(B)} \\\\ &amp; = \\frac{1 / 6}{1 / 2} \\end{align}\\] This picture should make it clear: 1.3.5.2 Independence Two events, \\(A\\) and \\(B\\), are independent if \\(P(A | B) = P(A)\\) This means that any knowledge of \\(B\\) does not (or should not) affect one’s belief about \\(A\\). In the example, obviously \\(A\\) and \\(B\\) are not independent, because once we know that the number if 4 or above, it changes the probability of whether it is an odd number or not. It can also be expressed as With independence, \\(P(A \\cap B) = P(A) P(B)\\) 1.3.5.3 Law of Total Probability When we talk about conditional probability, like \\(B_1\\) = 4 or above and \\(B_2\\) = 3 or below, we can get \\(P(A | B_1)\\) and \\(P(A | B_2)\\) (see the figure below), we refer \\(P(A)\\) as the marginal probability, meaning that the probability of \\(A\\) without knowledge of \\(B\\). If \\(B_1, B_2, \\cdots, B_n\\) are all mutually exclusive possibilities for an event (so they add up to a probability of 1), then by the law of total probability, \\[\\begin{align} P(A) &amp; = P(A \\cap B_1) + P(A \\cap B_2) + \\cdots + P(A \\cap B_n) \\\\ &amp; = P(A | B_1)P(B_1) + P(A | B_2)P(B_2) + \\cdots + P(A | B_n) P(B_n) \\\\ &amp; = \\sum_{k = 1}^n P(A | B_k) P(B_k) \\end{align}\\] 1.4 Bayes’s Theorem The Bayes’s theorem is, surprisingly (or unsurprisingly), very simple: \\[P(B | A) = \\frac{P(A | B) P(B)}{P(A)}\\] More generally, we can expand it to incorporate the law of total probability tomake it more applicable to data analysis. Consider \\(B_i\\) as one of the \\(n\\) many possible mutually exclusive events, then \\[\\begin{align} P(B_i | A) &amp; = \\frac{P(A | B_i) P(B_i)}{P(A)} \\\\ &amp; = \\frac{P(A | B_i) P(B_i)} {P(A | B_1)P(B_1) + P(A | B_2)P(B_2) + \\cdots + P(A | B_n)P(B_n)} \\\\ &amp; = \\frac{P(A | B_i) P(B_i)}{\\sum_{k = 1}^n P(A | B_k)P(B_k)} \\end{align}\\] If \\(B_i\\) is a continuous variable, we will replace the sum by an integral, \\[P(B_i | A) = \\frac{P(A | B_i) P(B_i)}{\\int_k P(A | B_k)P(B_k)}\\] The denominator is not important for practical Bayesian analysis, therefore, it is sufficient to write the above equality as \\[P(B_i | A) \\propto P(A | B_i) P(B_i)\\] 1.4.1 Example 1: Base rate fallacy (From Wikipedia) A police officer stops a driver at random and do a breathalyzer test for the driver. The breathalyzer is known to detect true drunkenness 100% of the time, but in 1% of the cases it gives a false positive when the driver is sober. We also know that in general, for every 1,000 drivers passing through that spot, one is driving drunk. Suppose that the breathalyzer shows positive for the driver. What is the probability that the driver is truly drunk? \\(P(\\text{positive} | \\text{drunk}) = 1\\) \\(P(\\text{positive} | \\text{sober}) = 0.01\\) \\(P(\\text{drunk}) = 1 / 1000\\) \\(P(\\text{sober}) = 999 / 1000\\) Using Bayes’ Theorem, \\[\\begin{align} P(\\text{drunk} | \\text{positive}) &amp; = \\frac{P(\\text{positive} | \\text{drunk}) P(\\text{drunk})} {P(\\text{positive} | \\text{drunk}) P(\\text{drunk}) + P(\\text{positive} | \\text{sober}) P(\\text{sober})} \\\\ &amp; = \\frac{1 \\times 0.001}{1 \\times 0.001 + 0.01 \\times 0.999} \\\\ &amp; = 100 / 1099 \\approx 0.091 \\end{align}\\] So there is less than 10% chance that the driver is drunk even when the breathalyzer shows positive. You can verify that with a simulation using R: set.seed(4) truly_drunk &lt;- c(rep(&quot;drunk&quot;, 100), rep(&quot;sober&quot;, 100 * 999)) table(truly_drunk) &gt;# truly_drunk &gt;# drunk sober &gt;# 100 99900 breathalyzer_test &lt;- ifelse(truly_drunk == &quot;drunk&quot;, # If drunk, 100% chance of showing positive &quot;positive&quot;, # If not drunk, 1% chance of showing positive sample(c(&quot;positive&quot;, &quot;negative&quot;), 999000, replace = TRUE, prob = c(.01, .99))) # Check the probability p(positive | sober) table(breathalyzer_test[truly_drunk == &quot;sober&quot;]) &gt;# &gt;# negative positive &gt;# 98903 997 # 997 / 99900 = 0.00997998, so the error rate is less than 1% # Now, Check the probability p(drunk | positive) table(truly_drunk[breathalyzer_test == &quot;positive&quot;]) &gt;# &gt;# drunk sober &gt;# 100 997 # 100 / (100 + 997) = 0.0911577, which is only 9.1%! 1.5 Bayesian Statistics Bayesian statistics is a way to estimate some parameter \\(\\theta\\) (i.e., some quantities of interest, such as population mean, regression coefficient, etc) by applying the Bayes’ Theorem. &gt; \\[P(\\theta = t | y) \\propto P(y | \\theta = t) P(\\theta = t)\\] There are three components in the above equality: \\(P(y | \\theta = t)\\), the probability that you observe the datum \\(y\\), assuming that the parameter \\(\\theta\\) has a value \\(t\\); this is called the likelihood (Note: It is the likelihood of \\(\\theta\\), but probability about \\(y\\)) \\(P(\\theta = t)\\), the probability that \\(\\theta\\) has a value of \\(t\\), without referring to the datum \\(y\\). This usually requires appeals to one’s degree of belief, and so is called the prior \\(P(\\theta = t | y)\\), the probability that \\(\\theta\\) has a value of \\(t\\), after observing the datum \\(y\\); this is called the posterior This is different from the classical/frequentist statistics, which focuses solely on the likelihood function.5 In Bayesian statistics, the goal is to update one’s belief about \\(\\theta\\) based on the observed datum \\(y\\). 1.5.1 Example 2: Locating a Plane (reference: http://87.106.45.173:3838/felix/BayesLessons/BayesianLesson1.Rmd) Consider a highly simplified scenario of locating a missing plane in the sea. Assume that we know the plane, before missing, happened to be flying on the same latitude, heading west across the Pacific, so we only need to find the longitude of it. We want to go out to collect debris (data) so that we can narrow the location (\\(\\theta\\)) of the plane down. We start with our prior. Assume that we have some rough idea that the plane should be, so we express our belief in a probability distribution like the following: which says that our belief is that the plane is about twice more likely to be towards the east than towards the west. Below are two other options for priors (out of infinitely many), one providing virtually no information and the other encoding stronger information: The prior is chosen to reflect the researcher’s belief, so it is likely that different researchers will formulate a different prior for the same problem, and that’s okay as long as the prior is reasonable and justified. Later we will learn that in regular Bayesian analyses, with a moderate sample size different priors generally make only a negligible differences. Now, assume that we have collected debris in the locations shown in the graph, Now, from Bayes’s Theorem, \\[\\text{Posterior Probability} \\propto \\text{Prior Probability} \\times \\text{Likelihood}\\] So we can simply multiply the prior probabilities and the likelihood to get the posterior probability for every location. A rescaling step is needed to make sure that the area under the curve will be 1, which is usually performed by the software. &gt;# Warning: `mapping` is not used by stat_function() The following shows what happen with a stronger prior: &gt;# Warning: `mapping` is not used by stat_function() 1.6 Comparing Bayesian and Frequentist Statistics Attributes Frequentist Bayesian Interpretation of probability Frequentist Subjectivist Uncertainty How estimates vary in repeated sampling from the same population How much prior beliefs about parameters change in light of data What’s relevant? Current data set + all that might have been observed Only the data set that is actually observed How to proceed with analyses MLE; ad hoc and depends on problems “Turning the Bayesian crank” 1.7 Software for Bayesian Statistics WinBUGS Bayesian inference Using Gibbs Sampling Free, and most popular until late 2000s. Many Bayesian scholars still use WinBUGS No further development One can communicate from R to WinBUGS using the package R2WinBUGS JAGS Just Another Gibbs Sampler Very similar to WinBUGS, but written in C++, and support user-defined functionality Cross-platform compatibility One can communicate from R to JAGS using the package rjags or runjags STAN Named in honour of Stanislaw Ulam, who invented the Markov Chain Monte Carlo method Uses new algorithms that are different from Gibbs sampling Under very active development Can interface with R throught the package rstan, and the R packages rstanarm and brms automates the procedure for fitting models in STAN for many commonly used models References "],
["bayesian-inference.html", "Chapter 2 Bayesian Inference 2.1 Steps of Bayesian Data Analysis 2.2 Real Data Example 2.3 Choosing a Model 2.4 Specifying Priors 2.5 Obtain the Posterior Distributions 2.6 Summarizing the Posterior Distribution 2.7 Model Checking 2.8 Posterior Predictive Check 2.9 Summary", " Chapter 2 Bayesian Inference This lecture describes the steps to perform Bayesian data analysis. Some authors described the process as “turning the Bayesian Crank,” as the same work flow basically applies to every research questions, so unlike frequentist which requires different procedures for different kinds of questions and data, Bayesian represents a generic approach for data analysis, and development in the area mainly involves development of new models (but still under the same work flow), invention of faster algorithms for bigger data sets, and evaluations of different choices of priors. 2.1 Steps of Bayesian Data Analysis Adapted from Kruschke (2015, 25), I conceptualize Bayesian data analysis as the following steps: Identify/Collect the data required to answer the research questions. As a general recommendation, it is helpful to visualize the data to get a sense of how the data look, as well as to inspect for any potential anomalies in the data collection. Choose a statistical model for the data in relation to the research questions. The model should have good theoretical justification and have parameters that are meaningful for the research questions. Specify prior distributions for the model parameters. Although this is a subjective endeavor, the priors chosen should be at least sensible to audience who are skeptical. Obtain the posterior distributions for the model parameters. As described below, this can be obtained by analytical or various mathematical approximations. For mathematical approximations, one should check the algorithms for convergence to make sure the results closely mimic the target posterior distributions Conduct a posterior predictive check to examine the fit between the model and the data, i.e., whether the chosen model with the estimated parameters generate predictions that deviate form the data being analyzed on important features. If the model does not fit the data, one should go back to step 2 to specify a different model If the fit between the model and the data is deemed satisfactory, one can proceed to interpret the results in the context of the research questions. It is also important to visualize the results in ways that are meaningful for the analysis. 2.2 Real Data Example We will be using a built-in data set in R about patients diagnosed with AIDS in Australia before 1 July 1991. For this week we will only look at a tiny subsample of 10 people. Here is a description of the variables (from the R documentation): state: Grouped state of origin: “NSW”includes ACT and “other” is WA, SA, NT and TAS. sex: Sex of patient. diag:(Julian) date of diagnosis. death: (Julian) date of death or end of observation. status: “A” (alive) or “D” (dead) at end of observation. T.categ: Reported transmission category. age: Age (years) at diagnosis. You should always first plot your data and get some summary statistics: data(&quot;Aids2&quot;, package = &quot;MASS&quot;) set.seed(15) Aids2_sub &lt;- Aids2 %&gt;% sample_n(10) Aids2_sub &gt;# state sex diag death status T.categ age &gt;# 1 VIC M 11386 11504 A hs 36 &gt;# 2 NSW M 10103 10297 D hs 25 &gt;# 3 QLD M 10874 11191 D hs 36 &gt;# 4 NSW M 11398 11504 A hs 42 &gt;# 5 NSW M 9598 10623 D hs 40 &gt;# 6 NSW M 9768 9945 D hs 69 &gt;# 7 NSW M 10828 11504 A hs 37 &gt;# 8 Other F 10519 11504 A id 30 &gt;# 9 NSW M 11054 11343 D hs 30 &gt;# 10 NSW M 10345 11054 D hs 41 # install.packages(&quot;psych&quot;) # uncomment to install the `psych` package library(psych) pairs.panels(Aids2_sub, ellipses = FALSE) 2.3 Choosing a Model We will be using the status variable, specifically the death rate of AIDS in this data set. Our simple research question is: What is the death rate of AIDS in Australia when the data were collected? Now let’s go through the Bayesian Crank. If we assume that the outcome of the observations are exchangeable, meaning that the observations can be reordered in any way and still gives the same inference, then one can choose a model: \\[y \\sim \\text{Bin}(n, \\theta)\\] \\(y\\) = number of “D” in the observed data \\(n\\) = number of patients in the data set \\(\\theta\\) = probability of “D” The model states that: the sample \\(y\\) is distributed as a binomial distribution with \\(n\\) observations and with a rate parameter \\(\\theta\\). 2.3.1 Exchangeability* Exchangeability is an important concept in Bayesian statistics, but for the purpose of this class it may be a bit beyond the scope. Instead of looking at its mathematical definition, I will illustrate it in an example. Say we take 6 rows in our data set: state sex diag death status T.categ age 1 NSW M 10905 11081 D hs 35 31 NSW F 10961 11504 A id 30 1802 QLD F 9495 9753 D blood 66 1811 QLD M 10770 11504 A hsid 29 2129 VIC M 8499 8568 D hs 43 2137 VIC M 9055 9394 D hs 29 Now, when we reorder the column status to something like: state sex diag death status T.categ age NSW M 10905 11081 D hs 35 NSW F 10961 11504 D id 30 QLD F 9495 9753 A blood 66 QLD M 10770 11504 D hsid 29 VIC M 8499 8568 D hs 43 VIC M 9055 9394 A hs 29 if the results are expect to be the same then we say that the observations are assumed exchangeable. This happens when we assume that all observations have one common mean. However, if we think that there is a mean for females and a different mean for males, we cannot reorder the outcome randomly because they are no longer exchangeable (i.e., you cannot exchange a female score for a male score and expect to get the same results). Exchangeability: A set of observations are said to be exchangeable if their joint probability distribution stays the same under all permutations. Roughly speaking, it means that the observations can be reordered and still provide the same inferences. 2.3.2 Probability Distributions Probability distributions are central in statistical modeling. You should’ve at least hear about the normal distribution, where a variable follows a bell shape. There are two groups of distributions, depending on whether the variable of interest is discrete or continuous. For this example, the variable concerns the count of death, so it is discrete. Each probability distribution has a mathematical form called a probability mass function (pmf) for a discrete variable, and a probability density function (pdf) for a continuous variable. The pmf specifies the probability for each possible value. 2.3.2.1 Binomial Distribution For a binomial variable, the pmf is \\[\\begin{align} P(y) &amp; = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y} \\\\ \\end{align}\\] The term \\(\\binom{n}{y}\\) is called a binomial coefficient. The mathematical form is not the most important thing for this class as it is usually handled by the software. Here is a plot for a binomial distribution with \\(n = 10\\) and different values of \\(\\theta\\) # par(mfrow = c(2, 2), mar = c(4, 4, 2, 0) + 0.1) # for (th in c(0.1, 0.2, 0.5, 0.8)) { # plot(0:10, dbinom(0:10, 10, th), type = &quot;h&quot;, xlab = &quot;y&quot;, # ylab = expression(italic(P)(Y == y)), bty = &quot;L&quot;, # lwd = 2) # pos &lt;- &quot;topright&quot; # if (th &gt; 0.6) pos &lt;- &quot;topleft&quot; # legend(pos, legend = bquote(theta == .(th))) # } th &lt;- c(0.1, 0.2, 0.5, 0.8) plist &lt;- vector(&quot;list&quot;, length = length(th)) for (i in 1:4) { label_xpos &lt;- if (th[i] &gt; .6) 2 else 8 plist[[i]] &lt;- ggplot(tibble(y = 0:10, prob = dbinom(0:10, 10, th[i])), aes(x = y, y = prob)) + geom_bar(stat = &quot;identity&quot;, width = 0.1) + labs(y = expression(italic(P)(Y == y)), x = expression(y)) + scale_x_continuous(breaks = 0:10) + annotate(&quot;text&quot;, x = label_xpos, y = Inf, vjust = 1, label = list(bquote(theta == .(th[i]))), parse = TRUE) } gridExtra::grid.arrange(grobs = plist, nrow = 2) Note in the above that the possible values of \\(y\\) are \\(\\{0, 1, \\ldots, 10\\}\\), and one can get the probability of each value. You will see many other probability distributions in this class, for both discrete and continuous variables. 2.3.3 The Likelihood After observing the data \\(y\\), we can obtain the likelihood for each value of \\(\\theta\\), \\(P(y | \\theta)\\). For example, for \\(\\theta = 0.5\\), I get \\[P(y | \\theta = 0.5) = 0.205\\] This can be obtained in R using the following line: # Probability of y = 6 for 10 trials, with theta = 0.5 dbinom(6, 10, 0.5) &gt;# [1] 0.205 As there are infinitely many possible \\(\\theta\\) values in the range \\([0, 1]\\), we cannot get \\(P(y | \\theta)\\) for every one of them. Here is a few typical values: th &lt;- seq(0, 1, by = 0.1) tibble(theta = th, likelihood = dbinom(6, 10, prob = th)) &gt;# # A tibble: 11 x 2 &gt;# theta likelihood &gt;# &lt;dbl&gt; &lt;dbl&gt; &gt;# 1 0 0 &gt;# 2 0.1 0.000138 &gt;# 3 0.2 0.00551 &gt;# 4 0.3 0.0368 &gt;# 5 0.4 0.111 &gt;# 6 0.5 0.205 &gt;# 7 0.6 0.251 &gt;# 8 0.7 0.200 &gt;# 9 0.8 0.0881 &gt;# 10 0.9 0.0112 &gt;# 11 1 0 And you can plot the likelihood function: ggplot(tibble(x = c(0, 1)), aes(x = x)) + stat_function(fun = dbinom, args = list(x = 6, size = 10), col = &quot;red&quot;) + labs(x = expression(theta), y = &quot;likelihood&quot;) 2.4 Specifying Priors The previous model has one parameter, \\(\\theta\\), which is the probability of getting a target outcome (i.e., “D”) in each observation. Because \\(\\theta\\) is a probability, its support is \\([0, 1]\\), meaning that it is continuous and can take any value from 0 to 1. For a continuous parameter there are infinitely many possible values, and it is impossible to specify our beliefs for each value. We can specify something like: to express a belief that \\(\\theta\\) is most likely to be in the range \\([.40, .60)\\) and is 5 times more likely than any values outside of that range. Bayesian analyses can handle such priors, although in general we prefer a prior distribution that has a known form and is smooth. One thing to remember is: A good prior should give a non-zero probability/density for all possible values of a parameter 2.4.1 Beta Distribution A commonly used family of prior distribution for a binomial model is the beta distribution, which has two parameters. We can write the prior as \\[P(\\theta) \\sim \\text{Beta}(a, b)\\] \\(a\\) and \\(b\\) are the two parameters. Here are a few examples: You will notice that when \\(a &gt; b\\), there will be more density closer to the right region (i.e., larger \\(\\theta\\)), and vice versa. Also, when \\(a\\) and \\(b\\) become larger, the variance decreases. A nice interpretation of \\(a\\) and \\(b\\) in a beta prior distribution is to consider \\(a - 1\\) = number of prior ‘successes’ (e.g., “D”) \\(b - 1\\) = number of prior ‘failures’ (e.g., “A”) Therefore, with \\(\\text{Beta}(1, 1)\\) one has seen 0 prior success and 0 failure, meaning that there are no prior information (i.e., noninformative). Therefore, it makes sense that all \\(\\theta\\) values are equally likely. On the other hand, if one chooses \\(\\text{Beta}(10, 20)\\), one has seen 9 prior successes and 19 prior failures, so one has quite a lot of prior information (indeed more than the data with only 10 observations), so this is a strong prior. The smaller the variance of the prior distribution, the stronger one’s belief before looking at the data, the more prior information So by manipulating the parameters you can control the shape of the prior distribution as well as its strength, so it is quite flexible. Another advantage of using a beta prior is that it is a conjugate prior of the binomial model, which means that the posterior distribution of \\(P(\\theta | y)\\) is also a beta distribution, the same as the prior distribution, although with different parameter values. Conjugate Prior: For a specific model, conjugate priors yield posterior distributions in the same distribution family as the priors This greatly simplifies the computational burden for Bayesian analyses, so conjugate priors are almost the only ones used in earlier literature. However, this limited the applications of Bayesian methods, as for many problems no conjugate priors can provide a realistic representation of one’s belief. Modern Bayesian analysis instead relies on simulation-based methods to approximate the posterior distribution, which can accommodate almost any kind of prior distributions. Aside from a few examples in this note and the next, mainly for pedagogical purposes, we will be using simulation-based methods and will discuss it in Week 4. Now, I will formulate my prior based on the information provided by the CDC in the US (https://www.cdc.gov/mmwr/preview/mmwrhtml/mm5021a2.htm), which says that As of December 31, 2000, 774,467 persons had been reported with AIDS in the United States; 448,060 of these had died. If I believe this also applies to Australia in 1991, then I will use a \\(\\text{Beta}(448,061, 326,408)\\) prior. However, because the time and location is different, I don’t want to use a prior that strong. Instead, I will divide the information by 10,000 times (an arbitrary number; in practice you usually don’t scale by that much) so that the prior is \\[P(\\theta) = \\text{Beta}(46, 34)\\] which is much weaker but still pretty strong. The prior represents an amount of information equaled to 78 observations. The business of choosing a suitable prior distribution is not trivial. Luckily, with more and more researchers and methodologists using Bayesian analyses, there have been more and more recommendations for suitable prior distributions for different commonly used models. Also, when the prior contains relatively few information as compared to the data, different choices of prior hardly make a difference in the posterior distributions. On the other hand, one can perform sensitivity analyses, meaning that one can experiment with a few sensible priors to see how the Bayesian inferences change; in the results stay pretty much the same, one can be more confident that the results are mainly driven by the data rather than the “subjectively chosen” priors. In general, one would worry more about choosing a model that does not fit the data than about misspecifying a prior. 2.5 Obtain the Posterior Distributions Remember again the relationship between the prior and the posterior: \\[P(\\theta | y) \\propto P(y | \\theta) P(\\theta)\\] The posterior distributions are mathematically determined once the priors and the likelihood are set. However, the mathematical form of the posterior is sometimes very difficult to deal with. Here are a few methods to make sense of the posterior, with all of them sharing the goal of knowing the shape and some important properties of the posterior. 2.5.1 Grid Approximation One straight forward, brute-force method is to compute the posterior density for a large number of values of the parameters. For example, by taking \\(\\theta\\) = 0, 0.01, 0.02, . . . , 0.98, 0.99, 1.00, which corresponds a grid of 101 points, I can evaluate the posterior at these 21 points. # Define a grid for the parameter th_grid &lt;- seq(0, 1, by = 0.01) # Get the prior density for each value on the grid prior &lt;- dbeta(th_grid, 46, 34) # Get the likelihood for each value on the grid lik &lt;- dbinom(6, 10, prob = th_grid) # Multiply to get the posterior post_raw &lt;- prior * lik # Scale it back to density so that the sum is 1 post_dens &lt;- post_raw / sum(post_raw * 0.01) # Print a table for a few values grid_dat &lt;- tibble(theta = th_grid, prior = prior, likelihood = lik, `prior x likelihood` = post_raw, posterior = post_dens) grid_dat[51:60, ] &gt;# # A tibble: 10 x 5 &gt;# theta prior likelihood `prior x likelihood` posterior &gt;# &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt;# 1 0.5 2.85 0.205 0.584 2.50 &gt;# 2 0.51 3.57 0.213 0.760 3.25 &gt;# 3 0.52 4.33 0.220 0.954 4.08 &gt;# 4 0.53 5.09 0.227 1.16 4.94 &gt;# 5 0.54 5.81 0.233 1.35 5.79 &gt;# 6 0.55 6.42 0.238 1.53 6.54 &gt;# 7 0.56 6.88 0.243 1.67 7.14 &gt;# 8 0.57 7.14 0.246 1.76 7.52 &gt;# 9 0.580 7.19 0.249 1.79 7.64 &gt;# 10 0.59 7.00 0.250 1.75 7.49 # par(mfrow = c(1, 2), mar = c(4, 4, 2, 0) + 0.1) # # Plot the prior # plot(th_grid, prior, type = &quot;o&quot;, col = &quot;green3&quot;, ylim = c(0, 5), # ylab = &quot;density&quot;, cex = 0.7, xlab = expression(theta)) # # Plot the likelihood (need to scale it) # lines(th_grid, lik / sum(lik) / 0.05, type = &quot;o&quot;, col = &quot;red&quot;, cex = 0.7) # # Plot the post raw (need to scale it) # lines(th_grid, post_dens, type = &quot;o&quot;, col = &quot;blue&quot;, cex = 0.7) # legend(&quot;topright&quot;, c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;), # lwd = 1, # col = c(&quot;green3&quot;, &quot;red&quot;, &quot;blue&quot;), cex = 0.7) # curve(dbeta(x, 52, 38), ylim = c(0, 5), # ylab = &quot;density&quot;, xlab = expression(theta), col = &quot;blue&quot;) # text(0.6, 4, &quot;Beta(52, 38)&quot;) p1 &lt;- ggplot(grid_dat, aes(x = theta)) + geom_point(aes(y = prior, col = &quot;Prior&quot;)) + geom_line(aes(y = prior, col = &quot;Prior&quot;)) + geom_point(aes(y = likelihood / sum(likelihood) / 0.01, col = &quot;Likelihood&quot;)) + geom_line(aes(y = likelihood / sum(likelihood) / 0.01, col = &quot;Likelihood&quot;)) + geom_point(aes(y = posterior, col = &quot;Posterior&quot;)) + geom_line(aes(y = posterior, col = &quot;Posterior&quot;)) + labs(x = expression(theta), y = &quot;Density&quot;, col = &quot;&quot;) + ylim(0, 8) + scale_color_manual(&quot;&quot;, values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green3&quot;)) + theme(legend.position = c(0.80, 0.80)) p2 &lt;- ggplot(grid_dat, aes(x = theta)) + stat_function(fun = dbeta, args = list(shape1 = 52, shape2 = 38), col = &quot;blue&quot;) + geom_text(x = 0.2, y = 7, label = &quot;Beta(52, 38)&quot;) + labs(x = expression(theta), y = &quot;Density&quot;, col = &quot;&quot;) + ylim(0, 8) gridExtra::grid.arrange(p1, p2, nrow = 1) The blue line above on the left graph shows an approximation of the posterior distribution. 2.5.2 Using Conjugate Priors As we’re using a conjugate prior, the posterior is also a beta distribution. It has been mathematically proven that \\[P(\\theta | y) \\sim \\text{Beta}(a + y, b + n - y),\\] which is a distribution for \\(a + y - 1\\) successes and \\(b + n - y\\) failures. This makes perfect sense as our prior information as \\(a - 1\\) successes and \\(b - 1\\) failures, and from our data we have \\(y\\) successes and \\(n - y\\) failures, so that our updated belief is based on adding up those successes and failures. With \\(a = 46\\), \\(b = 34\\), we know that the posterior distribution will be a \\(\\text{Beta}(52, 38)\\) distribution, as shown in the graph above in the right panel. The one obtained by grid approximation is quite close to the true posterior distribution, and can be improved by increasing the number of points in the grid. 2.5.2.1 Proof of Conjugacy* To derive the form of the posterior, first recognize that the beta distribution has the form: \\[\\begin{align} P(\\theta) &amp; = \\mathrm{B}^{-1}(a, b) \\theta^{a - 1} (1 - \\theta)^{b - 1} \\\\ &amp; \\propto \\theta^{a - 1} (1 - \\theta)^{b - 1} \\end{align}\\] Where \\(\\mathrm{B}(\\cdot)\\) is the beta function which is not very important for the class. As the density function is a function of \\(\\theta\\), it suffices to write only the terms that involve \\(\\theta\\). Similarly, \\[P(y | \\theta) \\propto \\theta^y (1 - \\theta)^{n - y}\\] with the binomial coefficient, which does not involve \\(\\theta\\), dropped. Therefore, \\[\\begin{align} P(\\theta | y) &amp; \\propto P(y | \\theta) P(\\theta) \\\\ &amp; \\propto \\theta^y (1 - \\theta)^{n - y} \\theta^{a - 1} (1 - \\theta)^{b - 1} \\\\ &amp; = \\theta^{a + y - 1} (1 - \\theta)^{b + n - y - 1}. \\end{align}\\] If we let \\(a^* = a + y\\), \\(b^* = b + n - y\\), we can see that \\(P(\\theta | y)\\) is in the same form as the prior with \\(a\\) and \\(b\\) replaced by \\(a^*\\) and \\(b^*\\). Therefore, the posterior is also a beta distribution. So the beta distribution is a conjugate prior for the binomial model. 2.5.3 Laplace Approximation with Maximum A Posteriori Estimation The Laplace approximation is like the Bayesian version of the Central Limit Theorem, where a normal distribution is used to approximate the posterior distribution. Because the normal distribution is symmetric with the mean being the point with highest density, with Laplace approximation the goal is to find the maximum point in the posterior distribution, which is called the maximum a posteriori (MAP) estimate. Generally, finding the MAP is much easier then deriving the whole posterior distribution. The following R code finds the MAP of the posterior distribution using the optim() function. log_prior &lt;- function(th) dbeta(th, 46, 34, log = TRUE) log_lik &lt;- function(th) dbinom(6, 10, th, log = TRUE) log_posterior &lt;- function(th) log_prior(th) + log_lik(th) # Find the MAP estimate with the Hessian fit &lt;- optim(runif(1, 0, 1), log_posterior, method = &quot;L-BFGS-B&quot;, lower = 1e-5, upper = 1 - 1e-5, control = list(fnscale = -1), hessian = TRUE) fit$par # MAP &gt;# [1] 0.58 1 / sqrt(-fit$hessian[1]) # estimate of posterior standard deviation &gt;# [1] 0.0526 The graph below shows the normal approximation of the posterior distribution in our AIDS example. # par(mar = c(4, 4, 2, 0) + 0.1) # curve(dbeta(x, 52, 38), xlab = expression(theta), ylab = &quot;density&quot;) # curve(dnorm(x, fit$par, 1 / sqrt(-fit$hessian)), add = TRUE, # col = &quot;blue&quot;, lty = &quot;dashed&quot;) # legend(&quot;topright&quot;, c(&quot;Beta(52, 38)&quot;, &quot;Normal approximation&quot;), # lty = c(&quot;solid&quot;, &quot;dashed&quot;), col = c(&quot;black&quot;, &quot;blue&quot;)) ggplot(tibble(x = c(0, 1)), aes(x = x)) + stat_function(fun = dbeta, args = list(shape1 = 52, shape2 = 38), aes(col = &quot;Beta(52, 38)&quot;, linetype = &quot;Beta(52, 38)&quot;), size = 2) + stat_function(fun = dnorm, args = list(mean = fit$par, sd = 1 / sqrt(-fit$hessian[1])), aes(col = &quot;Normal approximation&quot;, linetype = &quot;Normal approximation&quot;)) + labs(x = expression(theta), y = &quot;Density&quot;, col = &quot;&quot;, linetype = &quot;&quot;) + scale_color_manual(&quot;&quot;, values = c(&quot;skyblue&quot;, &quot;blue&quot;)) + scale_linetype_manual(&quot;&quot;, values = c(&quot;dashed&quot;, &quot;solid&quot;)) &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() The advantage of using Laplace approximation is mainly computational, as it is generally much easier to obtain the maximum point of a function than to get the whole shape. 2.5.4 Markov Chain Monte Carlo (MCMC) The Markov Chain Monte Carlo (MCMC) simulation method is the modern way of approximating complex forms of the posterior distribution. The idea is analogous to treating the posterior distribution as the population, and then repeatedly draw samples from it. As you’ve learned in basic statistics, when you draw a large enough sample (say 1,000), the sample distribution should be very close to the population distribution. One tweak of MCMC from the above analogy is that the samples drawn are correlated, so that if the first sample is high, the next one is more likely to be high too. This is needed because we don’t have a direct way to draw samples from the posterior distribution, which usually has a very complex form; instead we have some algorithms that can indirectly get us to the posterior. The correlation among samples usually is not a big problem, except that we need to draw more samples to compensate for it. We will devote a whole week for MCMC in this course. Below is an example. The left panel is a trace plot showing how the value of \\(\\theta\\) changed for each sample (up to 500), and the right panel shows the sample distribution of 8,000 samples using MCMC. As you can see, the shape of the MCMC sample distribution closely approximates the true beta posterior distribution. # This is called the Metropolis algorithm log_prior_kernel &lt;- function(th) (46 - 1) * log(th) + (34 - 1) * log(1 - th) log_lik_kernel &lt;- function(th, y = 6, n = 10) y * log(th) + (n - y) * log(1 - th) log_posterior_kernel &lt;- function(th, y = 6, n = 10) { log_prior_kernel(th) + log_lik_kernel(th, y, n) } post &lt;- rep(NA, 1e4 + 1) post[1] &lt;- runif(1) for (i in seq_len(length(post) - 1)) { proposal &lt;- rnorm(1, post[i], 0.1) if (proposal &gt; 1 | proposal &lt; 0) { post[i + 1] &lt;- post[i] next } p_accept &lt;- exp(log_posterior_kernel(proposal) - log_posterior_kernel(post[i])) post[i + 1] &lt;- ifelse(runif(1) &lt; p_accept, proposal, post[i]) } # Discard the burn-in post &lt;- post[-(1:2000)] # # Plot the posterior samples # par(mfrow = c(1, 2), mar = c(4, 4, 2, 0) + 0.1) # plot.ts(post[1:500], xlab = &quot;iterations&quot;, ylab = expression(theta)) # plot(density(post, bw = &quot;SJ&quot;), xlab = expression(theta), main = &quot;&quot;) p1 &lt;- ggplot(tibble(iter = 1:500, th = post[1:500]), aes(x = iter, y = th)) + geom_line() + labs(x = &quot;iterations&quot;, y = expression(theta)) p2 &lt;- ggplot(tibble(th = post), aes(x = th)) + geom_density(bw = &quot;SJ&quot;) + labs(x = expression(theta)) + xlim(0, 1) gridExtra::grid.arrange(p1, p2, nrow = 1) With MCMC, one needs to check for convergence to make sure that the samples approximates the posterior well enough. We will discuss this in a later lecture. 2.6 Summarizing the Posterior Distribution Although the plot of the posterior is very useful and you should always plot it for any Bayesian analyses, it would also help to have some concise ways to summarize the posterior distribution, just like we tend to report the central tendency and the dispersion of a distribution. Here I will list the most common ways to summarize the posterior in research reports using Bayesian analyses. 2.6.1 Posterior Mean, Median, and Mode The first thing one can do to summarize the posterior distribution is to report some form of central tendency: Posterior Mean: also called Bayesian estimate or expected a posteriori (EAP), is probably the most commonly used point estimate. Posterior Median: Generally similar to the posterior mean as the posterior distribution tends to be symmetric for the problems discussed in this course. It is, however, more robust to outliers and may be a better summary when the posterior is highly skewed. Posterior Mode: also called maximum a posteriori (MAP), is the point with the highest posterior probability. It will be very similar to the maximum likelihood estimates in frequentist statistics when the prior is non-informative. However, it can be far away from the center when the distribution is skewed, or when it is not unimodal. As you can see, when the distribution is highly skewed, the three central tendency measures can be quite different. In general, it is recommended to use the posterior mean, unless the posterior distribution is clearly skewed where the posterior median would be more appropriate. The code below shows the posterior point estimates using grid approximation, conjugate prior, Laplace approximation, and MCMC. # Grid approximation: median_llpt &lt;- max(which(cumsum(post_dens) &lt; sum(post_dens) / 2)) median_interplolate &lt;- (sum(post_dens) / 2 - cumsum(post_dens)[median_llpt]) / (diff(cumsum(post_dens)[median_llpt + 0:1])) bayes_est_grid &lt;- c(mean = mean(post_dens * th_grid), median = median_interplolate * 0.01 + th_grid[median_llpt], mode = th_grid[which.max(post_dens)]) # Conjugate prior bayes_est_conj &lt;- c(mean = 52 / (52 + 38), median = qbeta(.50, 52, 38), mode = (52 - 1) / (52 + 38 - 2)) # Laplace approximation bayes_est_laplace &lt;- c(mean = fit$par, median = fit$par, mode = fit$par) # MCMC mcmc_dens &lt;- density(post, bw = &quot;SJ&quot;) bayes_est_mcmc &lt;- c(mean = mean(post), median = median(post), mode = mcmc_dens$x[which.max(mcmc_dens$y)]) # Assemble the results to a table tibble(grid = bayes_est_grid, conjugate = bayes_est_conj, Laplace = bayes_est_laplace, MCMC = bayes_est_mcmc) &gt;# # A tibble: 3 x 4 &gt;# grid conjugate Laplace MCMC &gt;# &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt;# 1 0.572 0.578 0.580 0.579 &gt;# 2 0.573 0.578 0.580 0.580 &gt;# 3 0.580 0.580 0.580 0.589 2.6.2 Uncertainty Estimates One should never report just a point estimate as representing the whole posterior distribution, just like you need a standard error for an estimated regression coefficient in frequentist statistics. The simplest and most common ways to obtain an uncertainty estimate is to get the standard deviation of the posterior distribution, or the posterior SD. You can see in our example, the posterior mean of \\(\\theta\\) is 0.578, and the posterior SD is 0.052. Another uncertainty estimate is the mean absolute deviation from the median (MAD) of the posterior distribution, which is more robust than the posterior SD when the distribution is long-tailed. It is, however, generally smaller than the posterior SD, and for a normal distribution, MAD = 0.8 \\(\\times\\) SD. The code below shows the posterior SD estimates using grid approximation, conjugate prior, Laplace approximation, and MCMC. tibble(grid = sqrt(sum((th_grid - mean(post_dens * th_grid))^2 * post_dens) / sum(post_dens)), conjugate = sqrt(52 * 38 / (52 + 38 + 1)) / (52 + 38), Laplace = 1 / sqrt(-fit$hessian), MCMC = sd(post)) &gt;# # A tibble: 1 x 4 &gt;# grid conjugate Laplace[,1] MCMC &gt;# &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt;# 1 0.0521 0.0518 0.0526 0.0512 2.6.3 Credible Intervals The credible interval is analogous to the confidence interval in frequentist statistics, but with very different interpretations. For example, a 90% credible interval is the interval that has 90% probability of containing the true value of the parameter, an interpretation that is commonly and mistakenly associated with the confidence interval. For frequentist, as the population parameter is fixed, one cannot use probability for the parameter; instead, only the sample is probabilistic, and one has to interpret a 90% confidence interval in the sense that 90% of the intervals constructed with repeated sampling will contain the true parameter. On the hand, for Bayesian one can directly say that there is a 90% probability for the true value to be in the credible interval, without any reference to samples that could have been observed and were not. For any given posterior distribution, there are infinitely many intervals that have a C% (e.g., 90% or 95%) probability of containing the true parameter value. For example, the blue region in both graphs below are 50% credible intervals of the \\(\\mathrm{Beta}(52, 38)\\) distribution. However, there are some principles to help us choose which interval to use. The first one is to choose a interval such that every value included in the interval have a posterior probability that is at least as high as those for the values outside of the interval. There generally will only be one such interval for a given posterior distribution, and such interval is called a highest posterior density interval (HPDI). The graph on the left below is the 50% HPDI for a \\(\\mathrm{Beta}(52, 38)\\) distribution. Because the posterior distribution is close to symmetric in the above example, the HPDI and the equal-tailed CI are very similar. They can be quite different, however, when the posterior distribution is skewed, like the following: Another advantage of the HPDI is that among all credible intervals, it will have the shortest length. On the other hand, the computation of HPDI is usually not easy, making it not very commonly used in real Bayesian research. Another approach is to compute what is called an equal-tailed credible interval. The idea is to have an interval such that the probability on the left tail outside of the interval is equal to the probability on the right tail outside of the interval. For example, for a 50% equal-tailed interval, the lower limit will be the 25th percentile of the posterior distribution, whereas the upper limit will be the 75th percentile, as shown in the graph on the right. Although ideally an HPDI should be preferred, in practice the posterior distribution is usually quite symmetric and so the HPDI and the equal-tailed credible interval are similar. Also, the HPDI tends to be less stable for simulation-based methods to approximate the posterior distribution. So in general, you should use the equal-tailed credible interval to summarize the results. 2.6.4 Probability of \\(\\theta\\) Higher/Lower Than a Certain Value Besides point and interval estimates and uncertainty estimates, one can answer questions such as “What is the probability that \\(\\theta &lt; 0.5\\)?” In our example, we can find the posterior probability that \\(\\theta &lt; 0.5\\) as pbeta(0.5, 52, 38) &gt;# [1] 0.0687 In mathematical notation, \\(P(\\theta &lt; 0.5 | y) \\approx 0.069\\). So \\(\\theta = 0.58\\) is the most likely value, and it’s unlikely that \\(\\theta &lt; 0.5\\). 2.7 Model Checking In Bayesian statistics, a common way to check your model is to perform what is called the posterior predictive check (Gelman, Meng, and Stern 1996). To understand this, we need to discuss the posterior predictive distribution first. 2.7.1 Posterior Predictive Distribution A purpose of statistical model is to to make future observations. For example, with our binomial model and with an estimated \\(\\theta\\) of 0.578, if we were to obtain 10 more observations, we expect the number of deaths to have the following probability distribution: Notation-wise, we can write this distribution as \\(P(\\tilde y | \\theta = \\theta_1)\\), where \\(\\tilde y\\) represents a new data point that is different from the current data \\(y\\), \\(\\theta_1\\) is the posterior mean. However, one thing that is strongly emphasized in Bayesian statistics is to incorporate all the uncertainty in the results. Therefore, as \\(\\theta_1\\) is nothing but one possible value for \\(\\theta\\), we should include every value of \\(\\theta\\) for our prediction. The following are a few more examples: So to get the best prediction, we can “average” across the predictions across different values of \\(\\theta\\). However, we should not be treating each value of \\(\\theta\\) as equally credible; instead, the posterior distribution of \\(\\theta\\) tells us which values are more likely to be true. Therefore, in getting the best prediction, we should weight the prediction for each value of \\(\\theta\\) by the corresponding posterior probability. The resulting distribution is the posterior predictive distribution. In mathematical notation, it is \\[P(\\tilde y | y) = \\int_\\theta p(\\tilde y | \\theta, y) p(\\theta | y) \\; \\mathrm{d}\\theta\\] (See a video at https://www.youtube.com/watch?v=R9NQY2Hyl14) The formula is for your reference only, as we will not be using the formula to obtain posterior prediction in this course. Instead, we will be using simulations to approximate the posterior predictive distribution of \\(\\tilde y\\), which is just a straight-forward extension of the approximation of the posterior distribution of \\(\\theta\\). We will discuss more in later chapters; for now, note that the posterior predictive distribution for our example is: The bar in blue highlights the observed sample. 2.8 Posterior Predictive Check After obtaining the posterior predictive distribution, the posterior predictive check simply compares the observed data with the prediction from the model, both graphically and mathematically. The above graph shows that \\(\\tilde y = 6\\) is the most likely among all options, which says that our model fits the data well in that regard. Another common check is to plot a few simulated samples of the predicted data, and compared those to the data observed: The example is overly simple so there is not much to tell aside from checking whether the graphs look similar, and in this case we don’t see systematic differences between the observed data and the predicted ones. We will talk more about posterior predictive check in later weeks in the context of different models. 2.9 Summary In this chapter we were introduced the work flow for conducting Bayesian data analysis, with an example using the subsample of the AIDS data set. We talked about choosing a statistical model that is assumed to have generated the observed data, specifying a prior for the model parameter, and four ways to obtain the posterior distributions. We also talked about ways to summarize the posterior distribution, and the use of posterior predictive check to assess the fit of the model to the observed data. You will see the same work flow and concepts again and again for the remaining lectures; indeed, most of the remaining lectures are merely some commonly used models for common problems in the social sciences. That’s why people describe the act of doing Bayesian analyses as “turning the Bayesian crank.” 2.9.1 Key Concepts Model Laplace approximation credible interval Likelihood posterior mean (EAP) highest posterior density interval binomial distribution   posterior median posterior predictive distribution conjugate prior posterior mode (MAP) posterior predictive check approximation posterior standard deviation   References "],
["one-parameter-models.html", "Chapter 3 One-Parameter Models 3.1 Binomial/Bernoulli data 3.2 Poisson Data", " Chapter 3 One-Parameter Models This lecture surveys some very simple models with one parameter for some real research data. In your research it is rare to study only one variable with one parameter, but the concepts you learn in this lecture will naturally generalize to more regular statistical models, like regression and multilevel models. Specifically we will consider two models: Binomial/Bernoulli data Poisson data 3.1 Binomial/Bernoulli data You’ve already seen the binomial model in the AIDS example. When you have one event with two outcomes, like flipping a coin or sampling a respondent and asking whether he or she has a college degree, the variable is called a Bernoulli variable. Mathematically we will denote one outcome as success and coded as 1, and the other as failure and coded as 0 (poor terminology may be, but that’s by convention). For example, if the variable is coin flipping, we have 1 = tail, 0 = head. If the variable is whether the respondent has a college degree, it can be 1 = college or above, 0 = no college. Therefore, in the AIDS example, each observation is considered a “Bernoulli” outcome (Alive vs. Dead). An additional thing to note for the Bernoulli/binomial model is that, instead of setting the prior on \\(\\theta\\), sometimes we’re more interested in setting the prior for a transformed parameter that has values between \\(-\\infty\\) and \\(\\infty\\), such as one on the logit scale (as related to logistic regression). Instead of the subsample, here we will use the full sample in the Aids2 data set, with the same informative prior of Beta(46, 34). In the full data, we have data(&quot;Aids2&quot;, package = &quot;MASS&quot;) summary(Aids2$status) &gt;# A D &gt;# 1082 1761 So based on the conjugacy, the posterior of \\(\\theta\\) is Beta(1,807, 1,116). 3.1.1 Reparameterization* A problem in choosing an uninformative prior is that the prior is not invariant to reparameterization. Reparameterization means, instead of estimating the parameter in one way, we use another parameter that can be expressed as a function of the parameter we originally have. The easiest example would be for a normal distribution, you can estimate the variance, or you can estimate the standard deviation, which is the square root of the variance. Therefore, the SD can be considered a reparameterization of the variance, and vice versa. In a Bernoulli/Binomial model, one common reparameterization is to define \\(\\varphi = \\log[\\theta / (1 - \\theta)]\\), which is the logit of \\(\\theta\\). Here is the relationship between \\(\\varphi\\) and \\(\\theta\\): This reparameterization is popular because it is commonly used in logistic regression, and is actually a naturally parameter for a binomial distribution for some theoretical reason. Instead of having a support of [0, 1], it has a support of (\\(-\\infty\\), \\(\\infty\\)), or the whole real number line. However, a uniform prior on \\(\\theta\\) will not transform to a uniform prior on \\(\\varphi\\). If we take a large sample of \\(\\theta\\) from a uniform distribution: th_samples &lt;- runif(1e5, 0, 1) and transform each value to \\(\\varphi\\): phi_samples &lt;- log(th_samples / (1 - th_samples)) The resulting prior on \\(\\varphi\\) will be ggplot(tibble(phi = phi_samples), aes(x = phi)) + geom_density(bw = &quot;SJ&quot;) + labs(x = expression(varphi)) which assigns more density to \\(\\varphi = 0\\). Although generally you will get very similar results using either a non-informative prior on \\(\\theta\\) or on the logit \\(\\varphi\\), this non-invariance is one major critique on Bayesian from frequentist, especially the use of non-informative prior. The graph below shows, in black, the posterior of \\(\\varphi\\) for a uniform prior on \\(\\theta\\) for the subsample of size 10, and, in red, the posterior of \\(\\varphi\\) for a \\(\\mathcal{N}(0, 10)\\) prior on \\(\\varphi\\). They match pretty well. 3.1.2 Posterior Predictive Check Now, we need to know whether the model fit the data well. If we only have the status variable we don’t have much to check for a binomial model. However, as there is information for other variables, we can use them to check the exchangeability assumption. For example, we can ask whether the data from different state categories are exchangeable. The death rate across the 4 state categories are &gt;# status &gt;# state A D &gt;# NSW 664 1116 &gt;# Other 107 142 &gt;# QLD 78 148 &gt;# VIC 233 355 &gt;# status &gt;# state A D &gt;# NSW 0.373 0.627 &gt;# Other 0.430 0.570 &gt;# QLD 0.345 0.655 &gt;# VIC 0.396 0.604 Now, we can generate some predictions from our posterior distribution and our model. So the observed data (the first subgraph) look similar to the simulated data. We can also conduct a posterior predictive check by defining a test statistic, here we will be using the difference between the highest death rate and the lowest death rate across the 4 state categories: &gt;# `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The posterior predictive p-value (ppp) using a test statistic of the ratio between the highest death rate and the lowest death rate among the 4 state categories is 0.094. ppp is defined as the probability of drawing a sample from the posterior predictive distribution with a test statistic that is as high as the observed sample. Mathematically, if we define the test statistic as \\(T(y)\\), it is represented as \\(\\mathit{ppp} = P(T(\\tilde y) &gt; T(y) | y)\\)}, and it’s conditional on \\(y\\) because it is based on the posterior distribution. We generally do not apply the same cutoff of \\(p &lt; .05\\) as in frequentist statistics, which is a convention under strong criticism. Instead, what Bayesian strives for would be to get a posterior predictive \\(p\\)-value close to .50 for the key test statistic. Therefore, given the check, we should consider modeling the data as four different states, or better by using a hierarchical model. 3.1.3 Comparison to frequentist results Using maximum likelihood, the estimated death rate would be \\(\\hat \\theta = 1761 / 2843 = 0.619\\), with a standard error (SE) of \\(\\sqrt{0.619 (1 - 0.619) / n} = 0.009\\), with a 90% confidence interval of \\([0.604, 0.634]\\), which is similar to the interval with Bayesian inference. 3.1.4 Sensitivity to different priors You can see one needs a very strong prior (equivalent to 600 data points) and with the prior and the data not agreeing to get a substantially different conclusion. The \\(\\mathrm{Beta}(1 / 2, 1 / 2)\\) distribution is called a Jeffreys prior (https://en.wikipedia.org/wiki/Jeffreys_prior), which is derived according to some statistical principles for different models. One big advantage of a Jeffreys prior is that it is invariant, meaningful that the prior will stay the same even under reparameterization. However, like conjugate priors, Jeffreys prior limit the choice of prior even when there are true prior information available. 3.2 Poisson Data A Poisson distribution is suitable for count data in a fixed interval, when the average rate of the event happening is constant, and when the time from one occurrence of the event to the next is independent. For example, think about the number of e-mails for a person each day. If we know on average a person has 20 emails a day, and we assume that emails from different sources arrive independently, then we can model the number of emails in different days as a Poisson distribution. The Poisson distribution has one rate parameter, usually denote as \\(\\lambda\\), which is also the mean of a Poisson distribution. Below are a few examples of Poisson distributions with different \\(\\lambda\\): As you can see, the larger \\(\\lambda\\) is, the closer the distribution is to a normal distribution. One important property of a Poisson distribution is that the mean and the variance is always the same. Therefore, if you have a Poisson distribution with \\(\\lambda = 3\\), its variance must be \\(3\\), and so its SD is \\(\\sqrt{3} = 1.732\\). This will be an important property to check the fit of a Poisson model. 3.2.1 Example 2 # Download data file download.file(&quot;https://files.osf.io/v1/resources/47tnc/providers/osfstorage/553e51f98c5e4a21991987e7?action=download&amp;version=1&amp;direct&quot;, &quot;../data/redcard_data.zip&quot;) redcard_dat &lt;- readr::read_csv(&quot;../data/redcard_data.zip&quot;) %&gt;% filter(leagueCountry == &quot;England&quot;) %&gt;% group_by(player) %&gt;% summarise(rater_dark = (mean(rater1) + mean(rater2)) / 2, yellowCards = sum(yellowCards), redCards = sum(redCards)) We will use a data set by (Silberzahn et al. 2018), which the authors sent out to 29 research teams for analyses (read the paper for more information on the context). We will use a subset of the data, which contains information of 564 soccer players playing in England, and the total number of red cards (which happens when they committed a foul deemed serious by the referee of the match and will be sent off the match). Below is a summary and a plot of the data, and a theoretical Poisson distribution that matches the mean of the data (i.e., 0.9. &gt;# player rater_dark yellowCards redCards &gt;# Length:564 Min. :0.0 Min. : 0.0 Min. :0.0 &gt;# Class :character 1st Qu.:0.0 1st Qu.: 9.0 1st Qu.:0.0 &gt;# Mode :character Median :0.2 Median : 22.0 Median :0.0 &gt;# Mean :0.3 Mean : 27.8 Mean :0.9 &gt;# 3rd Qu.:0.5 3rd Qu.: 41.0 3rd Qu.:1.0 &gt;# Max. :1.0 Max. :146.0 Max. :7.0 &gt;# NA&#39;s :178 As you can see, the Poisson distribution describes the skewness of the data reasonably well, but there are more zeros in the data than what the Poisson would expect. However, we will proceed with the Poisson model first in this lecture, and we may deal with the extra zeros later in the cocurse. 3.2.2 Choosing a model With exchangeability, the model will be \\[\\mathtt{redCards}_i \\sim \\mathrm{Pois}(\\lambda)\\] 3.2.3 Choosing a prior A conjugate prior for the Poisson model is the Gamma distribution family, which has two parameters: \\(a\\) is the “shape” parameter, and \\(b\\) is the “scale” parameter. Notice that \\(\\lambda\\) has a support (i.e., acceptable values) of \\([0, \\infty)\\). \\(a\\) and \\(b\\) have (roughly) the following interpretations: \\(b\\): Number of prior observations \\(a\\): Sum of the counts in prior observations However, with the improvement in computational speed, the need to use a conjugate prior becomes relatively small, and so the for this example I will show you the use of a non-conjugate prior. A general strategy to using a weakly informative prior is to first transform the parameter into one with a support of \\([-\\infty, \\infty]\\), and the natural log transformation is common for the Poisson rate parameter. 3.2.4 Model Equations and Diagram Our Poisson model and prior are thus \\[\\begin{align} \\mathtt{redCards}_i &amp; \\sim \\mathrm{Pois}(\\lambda) \\\\ \\log(\\lambda) &amp; \\sim \\mathcal{N}(0, \\sigma_{\\log(\\lambda)}), \\end{align}\\] where \\(\\sigma_{\\log(\\lambda)}\\) is the standard deviation (not variance) of the normal prior, which controls the informativeness of our prior. In this case, \\(\\log(\\lambda)\\) = 2.5 would correspond to \\(\\lambda\\) = 12.182 and \\(\\log(\\lambda)\\) = -2.5 would correspond to \\(\\lambda\\) = 0.082, and it seems unlikely for \\(\\lambda\\) to be outside of this range. Therefore, I will choose \\(\\sigma_{\\log(\\lambda)}\\) = 2.5. The priors are plotted below: p1 &lt;- ggplot(tibble(log_lambda = c(-10, 10)), aes(x = log_lambda)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2.5)) + labs(x = expression(log(lambda)), y = &quot;Density&quot;) p2 &lt;- ggplot(tibble(lambda = c(0, 20)), aes(x = lambda)) + stat_function(fun = function(x) dnorm(log(x), mean = 0, sd = 2.5), n = 501) + labs(x = expression(lambda), y = &quot;Density&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) Here is a graphical sketch of the model and the prior using the R package DiagrammeR: DiagrammeR::grViz(&quot; digraph boxes_and_circles { # a &#39;graph&#39; statement graph [overlap = true, fontsize = 10] # data node [shape = box, fixedsize = true] Y [label = &#39;y@_{i}&#39;] # parameters node [shape = circle] loglambda [label = &#39;log(&amp;lambda;)&#39;] # transformed parameters node [shape = circle, peripheries = 2] lambda [label = &#39;&amp;lambda;&#39;] # fixed values in prior node [shape = circle, peripheries = 1] mu_loglambda [label = &#39;&amp;mu;@_{log(&amp;lambda;)}&#39;]; sigma_loglambda [label = &#39;&amp;sigma;@_{log(&amp;lambda;)}&#39;] # paths mu_loglambda -&gt; loglambda; sigma_loglambda -&gt; loglambda; loglambda -&gt; lambda; lambda -&gt; Y; } &quot;) Note that there currently is not a widely accepted standard for drawing these kinds of graphs, but as long as you have a reasonable and consistent way to represent them it should be fine. The double bordered node is used to show a deterministic node (i.e., a transformation of some variables). 3.2.5 Getting the posterior With a non-conjugate prior for a Poisson model, we will need to use MCMC. Below is the STAN code and the R code for running the model with our prior. We’ll talk more about STAN in later weeks; for now, just use the code to run the example. data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; y[N]; // data array (counts); } parameters { real log_lambda; // log of rate parameter } model { y ~ poisson_log(log_lambda); // prior log_lambda ~ normal(0, 5); } generated quantities { real lambda = exp(log_lambda); int yrep[N]; for (i in 1:N) { yrep[i] = poisson_log_rng(log_lambda); } } library(rstan) rstan_options(auto_write = TRUE) m2 &lt;- stan(file = &quot;../codes/poisson_model.stan&quot;, data = list(N = 564, y = redcard_dat$redCards), iter = 800, chains = 2, cores = 2) Below is a summary of the posterior distributions print(m2, pars = c(&quot;log_lambda&quot;, &quot;lambda&quot;)) &gt;# Inference for Stan model: poisson_model. &gt;# 2 chains, each with iter=800; warmup=400; thin=1; &gt;# post-warmup draws per chain=400, total post-warmup draws=800. &gt;# &gt;# mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat &gt;# log_lambda -0.11 0 0.04 -0.19 -0.14 -0.11 -0.08 -0.02 285 1.02 &gt;# lambda 0.90 0 0.04 0.82 0.87 0.90 0.93 0.98 288 1.02 &gt;# &gt;# Samples were drawn using NUTS(diag_e) at Fri Mar 6 10:07:09 2020. &gt;# For each parameter, n_eff is a crude measure of effective sample size, &gt;# and Rhat is the potential scale reduction factor on split chains (at &gt;# convergence, Rhat=1). The posterior density and the 95% CI (credible interval) for \\(\\lambda\\) is bayesplot::mcmc_areas(m2, pars = c(&quot;log_lambda&quot;, &quot;lambda&quot;), prob = 0.95) The frequentist maximum likelihood estimate for \\(\\lambda\\) is the mean of the data, which is \\(\\hat \\lambda = 0.897\\), and the SE is the square root of the sample mean, 0.04, with a 90% confidence interval [0.819, 0.975]. So the Bayesian results are similar, with a slightly more precise inference (slightly smaller SE). 3.2.6 Posterior Predictive Check As now you should know, always generate some simulated predictions from the model to check your results. bayesplot::ppc_bars(redcard_dat$redCards, yrep = as.matrix(m2, pars = &quot;yrep&quot;)) There’s also a useful graphical tool, rootogram, for diagnosing count models bayesplot::ppc_rootogram(redcard_dat$redCards, yrep = as.matrix(m2, pars = &quot;yrep&quot;), style = &quot;hanging&quot;) As can be seen, the predicted counts were off a little bit. Things that can improve the model includes: Using a different distribution than Poisson; Using a zero-inflated Poisson as there were more zeros in the data; Including predictors in a Poisson regression; Adjusting for the number of games each player played. References "],
["brief-introduction-to-stan.html", "Chapter 4 Brief Introduction to STAN 4.1 STAN 4.2 RStan 4.3 Resources", " Chapter 4 Brief Introduction to STAN The engine used for running the Bayesian analyses covered in this course is STAN, as well as the rstan package that allows it to interface with R. STAN requires some programming from the users, but the benefit is that it allows users to fit a lot of different kinds of models. The goal of this lecture is not to make you an expert of STAN; I myself only have used maybe just 1 or 2% of the power of STAN. Instead, the goal is to give you a brief introduction with some sample codes, so that you can study further by yourself, and estimate models that no frequentist estimation exists yet. 4.1 STAN STAN (http://mc-stan.org/) is itself a programming language, just like R. Strictly speaking it is not only for Bayesian methods, as you can actually do penalized maximum likelihood and automatic differentiation; however, it is most commonly used as an MCMC sampler for Bayesian analyses. It is written in C++, which makes it much faster than R (R is actually quite slow as a computational language). You can actually write a STAN program without calling R or other software, although eventually you may want to use statistical software to post-process the posterior samples after running MCMC. There are interfaces of STAN for different programs, including R, Python, MATLAB, Julia, Stata, and Mathematica, and for us we will be using the RStan interface. 4.1.1 STAN code In STAN, you need to define a model using the STAN language. Below is an example for the Poisson model, which is saved with the file name \"poisson_model.stan\". data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; y[N]; // data array (counts); } parameters { real log_lambda; // log of rate parameter } model { y ~ poisson_log(log_lambda); // prior log_lambda ~ normal(0, 5); } generated quantities { real lambda = exp(log_lambda); int yrep[N]; for (i in 1:N) { yrep[i] = poisson_log_rng(log_lambda); } } In STAN, anything after // denotes comments and will be ignored by the program, and in each blocks (e.g., data {}) a statement needs to be ended by a semicolon (;). There are several blocks in the above STAN code: data: The data for input for STAN is usually not only a data set, but include other information, including sample size, number of predictors, and prior scales. Each type of data has an input type, such as int = integer, real = numbers with decimal places, matrix = 2-dimensional data of real numbers, vector = 1-dimensional data of real numbers, and array = 1- to many-dimensional data. For example y[N] is a one-dimensional array of integers. you can set the lower and upper bounds so that STAN can check the input data parameters: The parameters to be estimated transformed parameters: optional variables that are transformation of the model parameters. It is usually used for more advanced models to allow for more efficient MCMC sampling. model: It includes definition of priors for each parameter, and the likelihood for the data. There are many possible distributions that can be used in STAN. generated quantities: Any quantities that are not part of the model but can be computed from the parameters for every iteration. Examples include posterior generated samples, effect sizes, and log-likelihood (for fit computation). 4.2 RStan STAN is written in C++, which is a compiled language. This is different from programs like R, which you can input a command and get results right away. In contrast, a STAN program needs to be converted to something that can be executed in your computer. The benefit, however, is that the programs can be run much faster after the compilation process. To feed data from R to STAN, and import output from STAN to R, you will use the rstan package (https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started). Then, follow the following steps: We will continue with the red card example: redcard_dat &lt;- readr::read_csv(&quot;../data/redcard_data.zip&quot;) %&gt;% group_by(player) %&gt;% summarise(rater_dark = (mean(rater1) + mean(rater2)) / 2, yellowCards = sum(yellowCards), redCards = sum(redCards)) 4.2.1 Assembling data list in R First, you need to assemble a list of data for STAN input, which should match the specific STAN program. In the STAN program we define two components (N and y) for data, so we need seven elements in an R list: pois_sdata &lt;- list( N = nrow(redcard_dat), # number of observations y = redcard_dat$yellowCards # outcome variable (yellow card) ) 4.2.2 Call rstan library(rstan) &gt;# Loading required package: StanHeaders &gt;# rstan (Version 2.19.3, GitRev: 2e1f913d3ca3) &gt;# For execution on a local, multicore CPU with excess RAM we recommend calling &gt;# options(mc.cores = parallel::detectCores()). &gt;# To avoid recompilation of unchanged Stan programs, we recommend calling &gt;# rstan_options(auto_write = TRUE) &gt;# &gt;# Attaching package: &#39;rstan&#39; &gt;# The following object is masked from &#39;package:tidyr&#39;: &gt;# &gt;# extract rstan_options(auto_write = TRUE) m3 &lt;- stan(file = &quot;../codes/poisson_model.stan&quot;, data = pois_sdata, # Below are optional arguments iter = 2000, chains = 4, cores = min(parallel::detectCores(), 4)) 4.2.3 Summarize the results After you call the stan function in R, it will compile the STAN program, which usually takes a minute or so. Then it starts sampling. You can now see a summary of the results by printing the results: print(m3, pars = c(&quot;lambda&quot;, &quot;log_lambda&quot;)) &gt;# Inference for Stan model: poisson_model. &gt;# 4 chains, each with iter=2000; warmup=1000; thin=1; &gt;# post-warmup draws per chain=1000, total post-warmup draws=4000. &gt;# &gt;# mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat &gt;# lambda 27.67 0 0.12 27.42 27.59 27.67 27.75 27.89 1156 1 &gt;# log_lambda 3.32 0 0.00 3.31 3.32 3.32 3.32 3.33 1155 1 &gt;# &gt;# Samples were drawn using NUTS(diag_e) at Fri Mar 6 10:07:20 2020. &gt;# For each parameter, n_eff is a crude measure of effective sample size, &gt;# and Rhat is the potential scale reduction factor on split chains (at &gt;# convergence, Rhat=1). And you can also use the shinystan package to visualize the results: shinystan::launch_shinystan(m3) 4.3 Resources STAN is extremely powerful and can fit almost any statistical models, but the price is that it takes more effort to code the model. To learn more about STAN, please check out http://mc-stan.org/documentation/ for the manual, examples of some common models, and case studies (which includes more complex models like item response theory). See https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html for a vignettes for working with the rstan package. As you see, fitting simple models in STAN may sometimes be more work, but as we go further we will use the brms program that simplify the process for many commonly used models, such as regression and multilevel models. On the other hand, for truly complex models, STAN is actually a lifesaver as it would be extremely hard to fit some of those models with other approaches. "],
["group-comparisons.html", "Chapter 5 Group Comparisons 5.1 Data 5.2 Between-Subject Comparisons 5.3 Notes on Model Comparison 5.4 Within-Subject Comparisons", " Chapter 5 Group Comparisons Last week we discussed the binomial and the Poisson model, both of which are models with only one parameters. In most research, however, there are at least two parameters, and sometimes one is talking about tens or hundreds of parameters. For this week we will be looking at group comparisons done in a Bayesian way. We will start with between-group comparisons, extend it to multiple groups, and then move on to comparisons for repeated measures. 5.1 Data We will use the data in Frank, Biberci, and Verschuere (2019), with is a replication study examining the response time (RT) for lying and whether RT is shorter or longer when lying in native language. In the original study, the researchers found that the difference between telling a lie and a truth (lie-truth difference) is smaller in English than in German for native German speakers. In the replication study, the same design was used to see whether the findings can be replicated on Dutch speakers. The data can be found at https://osf.io/x4rfk/. Instead of looking at lie-truth difference across languages, we will start off by comparing difference in RT for telling a lie in Dutch between males (man) and females (vrouw), as the model is easier to understand for independent sample comparisons, and the model can be applied generally for between-subject experimental designs. lies &lt;- readxl::read_excel(&quot;../data/ENDFILE.xlsx&quot;) # Rescale response time from ms to sec lies &lt;- lies %&gt;% mutate_at(vars(LDMRT:TEMRT), ~ . / 1000) # Describe the data psych::describe(lies %&gt;% select(Age, LDMRT:TEMRT)) &gt;# vars n mean sd median trimmed mad min max range skew kurtosis &gt;# Age 1 66 23.15 7.18 21.00 21.61 2.97 18.00 61.00 43.00 2.98 10.79 &gt;# LDMRT 2 63 1.47 0.51 1.41 1.42 0.47 0.53 3.26 2.73 0.97 1.41 &gt;# TDMRT 3 63 1.16 0.43 1.09 1.10 0.35 0.51 2.94 2.43 1.54 3.27 &gt;# LEMRT 4 63 1.45 0.46 1.41 1.43 0.50 0.49 2.42 1.93 0.29 -0.80 &gt;# TEMRT 5 63 1.29 0.41 1.28 1.26 0.42 0.57 2.60 2.02 0.86 0.69 &gt;# se &gt;# Age 0.88 &gt;# LDMRT 0.06 &gt;# TDMRT 0.05 &gt;# LEMRT 0.06 &gt;# TEMRT 0.05 # Plot the data psych::pairs.panels(lies %&gt;% select(Age, Gender, LDMRT:TEMRT)) 5.2 Between-Subject Comparisons 5.2.1 Plots lies %&gt;% select(PP, Gender, LDMRT, TDMRT) %&gt;% gather(key = &quot;veracity&quot;, value = &quot;RT&quot;, LDMRT:TDMRT) %&gt;% ggplot(aes(x = RT, col = veracity)) + geom_density(bw = &quot;SJ&quot;) + facet_wrap(~ Gender) &gt;# Warning: Removed 6 rows containing non-finite values (stat_density). 5.2.2 Independent sample t-test # independent t-test t.test(lies$LDMRT[which(lies$Gender == &quot;man&quot;)], lies$LDMRT[which(lies$Gender == &quot;vrouw&quot;)]) &gt;# &gt;# Welch Two Sample t-test &gt;# &gt;# data: lies$LDMRT[which(lies$Gender == &quot;man&quot;)] and lies$LDMRT[which(lies$Gender == &quot;vrouw&quot;)] &gt;# t = 4, df = 34, p-value = 3e-04 &gt;# alternative hypothesis: true difference in means is not equal to 0 &gt;# 95 percent confidence interval: &gt;# 0.255 0.778 &gt;# sample estimates: &gt;# mean of x mean of y &gt;# 1.81 1.30 5.2.3 Bayesian Normal Model The \\(t\\)-test above does not directly show the underlying model. In fully Bayesian analyses, this has to be made explicit. A statistical model states the underlying assumptions of the statistical procedure. And to say more directly, a model is mostly a set of distributional assumptions. Below is the one for the \\(t\\)-test, including some prior choices. \\[\\begin{align} \\texttt{LDMRT}_{ij} &amp; \\sim \\mathcal{N}^+(\\mu_j, \\sigma) \\\\ \\mu_j &amp; = \\begin{cases} \\mu_1, &amp; j = 1 \\\\ \\mu_1 + \\beta, &amp; j = 2 \\end{cases} \\\\ \\mu_1 &amp; \\sim \\mathcal{N}^+(0.5, 2.5) \\\\ \\beta &amp; \\sim \\mathcal{N}(0, 2.5) \\\\ \\sigma &amp; \\sim t^+(4, 0, 1) \\end{align}\\] Parameters: \\(\\mu_j\\): mean parameter for the \\(j\\)th group where \\(j\\) = 1 for man and \\(j\\) = 2 for vrouw \\(\\sigma\\): standard deviation parameter that’s assumed the same across the two gender groups \\(\\beta\\): mean difference between the two genders Transformed parameters: \\(\\mu_2\\) The above prior distributions are called weakly informative. They are informative because it does give some informative on what the most likely value is for each parameter prior to looking at the data, such as 0.5 for \\(\\mu_1\\). On the other hand, it is weak because the most likely values are basically covering all values that are reasonable. For example, a normal prior with a mean 0 and an SD of 2.5 for \\(\\beta\\) represents a belief that the difference in RT between males and females has a 68% chance to be no more than 2.5 seconds. Given that most people took just no more than two seconds to respond, this is a pretty weak prior. For \\(\\sigma\\), its lower bound is 0. The prior used here is a half-\\(t\\) distribution based on Gelman (2006). You may find this guide: https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations to be useful when choosing priors for a lot of the models we will discuss in this course. Below are the graphs for the three priors: p1 &lt;- ggplot(tibble(mu = c(0, 5)), aes(x = mu)) + stat_function(fun = function(x) dnorm(x, mean = 0.5, sd = 2.5) / (1 - pnorm(0, 0.5, sd = 2.5))) + labs(x = expression(mu), y = &quot;Density&quot;) p2 &lt;- ggplot(tibble(beta = c(-5, 5)), aes(x = beta)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2.5)) + labs(x = expression(beta), y = &quot;Density&quot;) p3 &lt;- ggplot(tibble(sig = c(0, 5)), aes(x = sig)) + stat_function(fun = function(x) dt(x, df = 2) * 2) + labs(x = expression(sigma), y = &quot;Density&quot;) gridExtra::grid.arrange(p1, p2, p3, nrow = 2) 5.2.3.1 Model Diagram 5.2.3.2 STAN Below is the STAN code for the model: data { int&lt;lower=0&gt; N1; // number of observations (group 1) int&lt;lower=0&gt; N2; // number of observations (group 2) vector[N1] y1; // response time (group 1); vector[N2] y2; // response time (group 2); } parameters { real&lt;lower=0&gt; mu_1; // mean of group 1 real beta; // difference in means real&lt;lower=0&gt; sigma; // pooled standard deviation } transformed parameters { real&lt;lower=0&gt; mu_2 = mu_1 + beta; } model { y1 ~ normal(mu_1, sigma); y2 ~ normal(mu_2, sigma); // prior mu_1 ~ normal(0.5, 2.5); beta ~ normal(0, 2.5); sigma ~ student_t(4, 0, 1); } generated quantities { real y1rep[N1]; real y2rep[N2]; for (i in 1:N1) { y1rep[i] = normal_rng(mu_1, sigma); } for (i in 1:N2) { y2rep[i] = normal_rng(mu_2, sigma); } } And let’s fit the model with rstan. library(rstan) rstan_options(auto_write = TRUE) # Exclude missing values lies_cc &lt;- drop_na(lies, LDMRT) m1 &lt;- stan(&quot;../codes/group_comparison.stan&quot;, data = list(N1 = sum(lies_cc$Gender == &quot;man&quot;), N2 = sum(lies_cc$Gender == &quot;vrouw&quot;), y1 = lies_cc$LDMRT[which(lies_cc$Gender == &quot;man&quot;)], y2 = lies_cc$LDMRT[which(lies_cc$Gender == &quot;vrouw&quot;)])) # Use the `broom` package to generate nicely formatted table broom::tidy(m1, pars = c(&quot;mu_1&quot;, &quot;mu_2&quot;, &quot;beta&quot;, &quot;sigma&quot;), conf.int = TRUE, conf.method = &quot;quantile&quot;) &gt;# # A tibble: 4 x 5 &gt;# term estimate std.error conf.low conf.high &gt;# &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt;# 1 mu_1 1.81 0.0999 1.61 2.00 &gt;# 2 mu_2 1.30 0.0724 1.15 1.44 &gt;# 3 beta -0.511 0.125 -0.759 -0.265 &gt;# 4 sigma 0.463 0.0427 0.388 0.557 plot(m1, pars = c(&quot;mu_1&quot;, &quot;mu_2&quot;, &quot;beta&quot;, &quot;sigma&quot;)) &gt;# ci_level: 0.8 (80% intervals) &gt;# outer_level: 0.95 (95% intervals) 5.2.3.3 Effect size For mean comparisons, it is also common to report a standardized effect size. For example, Cohen’s \\(d\\) was defined as: \\[d = \\frac{\\mu_2 - \\mu_1}{\\sigma}\\] We can directly put that in the STAN code for generated quantities, such as generated quantities { real cohen_d = (mu_2 - mu_1) / sigma; } However, if you have already run STAN, like what we did here, we can easily compute the effect size using the posterior samples of beta and sigma, as explained in the code below: # Extract posterior samples of the beta and the sigma parameters post_sam &lt;- as.data.frame(m1, pars = c(&quot;beta&quot;, &quot;sigma&quot;)) # Compute Cohen&#39;s d for each iteration post_sam$cohen_d &lt;- post_sam$beta / post_sam$sigma # Posterior density bayesplot::mcmc_areas(post_sam, pars = &quot;cohen_d&quot;, prob = .90) # HPDI library(coda) HPDinterval(as.mcmc(post_sam)) &gt;# lower upper &gt;# beta -0.756 -0.264 &gt;# sigma 0.379 0.544 &gt;# cohen_d -1.709 -0.563 &gt;# attr(,&quot;Probability&quot;) &gt;# [1] 0.95 From the normal model, it was estimated that the mean RT for man was 1.809 seconds, 95% CI [1.614, 1.997]. On average women had faster RT when asked to tell lies in Dutch than man, with an estimated difference of -0.511 seconds, 95% CI [-0.759, -0.265], d = -1.116, 95% CI [-1.697, -0.546]. 5.2.3.4 Posterior Predictive Check Let’s check whether the model works well. First look at the shape of the data: library(bayesplot) # Observed data y1 &lt;- lies_cc$LDMRT[which(lies_cc$Gender == &quot;man&quot;)] y2 &lt;- lies_cc$LDMRT[which(lies_cc$Gender == &quot;vrouw&quot;)] # Replicated data (randomly sampling 100) y1rep &lt;- as.matrix(m1, pars = &quot;y1rep&quot;)[sample.int(2000, 100), ] y2rep &lt;- as.matrix(m1, pars = &quot;y2rep&quot;)[sample.int(2000, 100), ] ppc_dens_overlay(y1, yrep = y1rep) ppc_dens_overlay(y2, yrep = y2rep) The main problem is that the predictions can be negative, which isn’t possible for response time. Below is a check for outliers: ppc_intervals(y1, yrep = y1rep) ppc_intervals(y2, yrep = y2rep) When the dark blue dots were outside of the intervals (which were the intervals of predicted value), it indicates that the model didn’t account for those values well. The intervals were 90% predictive intervals, so you should expect less than 10% of data points to be outside of the intervals. There were not a lot of unfitted data points to the model in this example. 5.2.4 Robust Model A robust Bayesian version of the \\(t\\) test can accommodate unequal standard deviations as well as outliers, by replacing the normal likelihood with a Student’s \\(t\\) likelihood. See homework instruction. We will talk more about this in a later week after we discuss regression. data { int&lt;lower=0&gt; N1; // number of observations (group 1) int&lt;lower=0&gt; N2; // number of observations (group 2) vector[N1] y1; // response time (group 1); vector[N2] y2; // response time (group 2); } parameters { real&lt;lower=0&gt; mu_1; // mean of group 1 real beta; // difference in means real lsigma_1; // log of scale parameter for group 1 real beta_lsigma; // difference in log standard deviation real&lt;lower=1&gt; nu; // degrees of freedom of student&#39;s t distribution } transformed parameters { real&lt;lower=0&gt; mu_2 = mu_1 + beta; real&lt;lower=0&gt; sigma_1 = exp(lsigma_1); real&lt;lower=0&gt; sigma_2 = exp(lsigma_1 + beta_lsigma); } model { y1 ~ student_t(nu, mu_1, sigma_1); y2 ~ student_t(nu, mu_2, sigma_2); // prior mu_1 ~ normal(0.5, 2.5); beta ~ normal(0, 2.5); lsigma_1 ~ student_t(4, 0, 1); beta_lsigma ~ std_normal(); nu ~ gamma(2, 0.1); } generated quantities { real sigma_ratio = sigma_2 / sigma_1; real y1rep[N1]; real y2rep[N2]; for (i in 1:N1) { y1rep[i] = student_t_rng(nu, mu_1, sigma_1); } for (i in 1:N2) { y2rep[i] = student_t_rng(nu, mu_2, sigma_2); } } m2 &lt;- stan(&quot;../codes/group_comparison_robust.stan&quot;, data = list(N1 = sum(lies_cc$Gender == &quot;man&quot;), N2 = sum(lies_cc$Gender == &quot;vrouw&quot;), y1 = lies_cc$LDMRT[which(lies_cc$Gender == &quot;man&quot;)], y2 = lies_cc$LDMRT[which(lies_cc$Gender == &quot;vrouw&quot;)])) 5.2.5 Shifted Lognormal Model* With Bayesian analyses, you’re not limited to just the usual \\(t\\)-test, but you can potentially extract more information by modeling the distribution of the data. One possible choice is the shifted lognormal model (see Heathcote, Brown, and Cousineau 2004), which assumes that there are two components in the response time: (a) decision time component that follows a log-normal distribution, and (b) non-decision time component due to processing of stimuli. I won’t get into the detail of the model, but instead wanted to show you several shifted lognormal distributions and the codes for running it on our example. 5.2.5.1 Shifted Lognormal Distributions 5.2.5.2 STAN data { int&lt;lower=0&gt; N1; // number of observations (group 1) int&lt;lower=0&gt; N2; // number of observations (group 2) vector&lt;lower=0&gt;[N1] y1; // response time (group 1); vector&lt;lower=0&gt;[N2] y2; // response time (group 2); } parameters { real mu_1; // mean of group 1 for the Gaussian component real beta_mu; // difference in means for the Gaussian component real&lt;lower=0&gt; sigma; // pooled standard deviation real lndt_1; // log of non-decision time for group 1 real beta_lndt; // difference in ndt } transformed parameters { real mu_2 = mu_1 + beta_mu; real&lt;lower=0&gt; ndt_1 = exp(lndt_1); real&lt;lower=0&gt; ndt_2 = exp(lndt_1 + beta_lndt); } model { target += lognormal_lpdf(y1 - ndt_1 | mu_1, sigma); target += lognormal_lpdf(y2 - ndt_2 | mu_2, sigma); target += std_normal_lpdf(mu_1); target += std_normal_lpdf(beta_mu); target += student_t_lpdf(sigma | 4, 0, 1) - student_t_lccdf(0 | 4, 0, 1); target += std_normal_lpdf(lndt_1); target += std_normal_lpdf(beta_lndt); } generated quantities { real&lt;lower=0&gt; y1rep[N1]; real&lt;lower=0&gt; y2rep[N2]; for (i in 1:N1) { y1rep[i] = lognormal_rng(mu_1, sigma) + ndt_1; } for (i in 1:N2) { y2rep[i] = lognormal_rng(mu_2, sigma) + ndt_2; } } m3 &lt;- stan(&quot;../codes/group_comparison_shifted_lognormal.stan&quot;, data = list(N1 = sum(lies_cc$Gender == &quot;man&quot;), N2 = sum(lies_cc$Gender == &quot;vrouw&quot;), y1 = lies_cc$LDMRT[which(lies_cc$Gender == &quot;man&quot;)], y2 = lies_cc$LDMRT[which(lies_cc$Gender == &quot;vrouw&quot;)]), control = list(adapt_delta = .999, max_treedepth = 12)) broom::tidy(m3, pars = c(&quot;mu_1&quot;, &quot;mu_2&quot;, &quot;beta_mu&quot;, &quot;sigma&quot;, &quot;ndt_1&quot;, &quot;ndt_2&quot;), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;) %&gt;% # Just to demonstrate HPDI knitr::kable(digits = 2) # round to two digits term estimate std.error conf.low conf.high mu_1 0.15 0.21 -0.25 0.54 mu_2 0.03 0.12 -0.21 0.24 beta_mu -0.12 0.19 -0.48 0.26 sigma 0.40 0.06 0.30 0.52 ndt_1 0.54 0.21 0.14 0.91 ndt_2 0.18 0.09 0.02 0.36 5.2.5.3 Difference in \\(\\mu\\) and in ndt # Extract posterior samples of the mu, sigma, and ndt parameters post_sam &lt;- as.data.frame(m3, pars = c(&quot;mu_1&quot;, &quot;mu_2&quot;, &quot;sigma&quot;, &quot;ndt_1&quot;, &quot;ndt_2&quot;)) # Compute means of decision components for each iteration post_sam$dt_1 &lt;- exp(post_sam$mu_1 + post_sam$sigma^2 / 2) post_sam$dt_2 &lt;- exp(post_sam$mu_2 + post_sam$sigma^2 / 2) post_sam$dt_diff &lt;- post_sam$dt_2 - post_sam$dt_1 # Compute difference for ndt post_sam$ndt_diff &lt;- post_sam$ndt_2 - post_sam$ndt_1 # Posterior density bayesplot::mcmc_areas(post_sam, pars = c(&quot;dt_diff&quot;, &quot;ndt_diff&quot;), prob = .90) 5.2.5.4 Posterior Predictive Check Let’s check whether the model works well. First look at the shape of the data: # Replicated data (randomly sampling 100) y1rep &lt;- as.matrix(m3, pars = &quot;y1rep&quot;)[sample.int(2000, 100), ] y2rep &lt;- as.matrix(m3, pars = &quot;y2rep&quot;)[sample.int(2000, 100), ] ppc_dens_overlay(y1, yrep = y1rep) ppc_dens_overlay(y2, yrep = y2rep) The above looks pretty good. Below is a check for outliers: ppc_intervals(y1, yrep = y1rep) ppc_intervals(y2, yrep = y2rep) There are still quite a handful of outliers. 5.3 Notes on Model Comparison In this note there are three models presented. Each of them have different interpretations. The big question is, of course, which one is better. It’s not an easy question to answer, but the choice should be ideally driven by substantive theories on how the data are generated. The posterior predictive check should also be use to see whether the models would yield data that are similar to the observed ones. Finally, There are also some useful statistical tools we can use to compare the different models, which we will discuss when we get to model comparisons. 5.4 Within-Subject Comparisons Our research question this time is whether RT is longer for lies than for truth in Dutch (native language) 5.4.1 Plots lies %&gt;% select(PP, Gender, LDMRT, TDMRT) %&gt;% gather(key = &quot;veracity&quot;, value = &quot;RT&quot;, LDMRT:TDMRT) %&gt;% ggplot(aes(x = RT, col = veracity)) + geom_density(bw = &quot;SJ&quot;) &gt;# Warning: Removed 6 rows containing non-finite values (stat_density). 5.4.2 Independent sample \\(t\\)-test # paired t-test t.test(lies$TDMRT, lies$LDMRT, paired = TRUE) &gt;# &gt;# Paired t-test &gt;# &gt;# data: lies$TDMRT and lies$LDMRT &gt;# t = -9, df = 62, p-value = 5e-13 &gt;# alternative hypothesis: true difference in means is not equal to 0 &gt;# 95 percent confidence interval: &gt;# -0.376 -0.240 &gt;# sample estimates: &gt;# mean of the differences &gt;# -0.308 5.4.3 Bayesian Normal Model \\[\\begin{align} \\mathtt{TDMRT}_{i} &amp; \\sim \\mathcal{N}^+(\\mu_1 + u_i, \\sigma) \\\\ \\mathtt{LDMRT}_{i} &amp; \\sim \\mathcal{N}^+(\\mu_2 + u_i, \\sigma) \\\\ \\mu_2 &amp; = \\mu_1 + \\beta \\\\ \\mu_1 &amp; \\sim \\mathcal{N}^+(0.5, 2.5) \\\\ \\beta &amp; \\sim \\mathcal{N}(0, 2.5) \\\\ \\sigma &amp; \\sim t^+(4, 0, 2.5) \\\\ u_i &amp; \\sim \\mathcal{N}(0, \\tau) \\\\ \\tau &amp; \\sim t^+(4, 0, 2.5) \\end{align}\\] Parameters: \\(\\mu_1\\): mean parameter for RT for truth \\(\\beta\\): mean difference between the two genders \\(\\log(\\sigma_1)\\): natural logarithm of the standard deviation parameter for RT for truth \\(\\beta_{\\log(\\sigma)}\\): difference in log(SD) for RT between truth and lies \\(u_1\\), \\(\\ldots\\), \\(u_{63}\\): individual difference nuisance parameters \\(\\tau\\): standard deviation of individual differences Transformed parameters: \\(\\sigma\\), \\(\\sigma_2\\) \\(\\mu_2\\) This is an example of a hierarchical model. Note that the priors for the \\(u\\)s are \\(\\mathcal{N}(0, \\tau)\\), which has another parameter on it. In this case, we’re letting the data to update our belief on how much individual difference there is. The parameter here, \\(\\tau\\), is called a hyperparameter, and this kind of prior is called a hierarchical prior. 5.4.3.1 STAN Below is the STAN code for the model: data { int&lt;lower=0&gt; N; // number of observations vector[N] y1; // response time (repeated measure 1); vector[N] y2; // response time (repeated measure 2); } parameters { real&lt;lower=0&gt; mu_1; // mean of group 1 real beta; // difference in means real&lt;lower=0&gt; sigma; // residual SD vector[N] zu; // individual difference parameters (scaled) real&lt;lower=0&gt; tau; // standard deviation of indivdiual difference } transformed parameters { real&lt;lower=0&gt; mu_2 = mu_1 + beta; vector[N] u = zu * tau; } model { y1 ~ normal(mu_1 + u, sigma); y2 ~ normal(mu_2 + u, sigma); // prior mu_1 ~ normal(0.5, 2.5); beta ~ normal(0, 2.5); sigma ~ student_t(4, 0, 2.5); zu ~ std_normal(); // hyperprior tau ~ student_t(4, 0, 2.5); } generated quantities { real y1rep[N]; real y2rep[N]; real cohen_d = (mu_2 - mu_1) / sqrt(sigma^2 + tau^2); for (i in 1:N) { y1rep[i] = normal_rng(mu_1 + u[i], sigma); y2rep[i] = normal_rng(mu_2 + u[i], sigma); } } And let’s fit the model with rstan. m4 &lt;- stan(&quot;../codes/group_comparison_paired.stan&quot;, data = list(N = 63, y1 = lies_cc$TDMRT, y2 = lies_cc$LDMRT), pars = &quot;zu&quot;, include = FALSE, seed = 104134) You might receive a warning from rstan from the above code. Don’t ignore those. Generally, when STAN shows a warning it usually indicates that the results are not trustworthy. 5.4.3.2 Effect size For repeated measures, there were several different ways to define effect size. To make it consistent with the between-subject comparison, Cohen’s \\(d\\) was defined as: \\[d = \\frac{\\mu_2 - \\mu_1}{\\sqrt{\\sigma^2 + \\tau^2}}\\] This time we directly put that in the STAN code for generated quantities. generated quantities { real cohen_d = (mu_2 - mu_1) / sqrt(sigma^2 + tau^2); } # Use the `broom` package to generate nicely formatted table broom::tidy(m4, pars = c(&quot;mu_1&quot;, &quot;mu_2&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;tau&quot;, &quot;cohen_d&quot;), conf.int = TRUE, conf.method = &quot;quantile&quot;) &gt;# # A tibble: 6 x 5 &gt;# term estimate std.error conf.low conf.high &gt;# &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt;# 1 mu_1 1.16 0.0604 1.04 1.28 &gt;# 2 mu_2 1.47 0.0606 1.35 1.59 &gt;# 3 beta 0.308 0.0346 0.240 0.377 &gt;# 4 sigma 0.194 0.0177 0.163 0.233 &gt;# 5 tau 0.441 0.0453 0.364 0.539 &gt;# 6 cohen_d 0.644 0.0906 0.471 0.828 plot(m4, pars = c(&quot;mu_1&quot;, &quot;mu_2&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;tau&quot;, &quot;cohen_d&quot;)) &gt;# ci_level: 0.8 (80% intervals) &gt;# outer_level: 0.95 (95% intervals) From the normal model, it was estimated that the mean RT for truth was 1.159 seconds, 95% CI [1.040, 1.279]. On average women had faster RT when asked to tell lies in Dutch than man, with an estimated difference of 0.308 seconds, 95% CI [0.240, 0.377], d = 0.644, 95% CI [0.471, 0.828]. 5.4.3.3 Posterior Predictive Check Let’s check whether the model works well. First look at the shape of the data: library(bayesplot) # Observed data y1 &lt;- lies_cc$TDMRT y2 &lt;- lies_cc$LDMRT # Replicated data (randomly sampling 100) y1rep &lt;- as.matrix(m4, pars = &quot;y1rep&quot;)[sample.int(2000, 100), ] y2rep &lt;- as.matrix(m4, pars = &quot;y2rep&quot;)[sample.int(2000, 100), ] ppc_dens_overlay(y1, yrep = y1rep) ppc_dens_overlay(y2, yrep = y2rep) The fit was good for lies but not for truth. Below is a check for outliers: ppc_intervals(y1, yrep = y1rep) ppc_intervals(y2, yrep = y2rep) As discussed in the previous note, you may want to consider some alternative models, maybe the Student’s \\(t\\) likelihood that accommodate outliers, or some response time models like the shifted lognormal model. I encourage you to modify the STAN code and try things out! 5.4.4 Using brms* Finally, one thing to mention is that many of the commonly used models have been implemented in the R package brms. I decided to talk about STAN first because it is the underlying engine and can fit almost any parametric models, but in practice I do use brms a lot. I will show you the brms code for a majority of the models moving forward. For example, here is one for the repeated measure comparison above, with a Student’s \\(t\\) likelihood. It does require restructuring the data first. library(brms) lies_long &lt;- lies %&gt;% select(PP, Gender, LDMRT, TDMRT) %&gt;% gather(key = &quot;veracity&quot;, value = &quot;RT&quot;, LDMRT:TDMRT) m2_brm &lt;- brm(RT ~ veracity + (1 | PP), data = lies_long, family = student(), prior = c(prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 2.5), class = &quot;sd&quot;), prior(student_t(4, 0, 2.5), class = &quot;sigma&quot;))) &gt;# Warning: Rows containing NAs were excluded from the model. &gt;# Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. &gt;# Running the chains for more iterations may help. See &gt;# http://mc-stan.org/misc/warnings.html#bulk-ess And there are some nice functions you can use to summarize models fitted by brms m2_brm &gt;# Family: student &gt;# Links: mu = identity; sigma = identity; nu = identity &gt;# Formula: RT ~ veracity + (1 | PP) &gt;# Data: lies_long (Number of observations: 126) &gt;# Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; &gt;# total post-warmup samples = 4000 &gt;# &gt;# Group-Level Effects: &gt;# ~PP (Number of levels: 63) &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# sd(Intercept) 0.41 0.04 0.34 0.50 1.01 732 1417 &gt;# &gt;# Population-Level Effects: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# Intercept 1.45 0.06 1.33 1.56 1.01 361 801 &gt;# veracityTDMRT -0.30 0.03 -0.36 -0.24 1.00 4341 2787 &gt;# &gt;# Family Specific Parameters: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# sigma 0.14 0.02 0.10 0.19 1.00 1110 1694 &gt;# nu 5.75 4.77 2.08 19.49 1.00 1603 1783 &gt;# &gt;# Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS &gt;# and Tail_ESS are effective sample size measures, and Rhat is the potential &gt;# scale reduction factor on split chains (at convergence, Rhat = 1). # sjPlot::tab_model(m2_brm, show.ci50 = FALSE) # broken in newest version of `brms` plot(m2_brm) From above, as the estimate of \\(\\nu\\) was quite small, it was pretty clear that there are outliers that need to be handled (and was handled somewhat by the robust Student’s \\(t\\) model). Let’s look at some posterior predictive checks: pp_check(m2_brm, nsamples = 100) pp_check(m2_brm, type = &quot;intervals_grouped&quot;, group = &quot;veracity&quot;) &gt;# Using all posterior samples for ppc type &#39;intervals_grouped&#39; by default. 5.4.5 Region of Practical Equivalence (ROPE) One thing that is often of interest in research is to establish equivalence between two values. For example, we may wonder, for example, whether an experimental manipulation has an effect size of zero (\\(d\\) = 0), whether two variables are truly uncorrelated (\\(r\\) = 0), whether the blood type distribution is the same across countires (see some examples here) whether two parallel forms of a test have the same difficulty level In this example, we can investigate, for example, Whether lying in native and in second languages requires the same response time. In all these above scenarios, we are interested in “confirming” whether a quantity is equal to another quantity. The traditional null hypothesis significance testing (NHST), however, won’t allow us to do that. That’s because NHST is set up to reject the null hypothesis, but failure to reject \\(d\\) = 0 does not confirm that \\(d\\) = 0; it just means that we don’t have enough evidence for that. In addition, we know in advance, with high degree of certainty, that \\(d\\) \\(\\neq\\) 0. Do we truly believe that the treatment group and the control group will perform exactly the same on an IQ test? Even when one group got 100 and the other group 100.0000001, the null hypothesis is false. Therefore, what we really meant when saying whether two things are equal is not that whether two quantities are exactly the same, which is basically impossible, but instead whether two quantities are close enough, or practically equivalent. In the IQ test example, most of us would agree that the two groups are practically equivalent. So what we actually want to test, in mathematically notation, is \\[|\\beta - \\beta_0| &lt; \\epsilon,\\] where \\(\\beta\\) is the parameter of interest (in this case the difference in RT between lying in Dutch and lying in English), \\(\\beta_0\\) is the value we wanted to compare \\(\\beta\\) to (in this case 0), and \\(\\epsilon\\) is some small value of a difference by which \\(\\beta\\) and \\(\\beta_0\\) are deemed practically equivalent. For example, in our analysis we may think that if the difference is less than .05 seconds (or 50ms), we may say that there are no difference in RT. In other words, if there is a high probability (in a Bayesian sense) that \\[-\\epsilon &lt; \\beta &lt; \\epsilon\\] than we considered there is sufficient evident that \\(\\beta\\) = \\(\\beta_0\\) in a practical sense. The interval (\\(-\\epsilon\\), \\(\\epsilon\\)) is what Kruschke (2013) referred to as the region of practical equivalence, or ROPE. Using brms, we run a model comparing the means of LDMRT and LEMRT: library(brms) lies_long &lt;- lies %&gt;% select(PP, Gender, LDMRT, LEMRT) %&gt;% gather(key = &quot;language&quot;, value = &quot;RT&quot;, LDMRT, LEMRT) m3_brm &lt;- brm(RT ~ language + (1 | PP), data = lies_long, family = student(), prior = c(prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 2.5), class = &quot;sd&quot;), prior(student_t(4, 0, 2.5), class = &quot;sigma&quot;)), iter = 4000) &gt;# Warning: Rows containing NAs were excluded from the model. As you can see, the 95% CI of \\(\\beta\\) is brms::posterior_interval(m3_brm, pars = &quot;b_languageLEMRT&quot;) &gt;# 2.5% 97.5% &gt;# b_languageLEMRT -0.0405 0.0399 So we can see that the 95% CI is completely inside the ROPE. Therefore, we would say: There is &gt; 95% chance that the mean RT of lying is practically the same in Dutch and in English You can also check the probability that \\(\\beta\\) is inside the ROPE: # Extract posterior samples beta_sam &lt;- as.matrix(m3_brm, pars = &quot;b_languageLEMRT&quot;) # Probability in the ROPE mean(beta_sam &lt; .05 &amp; beta_sam &gt; -.05) &gt;# [1] 0.982 which shows that there is a mean(beta_sam &lt; .05 &amp; beta_sam &gt; -.05) * 100% chance that the mean RT of lying is practically the same in Dutch and in English. You can do the same in frequentist with equivalence testing (see https://daniellakens.blogspot.com/2017/02/rope-and-equivalence-testing.html), but I found ROPE to be more scalable to other types of models, plus you’ll obtain a posterior interval anyway with Bayesian analyses. References "],
["markov-chain-monte-carlo.html", "Chapter 6 Markov Chain Monte Carlo 6.1 Monte Carlo Simulation With One Unknown 6.2 Markov Chain Monte Carlo (MCMC) With One Parameter 6.3 Markov Chain 6.4 Effective Sample Size (\\(n_\\text{eff}\\)) 6.5 MC Error 6.6 Burn-in/Warmup 6.7 Diagnostics of MCMC 6.8 Multiple Parameters 6.9 Hamiltonian Monte Carlo", " Chapter 6 Markov Chain Monte Carlo So far in this class, we have seen a few examples with Bayesian inferences where the posterior distribution concerns only one parameter, like the binomial and the Poisson model, and also worked on some group comparison examples. We have also discussed different approaches to obtain/approximate the posterior, and worked on a few examples where we simulated posterior samples from the posterior, including grid approximation and MCMC. In this lecture, we will provide a more conceptual discussion on the simulation method, see why we need special methods together called Markov Chain Monte Carlo, and extend it to multiple parameter problems. We will specifically discuss four MCMC methods that you will commonly see in Bayesian literature: The Metropolis algorithm The Metropolis-Hastings algorithm The Gibbs sampler Hamiltonian Monte Carlo No U-turn sampler (and several variants) But first, let’s talk about what the Monte Carlo method is. 6.1 Monte Carlo Simulation With One Unknown In a previous example, we see that with a conjugate prior (e.g., Beta), the posterior distribution is standard (Beta), and with R we can easily draw simulation samples from the posterior distribution. The more samples we draw, the better we can approximate the posterior distribution based on the simulation samples. This is exactly the same reason that if we have a very large sample, we can very precisely describe our population; here the true posterior distribution is considered the population, and the simulation samples we draw are, well, a sample from the population. With 10,000 or 100,000 samples, we can very accurately describe our population. For example, if using a conjugate prior we know that the posterior is a \\(\\mathrm{Beta}(15, 10)\\) distribution, consider drawing 10, 100, 10,00, and 10,000 samples from it using the R function rbeta, and contrast the density estimated from the samples (in the histogram) with that of the real \\(\\mathrm{Beta}\\) distribution (in red). &gt;# `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The figure below shows the values when drawing 100 samples in time order: So we can say that when the number of posterior samples is very large, the sample distribution converges to the population density. The Monte Carlo method will work for many situations. Note, of course, the number of simulation samples is controlled by the analysts; it is totally different from sample size, which is fixed and is a property of the data. In addition, most of the descriptive statistics (e.g., mean, SD) of the sample will converge to the corresponding values of the true posterior distribution. The graphs below showed how the mean, median, SD, and skewness converge to the true value (red dashed lines) when the number of simulation samples increases. 6.2 Markov Chain Monte Carlo (MCMC) With One Parameter However, the above Monte Carlo simulation works in the above example because (a) we know exactly that the posterior distribution is a beta distribution, and (b) R knows how to draw simulation samples form a beta distribution (with rbeta). However, as we progress through the class, it is more of an exception that we can use conjugate prior distribution, so in general neither (a) nor (b) would hold. For example, if we instead use a normal distribution for the prior of \\(\\theta\\), we may get something like \\[P(\\theta | y) = \\frac{\\mathrm{e}^{-(\\theta - 1 / 2)^2} \\theta^y (1 - \\theta)^{n - y}} {\\int_0^1 \\mathrm{e}^{-(t - 1 / 2)^2} t^y (1 - t)^{n - y} \\; \\mathrm{d}t}\\] and it would be very hard, if possible at all, to directly draw simulation samples from the posterior. Luckily, we have a clever (sets of) algorithm called Markov Chain Monte Carlo, which provides a way to draw samples from the posterior distribution without the need to know everything about the posterior distribution. Indeed, for some algorithms they only require that we know, for every two possible values of \\(\\theta\\), the ratio of their corresponding densities. 6.2.1 The Metropolis algorithm The Metropolis algorithm can generally be used to draw samples from a distribution as long as the density ratio of any two points can be computed. Remember in Bayesian inference, for two values in the posterior distribution, \\(\\theta_1\\) and \\(\\theta_2\\), \\[\\begin{align*} P(\\theta = \\theta_1 | \\boldsymbol{\\mathbf{y}}) &amp; = \\frac{P(\\boldsymbol{\\mathbf{y}} | \\theta = \\theta_1) P(\\theta = \\theta_1)} {P(y)} \\\\ P(\\theta = \\theta_2 | \\boldsymbol{\\mathbf{y}}) &amp; = \\frac{P(\\boldsymbol{\\mathbf{y}} | \\theta = \\theta_2) P(\\theta = \\theta_2)} {P(y)} \\end{align*}\\] Therefore, if we take the ratio of the posterior densities, we have \\[\\frac{P(\\theta = \\theta_2 | \\boldsymbol{\\mathbf{y}})}{P(\\theta = \\theta_1 | \\boldsymbol{\\mathbf{y}})} = \\frac{P(\\boldsymbol{\\mathbf{y}} | \\theta = \\theta_2) P(\\theta = \\theta_2)} {P(\\boldsymbol{\\mathbf{y}} | \\theta = \\theta_1) P(\\theta = \\theta_1)},\\] which does not involve \\(P(y)\\). Therefore, even though we may not know \\(P(\\theta = \\theta_1 | \\boldsymbol{\\mathbf{y}})\\) as it involves \\(P(y)\\) as the denominator, we can still compute the density ratio. In addition, the Metropolis algorithm requires the use of a proposal distribution, which can be any symmetric distribution. Common choices are a normal distribution or a uniform distribution. For example, let’s assume we will be using a \\(N(0, 1)\\) proposal distribution. The steps of a Metropolis algorithm are: Randomly start from a certain point in the parameter space, and call that point \\(\\theta_0\\) Randomly generate a sampled value from a \\(N(\\theta_0, 1)\\) distribution. Call this proposed value \\(\\theta^\\text{prop}\\) Compute the density ratio \\([P(\\theta = \\theta^\\text{prop} | \\boldsymbol{\\mathbf{y}})] / [P(\\theta = \\theta_0 | \\boldsymbol{\\mathbf{y}})]\\) If the ratio is larger than 1, accept \\(\\theta^\\text{prop}\\) and include this value in the sample If the ratio is smaller than 1, accept \\(\\theta^\\text{prop}\\) with probability equal to the density ratio. For example, if the ratio is 0.7, one first generated a simulated value, \\(u\\), from a uniform distribution between 0 and 1 (i.e., \\(U(0, 1)\\)). If \\(u\\) is smaller than the ratio, accept \\(\\theta^\\text{prop}\\) and include it in the sample. Otherwise, reject the proposed value, and include \\(\\theta_0\\) (again) in the sample After accepting \\(\\theta^\\text{prop}\\) or \\(\\theta_0\\) in the sample, denote the accepted value as \\(\\theta_0\\), and repeat steps 2 to 6. Under mild conditions, and after the chain runs for a while, the above algorithm will generate representative samples from the target posterior distribution. 6.2.1.1 Shiny App: To see a visual demonstration, you may run the shiny app I created by typing in R shiny::runGitHub(&quot;metropolis_demo&quot;, &quot;marklhc&quot;) Each step in MCMC is called an iteration. However, the sampled values are not independent, which means they are different from those generated using functions like rbeta or rnorm. Instead, the resulting sampled values will form a Markov chain, meaning that each sampled value is correlated with the previous value. This is because each time we propose a new value, the proposal distribution is centered at the previous value. As long as the previous value affects which values are likely proposed, the two consecutive values are not independent. You can see a trace plot below, which is generally the first thing you need to check after running an MCMC sampling. You will see that consecutive samples tend to be closer or the same. Compare this with the one in a previous section using rbeta. The graph on the right panel is an autocorrelation plot, which shows the correlations between sampled values that are \\(l\\) iterations apart. The table below has three columns, the first one is the first 10 sampled values from the chain, the second is the lag-1 behind, meaning values in the 2nd to the 11th iterations, and the third column is the lag-2 values. A lag-1 autocorrelation is the correlation between the first column and the second column, and a lag-2 autocorrelation is the correlation between the first column and the third column. The graph above shows that the correlation between values from two consecutive iterations have a correlation of 0.656. &gt;# # A tibble: 10 x 3 &gt;# self lag1 lag2 &gt;# &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt;# 1 0.2 0.2 0.2 &gt;# 2 0.2 0.2 0.194 &gt;# 3 0.2 0.194 0.194 &gt;# 4 0.194 0.194 0.194 &gt;# 5 0.194 0.194 0.194 &gt;# 6 0.194 0.194 0.328 &gt;# 7 0.194 0.328 0.328 &gt;# 8 0.328 0.328 0.415 &gt;# 9 0.328 0.415 0.515 &gt;# 10 0.415 0.515 0.515 6.2.2 The Metropolis-Hastings Algorithm The Metropolis-Hastings (MH) algorithm is a generalization of the Metropolis algorithm, where it allows a proposal distribution that is not symmetric, with some additional adjustment made. The MH algorithm can perform better for some cases where the target distribution is bounded and non-symmetric, but this is beyond the scope of this course. For multiparameter problems, the Gibbs sampler which we will talk about later in this note is actually also a special case of the MH algorithm. 6.3 Markov Chain A Markov chain is a chain of random samples, where the next sample depends on where the previous sample(s) are at. Recall that in the Metropolis algorithm, the proposal distribution is centered at the previous sample. This is called first order Markov chain, as the current state only depends on just the previous 1 state. Under a condition called ergodicity, a Markov chain will converge to a stationary distribution. This means that, after a certain amount of samples, when the chain arrives at the high density region of the posterior distribution, all simulated samples after that can be considered a random (but correlated) sample of the posterior distribution. Like regular simulation, as long as you have enough samples, the sample density will converge to the population density. However, it takes thousands or tens of thousands Metropolis samples to make the sample density sufficiently close to the target posterior distribution, which is much more than what is required with Monte Carlo simulation with independent samples. As you can see below, with 1,000 samples, the summary statistics are still not very close to the true values. 6.4 Effective Sample Size (\\(n_\\text{eff}\\)) The information contained in 1,000 simulated samples using the Metropolis algorithm in this example was approximately equivalent to 207.517 samples if the simulated samples are independent. In other words, the effective sample size of the 1,000 simulated samples is only 207.517. Therefore, using MCMC requires drawing much more samples than using techniques for drawing independent samples like the rbeta function. If the proposal distribution is exactly the same as the posterior, then we can accept all proposed value. Why? Because each proposed value is already a random sample from the posterior! This will be relevant when we talk about Gibbs sampling. 6.5 MC Error The Monte Carlo error or Monte Carlo standard error tells the margin of error when using the MCMC samples to estimate the posterior mean. A simple method to estimate the MC error is: \\[\\mathit{SE}_\\mathrm{mc} = \\sqrt{\\frac{\\widehat{\\mathrm{Var}}(\\theta | \\boldsymbol{\\mathbf{y}})}{n_\\text{eff}}}\\] In the above example, \\(\\mathit{SE}_\\mathrm{mc} = 0.007\\). So if the true posterior mean is 0.6, using 1,000 MCMC samples will likely give an estimated posterior mean in the range [0.593, 0.607]. Generally, we would like to have something more precise, and it’s better to get an \\(n_\\text{eff}\\) above 1,000. 6.6 Burn-in/Warmup Every Markov chain needs a certain amount of iterations to reach the stationary distribution. Whereas in the previous examples, the chain quickly get to the regions with relative high density, for some situations, especially for multiparameter problems, it usually takes hundreds or thousands of iterations to get there, as shown in the graph below (for approximating a \\(N[15, 2]\\) distribution). Iterations obtained before a Markov chain reaches the stationary distribution are called burn-in in WinBUGS and warmup in Stan. As they are not considered samples of the posterior distribution, they should not be included when approximating the posterior distribution. In Stan, the first half of the samples (e.g., 1,000 out of 2,000) are discarded. When people say they get \\(8,000\\) samples/iterations using MCMC, \\(8,000\\) is the number the burn-in/warmup. Warmup is the term used in STAN to tune the algorithm so it’s not the same as burn-in as discussed here. See the chapter by McElreath (2016). 6.6.1 Thinning You will sometimes hear the term thinning, which means only saving every \\(t\\)th sample, where \\(t\\) is the thinning interval. For example, based on the autocorrelation plot it appears that the samples are approximated uncorrelated when they are 11 lag apart, so instead of using all 1,000 samples, we just use the 1st, 12th, 23rd, \\(\\ldots\\) samples. However, this is generally not recommended unless you have a concern for not having enough storage space, which happens when, for example, using multiple imputation to handle missing data. Otherwise, it is strongly recommended that you include all draws after burn-in/warmup, even if they are correlated. 6.7 Diagnostics of MCMC 6.7.1 Mixing One thing you should look at to diagnose the convergence of a Markov chain is the trace plot. Look at the three examples below: When multiple chains were run, each with a different initial value, it was recently recommended that researchers examine the rank plot (see this paper) as it is more robust. The first graph on the left shows good mixing behavior as it explores the region with most of the density (bounded by the blue dashed line) smoothly and bounces from one point to another quickly. For the middle graph, this is a chain where, although in every iteration it moves to a new place, the jump is relatively small, so it takes a long time to get from one end of the distribution to another end, and it never explores regions that are just within the blue lines and outside of the blue lines. For the bottom graph, you can see it tends to stay in one point for quite some time, and at some point it takes almost 100 iterations for it to move. The first graph demonstrates good mixing, which will converge to a stationary distribution (the posterior) pretty quickly. The middle and the bottom graph demonstrates poor mixing, which takes a lot more iterations to converge; if you stop the chain before that, you can get a biased representation of the posterior distribution. The autocorrelation plots on the right show the corresponding autocorrelations. You can see that whereas the autocorrelation dies out for the first chain pretty soon (at about the 10th lag), it remains high for the other two cases. 6.7.2 Acceptance Rate If you’re using the Metropolis/MH algorithm, you want to monitor the acceptance rate and make sure it is within optimal range. If you accept almost every time, that tells you that each time the chain only jumps a very small step (so that the acceptance ratio is close to 1 every time), which will make the algorithm slow in converging to the stationary distribution. On the other hand, if the acceptance rate is very low, then that says that the chain got stuck to just a few locations and it takes hundreds of iterations for it to make one jump. For the Metropolis/MH algorithm, an optimal acceptance rate would be something between 10% to 60%. For Hamiltonian Monte Carlo and other newer method, which we will discuss later, the optimal acceptance rate would be much higher, from 80% to 99%, or even higher. 6.7.3 Diagnostics Using Multiple Chains Another important thing to check is to see the convergence with multiple chains. So far we’ve been just talking about one chain, but it is common practice to use two or more chains (and 4 chains are generally recommended nowadays), each starting at a different, preferably more extreme, place, and see whether they explore a similar region. The two plots below are called rank plots. The top one shows a relatively healthy chains, as the ranks are relatively uniformly distributed (meaning that one chain does not have higher values than another for a long period of time). The plot below, however, shows an unhealthy chain. &gt;# Warning: The following arguments were unrecognized and ignored: n_warmup &gt;# Warning: The following arguments were unrecognized and ignored: n_warmup 6.7.3.1 \\(\\hat{R}\\), a.k.a Potential Scale Reduction Factor A commonly used numerical index in diagnosing convergence is \\(\\hat{R}\\), also called the potential scale reduction factor, proposed by Gelman and Rubin (1992) and later an extension for multivariate distributions by Brooks and Gelman (1997). \\(\\hat{R}\\) measures the ratio of the total variability combining multiple chains to the within-chain variability. To understand this, remember in ANOVA, the \\(F\\)-ratio is a measure of \\[F = \\frac{\\text{Between-group difference} + \\text{error}}{\\text{error}}\\] \\(\\hat{R}\\) has a very similar meaning conceptually, with error meaning the within-chain variance. When the Markov chains converge, they reach the stationary distribution. As each chain is based on the same posterior distribution, they should have the same variance, meaning that after the chains converge, there should be no differences between the chains, and so \\(\\hat{R}\\) should be very close to 1.0. Note that in Stan, \\(\\hat{R}\\) is computed by splitting each chain into half. So if you have two chains, \\(\\hat{R}\\) will be based on four groups. For the two graphs above, the first one has an \\(\\hat{R}\\) of 1.021, and the second one has an \\(\\hat{R}\\) of 2.93. Gelman et al. (2013) recommended an \\(\\hat{R}\\) less than 1.1 for acceptable convergence of the Markov chains, but more recently a more stringent cutoff of 1.01 is proposed. It should also be pointed out that there are many versions of \\(\\hat R\\) being used in the literature, but researchers rarely discussed which version they used. In STAN, the version was mainly based on Gelman et al. (2013), but in future upgrade it’s likely going to use a newer and more robust \\(\\hat R\\) based on this paper. 6.8 Multiple Parameters If you think about some of the statistical techniques you have learned, there are generally more than one parameter. For example, with a linear regression, you have at least one parameter for the intercept, \\(\\beta_0\\), and one parameter for the slope, \\(\\beta_1\\). With two parameters, when we generate posterior samples, we want to get more points in the regions with higher density. It’s helpful to understand this with an example. Look at the 3D plot below: Another way to show the joint distribution of two variables is to use a contour plot to show the lines at different density levels: With multiple parameters, one can still use the Metropolis/MH algorithm. However, one will need a multidimensional proposal distribution, and it is usually rather inefficient. Before 2010, a more efficient algorithm is the Gibbs sampling, which relies on conditional distributions as proposal distributions to sample each dimension the posterior distributions. The two popular Bayesian language, BUGS and JAGS, both used Gibbs sampling. You likely will still see it in a lot of articles doing Bayesian analyses. However, Gibbs sampling is rather restrictive as it relies on conjugate priors, so your choices of priors are rather limited. Also, it may run into convergence issues in more complex models such as multilevel models. Given that STAN uses different and usually more efficient sampling methods, I will not go into detail on Gibbs sampling, and will just move on to the ones that STAN uses. To learn more, you may see the following Shiny app. 6.8.0.1 Shiny app: Shiny app illustration by Jonah Gabry: shiny::runGitHub(repo = &quot;jgabry/gibbs_shiny&quot;) 6.9 Hamiltonian Monte Carlo Stan is using something different than the Metropolis-Hastings algorithm or the special case that is Gibbs sampling. Although its algorithms have gone through multiple revision, and in the next few updates they may have some changes as well, their basic algorithm is under the umbrella of Hamiltonian Monte Carlo or hybrid Monte Carlo (HMC). The algorithm is a bit complex and I will just try to help you develop some intuition of the algorithm, hopefully enough to help you diagnose the Markov chains. It produces simulation samples that are much less correlated, meaning that if you get 10,000 samples from Gibbs and 10,000 samples from HMC, HMC generally provides a much higher effective sample size. When there is a problem in convergence, HMC tends to raise a clear red flag, meaning it goes really really wrong. So now, how does HMC work? First, consider a two-dimensional posterior below, which is based on a real data example of a hierarchical model, with two of the parameters \\((\\mu, \\tau)\\). For illustration of HMC, let’s turn it upside down: HMC requires you to think of the inverted posterior density a park for ice skating. Imagine if you stand on a certain point of a slope and does not move your body, then you will soon fall down to the bottom point where the posterior mode is located. However, if you have some jet engine or device to provide thrust for you to resist the gravitational force, you will be able to explore the surface of the park. In HMC, you will randomly choose a direction to go, and randomly choose an energy level for the engine. When you stop, your coordinate will be a sample value from the posterior distribution. It was shown that, compared to Gibbs sampling or Metropolis algorithm, HMC is much more efficient in getting samples with lower autocorrelations. This means that, the effective sample size for HMC is usually much higher than the other methods we discussed when they have the same number of iterations. For example, with Gibbs sampling researchers usually need 50,000 or 100,000 iterations, whereas with STAN something around 2,000 would be enough for regression models. See https://chi-feng.github.io/mcmc-demo/app.html#HamiltonianMC,banana for a demonstration of HMC sampling. HMC will simulate the motion of gliding around the surface of the posterior by breaking path into discrete segments, each called a leapfrog step. The complexity of HMC lies in determining how many leapfrog steps to use in one iteration, as well as how far to go along the trajectory (called stepsize), as that varies a lot with different contour shapes, distributions, and dimensions, and one needs to tune these effectively for HMC to work well. Building on HMC, currently STAN used a modified algorithm called the No-U-Turn Sampler (NUTS). The detail is beyond this note, but you should note the names corresponding to the tuning parameters in STAN: adapt_delta: this parameter controls the target acceptance rate of the NUTS algorithm, with a default of .80. Increasing it with reduces the stepsize so that the algorithm won’t go two far away in each jump. When the default is not enough, you will receive a warning asking you to increase the value, usually to .95 or .99 or even higher. It will take longer time to one, but you should NEVER ignore such a warning. max_treedepth: it controls the maximum number of leapfrog steps. When the number of steps are too small, NUTS may be too slow in exploring the surface of the posterior, especially when adapt_delta is large. Like adapt_delta, you should increase it when receiving a warning message in the previous run. See this page for some of the common STAN warning messages. References "],
["linear-models.html", "Chapter 7 Linear Models 7.1 What is Regression? 7.2 One Predictor 7.3 Multiple Regression 7.4 Tabulating the Models", " Chapter 7 Linear Models 7.1 What is Regression? Regression is a class of statistical techniques to understand the relationship between an outcome variable (also called a criterion/response/dependent variable) and one or more predictor variables (also called explanatory/independent variables). For example, if we have the following scatter plot between two variables (\\(Y\\) and \\(X\\)): We want to find some pattern from this relationship. In conventional regression, we model the conditional distribution of \\(Y\\) given \\(X\\), \\(P(Y \\mid X)\\), by separating the outcome variable \\(Y\\) into (a) a systematic component that depends on the predictor, and (b) a random/probabilistic component that does not depend on the predictor. For example, we can start with a systematic component which only depends on the predictor value: As you can see, all the red dots fall exactly on the curve in the graph above, meaning that as long as one knows the \\(X\\) value, one can predict the \\(Y\\) value with 100% accuracy. We can thus write \\(Y^* = f(X)\\) (where \\(Y^*\\) is the systematic component of \\(Y\\)). However, in almost all scientific inquiries, one can never make prediction with 100% certainty (even in physics, which has measurement error and quantum mechanics). This can be due to the fact that we haven’t obtain all the factors that determine \\(Y\\), and there are things that are truly random (as in quantum physics). Therefore, we need to expand our model to incorporate this randomness, by adding a probabilistic component. Therefore, instead of saying the \\(Y\\) depends just on \\(X\\), we say that the value of \\(Y\\) is random, but the information about \\(X\\) provides information about how \\(Y\\) is distributed. This is achieved by studying the conditional distribution \\(P(Y \\mid X)\\) such that the conditional expectation, \\(\\mathrm{E}(Y \\mid X)\\), is completely determined by \\(X\\), whereas on top of the conditional expectation, the observed \\(Y\\) value can be scattered around the conditional expectations, like the graph on the left below: We can write the systematic part as: \\[\\mathrm{E}(Y \\mid X) = f(X; \\beta_1, \\beta_2, \\ldots), \\] where \\(\\beta_1\\), \\(\\beta_2\\), \\(\\ldots\\) are the parameters for some arbitrary function \\(f(\\cdot)\\). The random part is about \\(P(Y \\mid X)\\) which can take some arbitrary form of distributions. The problem is that in reality, even if such a model holds, we do not know what \\(f(\\cdot)\\) and the true distribution of \\(Y \\mid X\\) are, as we are only presented with data like those illustrated in the graph on the right above. The regression model we will discuss here, which you have learned (or will learn) in introductory statistics, assumes that the function for the systematic component, \\(f(\\cdot)\\), is a linear function (in the \\(\\beta\\)s), \\(Y \\mid X\\) is normally distributed, and \\(Y_i\\)’s are conditionally exchangeable given \\(X\\) with equal variance \\(\\sigma^2\\). Under these conditions, if we assume \\(Y\\) and \\(X\\) have a linear relationship that can be quantified by a straight line with an intercept \\(\\beta_0\\) and a slope \\(\\beta_1\\), we have a model \\[Y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 X_i, \\sigma)\\] 7.2 One Predictor 7.2.1 A continuous predictor We will use a data set, kidiq, that is available in the rstanarm package. You can import the data into R by (Internet connection needed): kidiq &lt;- haven::read_dta(&quot;http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta&quot;) Or from the file I uploaded kidiq &lt;- haven::read_dta(&quot;../data/kidiq.dta&quot;) psych::describe(kidiq) &gt;# vars n mean sd median trimmed mad min max range skew &gt;# kid_score 1 434 86.80 20.41 90.0 87.93 19.27 20 144 124.0 -0.46 &gt;# mom_hs 2 434 0.79 0.41 1.0 0.86 0.00 0 1 1.0 -1.39 &gt;# mom_iq 3 434 100.00 15.00 97.9 99.11 15.89 71 139 67.9 0.47 &gt;# mom_work 4 434 2.90 1.18 3.0 2.99 1.48 1 4 3.0 -0.45 &gt;# mom_age 5 434 22.79 2.70 23.0 22.71 2.97 17 29 12.0 0.18 &gt;# kurtosis se &gt;# kid_score -0.19 0.98 &gt;# mom_hs -0.07 0.02 &gt;# mom_iq -0.59 0.72 &gt;# mom_work -1.39 0.06 &gt;# mom_age -0.65 0.13 Below is a description of the data kidiq Data from a survey of adult American women and their children (a subsample from the National Longitudinal Survey of Youth). Source: Gelman and Hill (2007) 434 obs. of 5 variables kid_score Child&#39;s IQ score mom_hs Indicator for whether the mother has a high school degree mom_iq Mother&#39;s IQ score mom_work 1 = did not work in first three years of child&#39;s life 2 = worked in 2nd or 3rd year of child&#39;s life 3 = worked part-time in first year of child&#39;s life 4 = worked full-time in first year of child&#39;s life mom_age Mother&#39;s age 7.2.1.1 Visualizing the data Let’s first see a scatterplot matrix psych::pairs.panels(kidiq) We will first use mother’s score on an IQ test to predict the child’s test score, as shown in the following scatter plot library(ggplot2) # With ggplot2, first specify the `aesthetics`, i.e., what is the x variable # and what is the y variable ggplot(aes(x = mom_iq, y = kid_score), data = kidiq) + geom_point(size = 0.7) + # add a layer with the points geom_smooth() # add a smoother &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Here we use the ggplot2 package to plot the data. You have already used the this package for some previous assignments and exercise, but here I’ll give you a little bit more information. It is an extremely powerful graphical system based on the grammar of graphics (gg), and is used a lot in for data analysts in both academia and industry (if you want to learn more, check out this tutorial: http://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html and this book: https://ggplot2-book.org). The blue line above is obtained with smoothing, which is a non-parametric way to estimate the true relationship between \\(X\\) and \\(Y\\) and can be used to check whether a linear regression model is appropriate. The grey region is the 95% CI for the smoother. 7.2.1.2 Choosing a model We will use a linear regression model: \\[\\texttt{kid_score}_i \\sim \\mathrm{N}(\\mu_i, \\sigma)\\] which, as you should recognize, is the normal model with conditional exchangeability you’ve seen for group comparisons. However, this time \\(\\mu_i\\) is modelled as a function of a continuous variable, mom_iq, instead of a binary grouping variable: \\[\\mu_i = \\beta_0 + \\beta_1 \\texttt{mom_iq}_i\\] In this model there are three parameters: \\(\\beta_0\\): mean kid_score when mom_iq = 0; also called regression intercept. \\(\\beta_1\\): mean increase in kid_score for every unit increase in mom_iq; also called regression slope or regression coefficient. \\(\\sigma\\): error standard deviation \\(\\sigma\\); i.e., variability of kid_score among those with same mom_iq score, and is assumed constant across mom_iq levels. You may not be aware when you first learned regression that \\(\\sigma\\), sometimes also called residual standard error in least square estimation, is also a parameter; however, as long as it appears in the conditional distribution it needs to be estimated. 7.2.1.3 Choosing priors In the general case, we need to specify a 3-dimensional joint prior distribution for the three parameters. However, a general practice is to assume prior independence among the parameters, which implies that prior to looking at the data, we have no knowledge whether the parameters are positively related or negatively related. With independence we are allowed to just specify three different priors for the three parameters. In general, we want to be conservative by specifying some weakly informative priors, so the variance of the priors should be large but not unrealistic. For example, we don’t expect a one unit difference in mom_iq is associated with a 100 units difference in kid_iq. Also, to increase numerical stability, we will rescale mom_iq and kid_iq by dividing them by 100, respectively: kidiq100 &lt;- kidiq %&gt;% mutate(mom_iq = mom_iq / 100, # divid mom_iq by 100 kid_score = kid_score / 100) # divide kid_score by 100 We will be using the prior distributions: \\[\\begin{align*} \\beta_0 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_1 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\sigma &amp; \\sim t^+(4, 0, 1) \\end{align*}\\] which are similar to the ones in the group comparison example. The half-\\(t\\) distribution is recommended by Gelman (2006) and has the following shape: &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() As you can see, there is more density towards zero, but the tail is still quite heavy (as compared to a normal distribution), as you can see by comparing it to the tail of a half-normal distribution (which just means it starts from 0 instead of \\(-\\infty\\)). This will avoid some extremely large values, but also not be overly restrictive in case \\(\\sigma\\) is extremely large. These priors can be set in brms. First check the default prior set up using get_prior: get_prior(kid_score ~ mom_iq, data = kidiq100) &gt;# prior class coef group resp dpar nlpar bound &gt;# 1 b &gt;# 2 b mom_iq &gt;# 3 student_t(3, 0.9, 2.5) Intercept &gt;# 4 student_t(3, 0, 2.5) sigma m1 &lt;- brm(kid_score ~ mom_iq, data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), prior(normal(0, 1), class = &quot;b&quot;, coef = &quot;mom_iq&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) 7.2.1.4 Obtaining the posteriors 7.2.1.4.1 Check convergence The brm function by default used 4 chains, with 2,000 iterations for each chain, and half of the iterations are used for warmup (so leaving 4,000 draws in total for summarizing the posterior). If you run summary(m1) (in the next subsection), you will get a summary of the posterior distributions for each parameter, and in this example all Rhat is 1.00, so it appears that the chains have converged. You can see more with a graphical interface: shinystan::launch_shinystan(m1) 7.2.1.4.2 Summarizing the posterior If you use the summary function on the model you will see a concise output with the estimate and the posterior SD. summary(m1, prob = 0.95) # prob = 0.95 is the default &gt;# Family: gaussian &gt;# Links: mu = identity; sigma = identity &gt;# Formula: kid_score ~ mom_iq &gt;# Data: kidiq100 (Number of observations: 434) &gt;# Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; &gt;# total post-warmup samples = 4000 &gt;# &gt;# Population-Level Effects: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# Intercept 0.26 0.06 0.15 0.38 1.00 3331 2414 &gt;# mom_iq 0.61 0.06 0.49 0.72 1.00 3248 2385 &gt;# &gt;# Family Specific Parameters: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# sigma 0.18 0.01 0.17 0.20 1.00 4069 2777 &gt;# &gt;# Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS &gt;# and Tail_ESS are effective sample size measures, and Rhat is the potential &gt;# scale reduction factor on split chains (at convergence, Rhat = 1). And here is the HPDI with the broom package and the tidy() function: broom::tidy(m1, conf.method = &quot;HPDinterval&quot;, conf.level = .90) &gt;# term estimate std.error lower upper &gt;# 1 b_Intercept 0.261 0.05862 0.165 0.358 &gt;# 2 b_mom_iq 0.607 0.05789 0.511 0.702 &gt;# 3 sigma 0.183 0.00604 0.174 0.193 &gt;# 4 lp__ 117.176 1.18197 114.843 118.463 You can plot the density and mixing of the posterior distributions: plot(m1) 7.2.1.5 Posterior Predictive Check Now, we want to draw some new data based on the posterior distributions. This can be done with pp_check pp_check(m1, nsamples = 100) Looks like there is some skewness not captured in the model. We will talk more about diagnostics for regression models next week. 7.2.1.6 Visualizing and interpreting Using the posterior mean, we have the following regression line \\[\\widehat{\\texttt{kid_score}} = 0.261 + 0.607 \\times \\texttt{mom_iq}\\] So, based on our model, if we observe two participants with 1 unit difference in mom_iq, the child’s IQ score is expected to be different by 0.607 points, 95% CI [0.494, 0.720]. As mom_iq and kid_score are on similar scale, there seems to be strong heritability for IQ. However, in Bayesian statistics, we want to be explicit about the uncertainty in the parameter estimate, as the posterior mean/median is just one of the infinitely many possible values in the posterior distribution. We can visualize with the following code: draws_m1 &lt;- as.data.frame(m1) # Store the posterior draws as a data frame # change the names for the first 2 columns just for convenience colnames(draws_m1)[1:2] &lt;- c(&quot;a&quot;, &quot;b&quot;) ggplot(aes(x = mom_iq, y = kid_score), data = kidiq100) + # Add transparent regression lines using different posterior draws geom_abline(data = draws_m1, aes(intercept = a, slope = b), color = &quot;skyblue&quot;, size = 0.2, alpha = 0.10) + geom_point(size = 0.7) + # add a layer with the points # Add the predicted line with the posterior means on top geom_abline(intercept = fixef(m1)[1, &quot;Estimate&quot;], slope = fixef(m1)[2, &quot;Estimate&quot;]) Or with brms, we can use the handy marginal_effects() function: plot(marginal_effects(m1), points = TRUE, point_args = list(size = 0.5)) &gt;# Warning: Method &#39;marginal_effects&#39; is deprecated. Please use &gt;# &#39;conditional_effects&#39; instead. 7.2.1.6.1 Predictive intervals In addition, one can construct a predictive interval for each level of mom_iq. A 90% predictive interval is one such that a new observation generated from our model will have a 90% chance of falling in that interval. This is an interval about the probability of new data, \\(\\tilde y\\), which is different from a credible interval as the latter is about the probability of the parameter. # Need to load the mmp_brm.R script mmp_brm(m1, x = &quot;mom_iq&quot;, prob = 0.90, plot_pi = TRUE) # the predictive interval will be shown in green &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 7.2.1.6.2 \\(R^2\\) effect size We can also compute an \\(R^2\\) as an effect size for the results. \\(R^2\\) is the proportion of variance of the outcome variable predicted by the predictor, or \\[R^2 = \\frac{\\mathrm{Var}(\\beta_0 + \\beta_1 X)}{\\mathrm{Var}(\\beta_0 + \\beta_1 X) + \\sigma^2} = \\frac{\\beta_1^2 \\mathrm{Var}(X)}{\\beta_1^2 \\mathrm{Var}(X) + \\sigma^2} = 1 - \\frac{\\sigma^2}{\\beta_1^2 \\mathrm{Var}(X) + \\sigma^2}\\] Without going too much into the detail, you can get: bayes_R2(m1) # Bayesian R^2 &gt;# Estimate Est.Error Q2.5 Q97.5 &gt;# R2 0.199 0.0302 0.14 0.258 bayes_R2(m1, summary = FALSE) %&gt;% mcmc_areas(prob = .90) # showing density and 95% CI This is interpreted as: Based on the model, 19.942% of the variance of kid’s score can be predicted by mother’s IQ, 95% CI [ 14.025%, 25.8%]. Note that \\(R^2\\) is commonly referred to variance explained, but as “explained” usually implies causation this only makes sense when causal inference is the goal. 7.2.2 Centering In the previous model, the intercept is the estimated mean kid_score when mom_iq is zero. As illustrated in the graph below, this value is not very meaningful, as mom_iq = 0 is far from the main bulk of data: And many scholar caution against extrapolation in regression. Therefore, for interpretation purpose one should consider center the predictors so that the zero point is meaningful. One can center the predictors to a meaningful value in the data. For mom_iq, usually the population mean for IQ is 100, so we can center the predictor to 1 by subtracting 1 from it: kidiq100 &lt;- kidiq100 %&gt;% mutate(mom_iq_c = mom_iq - 1) m1c &lt;- brm(kid_score ~ mom_iq_c, data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), prior(normal(0, 1), class = &quot;b&quot;, coef = &quot;mom_iq_c&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) broom::tidy(m1c) &gt;# term estimate std.error lower upper &gt;# 1 b_Intercept 0.868 0.00871 0.853 0.882 &gt;# 2 b_mom_iq_c 0.607 0.05865 0.512 0.702 &gt;# 3 sigma 0.183 0.00638 0.173 0.194 &gt;# 4 lp__ 117.109 1.24779 114.517 118.465 Now, we can interpret the intercept as the predicted average kid_score when mom_iq = 100 (or 1 in the rescaled version) for participants whose mother does not have a high school degree, which is 86.771 points, 95% CI [85.059, 88.541]. Centering is especially important when evaluating interaction models. 7.2.3 A categorical predictor We can repeat the analysis using a categorical predictor, mom_hs, indicating whether the mother has a high school degree (1 = yes, 0 = no). We can visualize the data: ggplot(aes(x = factor(mom_hs), y = kid_score), data = kidiq100) + geom_boxplot() # add a layer with a boxplot Our regression model is \\[\\begin{align} \\texttt{kid_score}_i &amp; \\sim \\mathcal{N}(\\mu_i), \\sigma) \\\\ \\mu_i &amp; = \\beta_1 \\texttt{mom_hs}_i \\end{align}\\] where \\(\\beta_0\\) is the expected kid_score when the mother did not have a high school degree, and \\(\\beta_1\\) is the expected difference between those whose mothers have a high school degree and those without. We will choose the following priors: \\[\\begin{align*} \\beta_0 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_1 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\sigma &amp; \\sim t^+(4, 0, 1) \\end{align*}\\] It is safe to say that whether mother has a high school degree would not lead to a difference of 100 points in IQ. # First recode `mom_hs` to be a factor (not necessary but useful for plot) kidiq100 &lt;- kidiq100 %&gt;% mutate(mom_hs = factor(mom_hs, labels = c(&quot;no&quot;, &quot;yes&quot;))) m2 &lt;- brm(kid_score ~ mom_hs, data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) You can use the summary function, or the tidy() function in the broom package: broom::tidy(m2) &gt;# term estimate std.error lower upper &gt;# 1 b_Intercept 0.775 0.02027 0.7424 0.808 &gt;# 2 b_mom_hsyes 0.118 0.02320 0.0805 0.156 &gt;# 3 sigma 0.199 0.00678 0.1886 0.211 &gt;# 4 lp__ 81.244 1.24461 78.8563 82.565 The chains have converged. Using the posterior medians, the estimated child’s IQ score is 77.506 points, 95% CI [73.464, 81.449] for the group whose mother does not have a high school degree, and the estimated average difference between the group whose mother has a high school degree and those who does not on kid_score is 11.800 points, 95% CI [7.200, 16.306]. We can also obtain the posterior distribution for the mean of the mom_hs = 1 group by adding up the posterior draws of \\(\\beta_0\\) and \\(\\beta_1\\): draws_m2 &lt;- as_tibble(m2) # Store the posterior draws as a data frame # Add up the two columns to get the predicted mean for `mom_hs = 1` yhat_hs &lt;- draws_m2$b_Intercept + draws_m2$b_mom_hsyes psych::describe(yhat_hs) &gt;# vars n mean sd median trimmed mad min max range skew kurtosis se &gt;# X1 1 4000 0.89 0.01 0.89 0.89 0.01 0.85 0.94 0.08 0.03 0.15 0 You can also use marginal_effects(): plot(marginal_effects(m2)) &gt;# Warning: Method &#39;marginal_effects&#39; is deprecated. Please use &gt;# &#39;conditional_effects&#39; instead. This is an example of the beauty of Bayesian and the MCMC method. In frequentist, although it’s easy to get \\(\\hat{\\beta_0} + \\hat{\\beta_1}\\), it is hard to get the corresponding \\(\\mathit{SE}\\), whereas with MCMC, one just needs to do the addition in each iteration, and in the end all those values will form the posterior samples of \\(\\beta_0 + \\beta_1\\). 7.2.4 Predictors with multiple categories In Bayesian, using predictors with multiple categories is just the same as in frequentist. In R this can be handled automatically. For example, if I recode mom_iq into three categories: kidiq_cat &lt;- kidiq100 %&gt;% mutate(mom_iq_cat = findInterval(mom_iq, c(.7, .85, 1, 1.15)) %&gt;% factor(labels = c(&quot;low&quot;, &quot;below average&quot;, &quot;above average&quot;, &quot;high&quot;))) I can put the categorical predictor into the model with brm() m1_cat &lt;- brm(kid_score ~ mom_iq_cat, data = kidiq_cat, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) And R by default will choose the first category as the reference group. See the results below. plot(marginal_effects(m1_cat)) &gt;# Warning: Method &#39;marginal_effects&#39; is deprecated. Please use &gt;# &#39;conditional_effects&#39; instead. 7.2.5 STAN It’s also easy to implement it in STAN library(rstan) rstan_options(auto_write = TRUE) data { int&lt;lower=0&gt; N; // number of observations vector[N] y; // response variable; int&lt;lower=0&gt; p; // number of predictor variables (exclude intercept) matrix[N, p] X; // predictor variable;matrix } parameters { real beta_0; // intercept vector[p] beta; // slopes real&lt;lower=0&gt; sigma; // error standard deviation } model { // `normal_id_glm` is specially designed for regression y ~ normal_id_glm(X, beta_0, beta, sigma); // prior beta_0 ~ normal(0, 1); beta ~ normal(0, 1); sigma ~ student_t(4, 0, 1); } generated quantities { real yrep[N]; // simulated data based on model vector[N] yhat = beta_0 + X * beta; // used to compute R-squared effect size for (i in 1:N) { yrep[i] = normal_rng(yhat[i], sigma); } } m1_stan &lt;- stan(&quot;../codes/normal_regression.stan&quot;, data = list(N = nrow(kidiq100), y = kidiq100$kid_score, p = 1, X = as.matrix(kidiq100$mom_iq_c)), seed = 1234) And the \\(R^2\\) can be obtained as m1_r2 &lt;- bayes_R2(as.matrix(m1_stan, &quot;yhat&quot;), y = kidiq100$kid_score) psych::describe(m1_r2) &gt;# vars n mean sd median trimmed mad min max range skew kurtosis se &gt;# X1 1 4000 0.2 0.03 0.2 0.2 0.03 0.1 0.3 0.2 0.04 -0.09 0 7.3 Multiple Regression 7.3.1 Two Predictor Example Now let’s put both predictors to the model, as in multiple regression. \\[\\begin{align} \\texttt{kid_score}_i &amp; \\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq_c}_i) + \\beta_2 (\\texttt{mom_hs}_i) \\end{align}\\] Remember that the coefficients are are slopes when all other predictors are constant. We will choose the following priors, same as the previous models: \\[\\begin{align*} \\beta_0 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_1 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_2 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\sigma &amp; \\sim t^+(4, 0, 1) \\end{align*}\\] m3 &lt;- brm(kid_score ~ mom_iq_c + mom_hs, data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) The chains have converged. We have the following results (with mcmc_areas) stanplot(m3, type = &quot;areas&quot;, prob = 0.90) &gt;# Warning: Method &#39;stanplot&#39; is deprecated. Please use &#39;mcmc_plot&#39; instead. We can plot the data with two regression lines (left for mom_hs = “no” and right for mom_hs = \"yes): plot( marginal_effects(m3, effects = &quot;mom_iq_c&quot;, # Request two lines using `conditions` conditions = tibble(mom_hs = c(&quot;no&quot;, &quot;yes&quot;))), points = TRUE, point_args = list(size = 0.5) ) &gt;# Warning: Method &#39;marginal_effects&#39; is deprecated. Please use &gt;# &#39;conditional_effects&#39; instead. Using the posterior mean, we have the following regression line \\[\\widehat{\\texttt{kid_score}} = 0.821 + 0.562 \\times \\texttt{mom_iq_c} + 0.06 \\times \\texttt{mom_hs}\\] So, based on our model, if we observe two participants with 1 unit difference in mom_iq_c, and for both the mothers have high school degree (or both without), the child’s IQ score is expected to be different by 0.562 points, 95% CI [0.442, 0.679]. On the other hand, for two observations with the same mom_iq_c, our model predicted that the child’s IQ score when the mother has high school degree is higher by 5.999 points, 95% CI [1.723, 10.226] on average. 7.3.2 Interactions The previous model assumes that the average difference in kid_score for participants that are 1 unit different in mom_iq_c is constant for the mom_hs = 1 group and the mom_hs = 0 group, as indicated by the same slope of the two regression lines associated with mom_iq_c. However, this assumption can be relaxed by including an interaction term: \\[\\begin{align} \\texttt{kid_score}_i &amp; \\sim \\mathcal{N}(\\mu_i), \\sigma) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq_c}_i) + \\beta_2 (\\texttt{mom_hs}_i) + \\beta_3 (\\texttt{mom_iq_c}_i \\times \\texttt{mom_hs}_i) \\\\ \\beta_0 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_1 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_2 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_3 &amp; \\sim \\mathcal{N}(0, 0.5) \\\\ \\sigma &amp; \\sim t^+(4, 0, 1) \\end{align}\\] Note that the prior scale is smaller for \\(\\beta_3\\). This is chosen because generally the magnitude of an interaction effect is smaller than the main effect. m4 &lt;- brm(kid_score ~ mom_iq_c * mom_hs, data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), # for interaction prior(normal(0, 0.5), class = &quot;b&quot;, coef = &quot;mom_iq_c:mom_hsyes&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) ~ mom_iq_c * mom_hs means including the interaction effect as well as the individual main effects. The chains have converged. We have the following results summary(m4) &gt;# Family: gaussian &gt;# Links: mu = identity; sigma = identity &gt;# Formula: kid_score ~ mom_iq_c * mom_hs &gt;# Data: kidiq100 (Number of observations: 434) &gt;# Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; &gt;# total post-warmup samples = 4000 &gt;# &gt;# Population-Level Effects: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# Intercept 0.85 0.02 0.80 0.89 1.00 2643 2505 &gt;# mom_iq_c 0.91 0.14 0.63 1.19 1.00 2103 2092 &gt;# mom_hsyes 0.03 0.02 -0.01 0.08 1.00 2822 2754 &gt;# mom_iq_c:mom_hsyes -0.42 0.16 -0.73 -0.11 1.00 2114 2057 &gt;# &gt;# Family Specific Parameters: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# sigma 0.18 0.01 0.17 0.19 1.00 3249 2175 &gt;# &gt;# Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS &gt;# and Tail_ESS are effective sample size measures, and Rhat is the potential &gt;# scale reduction factor on split chains (at convergence, Rhat = 1). Using the posterior median, we have the following regression line \\[\\widehat{\\texttt{kid_score}} = 0.849 + 0.911 \\times \\texttt{mom_iq_c} + 0.033 \\times \\texttt{mom_hs} + -0.419 \\times \\texttt{mom_iq_c} \\times \\texttt{mom_hs}\\] Interaction effect is generally not easy to interpret. It would be easier to write the regression line for mom_hs = “no” (0) and mom_hs = “yes” (1). To do this, note that the regression line for mom_hs = “yes” is \\[\\begin{align*} \\mathrm{E}(\\texttt{kid_score} \\mid \\texttt{mom_iq_c}, \\texttt{mom_hs} = 1) = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)(\\texttt{mom_iq_c}) = \\beta_0^* + \\beta_1^*(\\texttt{mom_iq_c}) \\end{align*}\\] Note that the posterior mean of \\(\\beta_0^*\\) is equal to the sum of the posterior means of \\(\\beta_0\\) and \\(\\beta_2\\) (and same for \\(\\beta_1^*\\)). (However, the posterior medians may be different, because the median is not a linear function of the posterior samples) When mom_hs = 0, \\[\\widehat{\\texttt{kid_score}} = 0.849 + 0.911 \\times \\texttt{mom_iq_c}\\] and when mom_hs = 1 \\[\\widehat{\\texttt{kid_score}} = 0.882 + 0.492 \\times \\texttt{mom_iq_c}\\] We can plot the data with two regression lines with the following code: plot( marginal_effects(m4, effects = &quot;mom_iq_c&quot;, # Request two lines using `conditions` conditions = tibble(mom_hs = c(&quot;no&quot;, &quot;yes&quot;))), points = TRUE, point_args = list(size = 0.5) ) &gt;# Warning: Method &#39;marginal_effects&#39; is deprecated. Please use &gt;# &#39;conditional_effects&#39; instead. We can get an \\(R^2\\) effect size. bayes_R2(m4) &gt;# Estimate Est.Error Q2.5 Q97.5 &gt;# R2 0.227 0.0312 0.163 0.288 So the two predictors, plus the main effect, explained 22.719% of the variance of kid_score. However, comparing to the model with only mom_iq_c as predictor, including mom_hs and the interaction increased the \\(R^2\\) by \\(2.777\\%\\). You can plot the density of the posterior distributions for the three \\(\\beta\\)s: # `pars = &quot;b&quot;` will include all regression coefficients stanplot(m4, type = &quot;areas&quot;, pars = &quot;b&quot;, prob = 0.90) &gt;# Warning: Method &#39;stanplot&#39; is deprecated. Please use &#39;mcmc_plot&#39; instead. And below I plot the 90% predictive intervals and the variations of the regression lines, separated by the status of mom_hs. The R code is a bit cumbersome though. # Obtain the predictive intervals pi_m4 &lt;- predictive_interval(m4, prob = 0.9) colnames(pi_m4) &lt;- c(&quot;lwr&quot;, &quot;upr&quot;) # change the names for convenience # Combine the PIs with the original data df_plot &lt;- cbind(kidiq100, pi_m4) # Create a data frame for the regression lines draws_m4 &lt;- as.matrix(m4) df_lines &lt;- rbind(data.frame(mom_hs = &quot;no&quot;, a = draws_m4[ , 1], b = draws_m4[ , 2]), data.frame(mom_hs = &quot;yes&quot;, a = draws_m4[ , 1] + draws_m4[ , 3], b = draws_m4[ , 2] + draws_m4[ , 4])) df_mean_line &lt;- data.frame(mom_hs = c(&quot;no&quot;, &quot;yes&quot;), a = c(fixef(m4)[1, &quot;Estimate&quot;], sum(fixef(m4)[c(1, 3), &quot;Estimate&quot;])), b = c(fixef(m4)[2, &quot;Estimate&quot;], sum(fixef(m4)[c(2, 4), &quot;Estimate&quot;]))) ggplot(aes(x = mom_iq_c, y = kid_score), data = df_plot) + facet_wrap( ~ mom_hs) + # Add a layer of predictive intervals geom_ribbon(aes(ymin = lwr, ymax = upr), fill = &quot;grey&quot;, alpha = 0.5) + geom_abline(data = df_lines, aes(intercept = a, slope = b), color = &quot;skyblue&quot;, size = 0.2, alpha = 0.10) + geom_point(size = 0.7, aes(col = factor(mom_hs))) + geom_abline(data = df_mean_line, aes(intercept = a, slope = b)) And you can see the uncertainty is larger for mom_hs = 0. This makes sense because there are less participants in this group. 7.4 Tabulating the Models There is a handy function in the sjPlot package, tab_model(), which can show a neat summary of the various models: sjPlot::tab_model(m1c, m2, m3, m4)   kid.score kid.score kid.score kid.score Predictors Estimates CI (95%) Estimates CI (95%) Estimates CI (95%) Estimates CI (95%) Intercept 0.87 0.85 – 0.89 0.78 0.73 – 0.81 0.82 0.78 – 0.86 0.85 0.80 – 0.89 mom.iq 0.61 0.49 – 0.72 0.56 0.44 – 0.68 0.91 0.63 – 1.19 mom_hs: yes 0.12 0.07 – 0.16 0.06 0.02 – 0.10 0.03 -0.01 – 0.08 mom_iq_c.mom_hsyes -0.42 -0.73 – -0.11 Observations 434 434 434 434 R2 Bayes 0.200 0.056 0.214 0.228 However right now it only supports HTML. You can also use the following code: source(&quot;extract_brmsfit.R&quot;) texreg::screenreg(map(list(m1c, m2, m3, m4), extract_brmsfit)) &gt;# &gt;# ============================================================================ &gt;# Model 1 Model 2 Model 3 Model 4 &gt;# ---------------------------------------------------------------------------- &gt;# Intercept 0.87 * 0.78 * 0.82 * 0.85 * &gt;# [0.85; 0.88] [0.73; 0.81] [0.78; 0.86] [ 0.80; 0.89] &gt;# mom_iq_c 0.61 * 0.56 * 0.91 * &gt;# [0.49; 0.72] [0.45; 0.68] [ 0.64; 1.20] &gt;# mom_hsyes 0.12 * 0.06 * 0.03 &gt;# [0.08; 0.17] [0.02; 0.10] [-0.01; 0.08] &gt;# mom_iq_c:mom_hsyes -0.42 * &gt;# [-0.75; -0.13] &gt;# ---------------------------------------------------------------------------- &gt;# R^2 0.20 0.06 0.21 0.23 &gt;# Num. obs. 434 434 434 434 &gt;# loo IC -240.07 -167.76 -245.19 -252.03 &gt;# WAIC -240.08 -167.77 -245.20 -252.05 &gt;# ============================================================================ &gt;# * Null hypothesis value outside the confidence interval. Replacing texreg::screenreg() by texreg::texreg() will generate table for PDF output. We will talk about model checking, robust models, and other extensions to the normal regression model next week. References "],
["model-diagnostics.html", "Chapter 8 Model Diagnostics 8.1 Assumptions of Linear Models 8.2 Diagnostic Tools 8.3 Other Topics", " Chapter 8 Model Diagnostics All statistical models are sets of assumptions about the data generating process, and estimation will be meaningless or misleading if theses assumptions do not hold for the data. As we have discussed, choosing a good model is generally way more important than choosing a good prior. In this week we will learn some tools to check the validity of linear models. Most of them are similar to what you have learned in frequentist regression. 8.1 Assumptions of Linear Models The assumptions of the linear model is encoded in the model. The model is \\[\\begin{align*} Y_i &amp; \\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots \\end{align*}\\] From the model we have the following assumptions, in the order of the most important one to the least important one: Correct specification of the model. This means that all relevant predictors for \\(Y\\) have been included in the model. This is probably an assumption that is never satisfied in real data, as one can never include all relevant factors that can have an impact on \\(Y\\), be it small or large. However, it is important to be thoughtful to include major predictors that have shown to relate to \\(Y\\). Leaving out key predictors can bias the coefficients \\(\\beta\\). Linearity. This is about the conditional mean, \\(\\mu = \\mathrm{E}(Y | X_1, X_2, \\ldots)\\), being a linear function. If you have a function like \\(\\mu = \\exp[\\beta_1 X_1 \\sin (\\beta_2 X_2)]\\), the conditional mean is not a linear function. Note that linearity does not require \\(\\mu\\) to be a linear function of predictors; quadratic and exponential relationships, interaction, and polynomials can all handled by linear models. (And technically, linearity requires \\(\\mu\\) to be a linear function of the coefficients.) Independent observations. This assumption is not directly encoded in the model equation above, mainly because I omit that part when writing out the model. This assumption requires that the value of one observation is independent to the value of another observation after taking into account the conditional mean, \\(\\mu\\). This will be discussed more in multilevel models. Equal variance of errors. This means that \\(\\sigma^2\\) has to be constant for each observation. In general, violation of this assumption is generally a minor issue, although it can affect the standard errors. Normality. This requires that the conditional distribution of \\(Y\\) is normal. Violation of the normality assumption generally does not affect the estimation of the coefficients, and will be a minor issue when the sample size is large enough (&gt; 30) and when the degree of nonnormality is small to moderate. 8.2 Diagnostic Tools Now let’s review some tools for regression diagnostics for Bayesian regression. There are hundreds of plots available that I will not cover here, and you can treat what is discussed in this note as a minimal requirement for regression diagnostics. The first one is about a correct specification of the model, which can be partly assessed with posterior predictive check. 8.2.1 Posterior Predictive Check kidiq &lt;- haven::read_dta(&quot;../data/kidiq.dta&quot;) kidiq100 &lt;- kidiq %&gt;% mutate(mom_iq = mom_iq / 100, # divid mom_iq by 100 kid_score = kid_score / 100, # divide kid_score by 100 mom_iq_c = mom_iq - 1, mom_hs = factor(mom_hs, labels = c(&quot;no&quot;, &quot;yes&quot;))) m4 &lt;- brm(kid_score ~ mom_iq_c * mom_hs, data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), # for interaction prior(normal(0, 0.5), class = &quot;b&quot;, coef = &quot;mom_iq_c:mom_hsyes&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) Below is the posterior predictive graphical check for the interaction model we fit last week (with centering): pp_check(m4, nsamples = 100) Based on the graphical check, we do not see any major systematic discrepancies of our data from what can be predicted from our model. We should do the posterior predictive check with test statistics too. The following functions show the mean, maximum value, and minimum value of the outcome. # PPC for the mean (it should always fit) pp_check(m4, type = &quot;stat_grouped&quot;, stat = &quot;mean&quot;, group = &quot;mom_hs&quot;) &gt;# Using all posterior samples for ppc type &#39;stat_grouped&#39; by default. &gt;# `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # PPC for the maximum and minimum values pp_check(m4, type = &quot;stat_2d&quot;, stat = c(&quot;max&quot;, &quot;min&quot;)) &gt;# Using all posterior samples for ppc type &#39;stat_2d&#39; by default. Here is a ribbon plot to check for outliers: pp_check(m4, type = &quot;ribbon_grouped&quot;, x = &quot;mom_iq_c&quot;, group = &quot;mom_hs&quot;) &gt;# Using all posterior samples for ppc type &#39;ribbon_grouped&#39; by default. Some points are outside the 90% predictive intervals. 8.2.2 Marginal model plots To check the linearity assumption, we need to make sure that the conditional mean of \\(Y\\) fit according to the model. A marginal model plot compares the model predicted relationship between the outcome and each predictor, and the relationship obtained using nonparametric methods with smoothing. There is not a built-in function for marginal model plot in R for Bayesian regression, but it’s available in the R function mmp_brm I wrote. mmp_brm(m4, x = &quot;mom_iq_c&quot;, prob = .95) &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Generally, marginal model plot is more appropriate for ordinal or continuous predictors. As you can see above, the red line (for nonparametric fit) and the blue line (from the linear model) fit the data well, except that for the left tail area where there are fewer data points. If the linearity assumption holds, these two lines should be very similar. Generally, a little bit of deviations in the left tail and the right tail are okay. Deviations in the middle would indicate a strong misspecification that needs to be fixed. Also, we want to check outliers that lie way outside of the predictive interval. With a 95% predictive interval, we generally expect 5% to lie outside of the predictive interval band. In this example we don’t see a great problem of outliers, and generally a few outliers would be okay with a moderate to large sample size. 8.2.3 Residual plots For regression analyses one can learn a lot about model fit from the residuals, which is \\(y_i - \\tilde{y}_i | \\theta\\), i.e., subtracting the observed \\(y_i\\) values by the posterior predictions. Because in Bayesian there are not just one prediction, but a whole predictive distribution, one also has an entire posterior distribution for each residual. Below is a check of the average residual \\(Y\\) (i.e., \\(Y\\) - \\(\\hat Y\\)) and the true value of \\(Y\\). If the model fit the data well, the points should be scattered with no specific pattern, like the graph below: pp_check(m4, type = &quot;error_scatter_avg_vs_x&quot;, size = 1.1, x = &quot;mom_iq_c&quot;) &gt;# Using all posterior samples for ppc type &#39;error_scatter_avg_vs_x&#39; by default. No big problem was found in the residuals. If you see that the SD of the residuals is not uniform, or the residuals have some non-linear relationships with the predictor, there can be some problems. 8.2.3.1 Standardized Residuals There are no intrinsic way to standardize the residuals in Bayesian methods, and it’s not necessary to standardize them. However, it’s easier to compare with frequentist results to flag cases with standardized residuals larger than 3 or 4 in standardized values. The plot below shows the standardized residuals against the predicted \\(y\\) values. res_df &lt;- m4$data %&gt;% mutate(predict_y = predict(m4)[ , &quot;Estimate&quot;], std_resid = residuals(m4, type = &quot;pearson&quot;)[ , &quot;Estimate&quot;]) &gt;# Warning: Type &#39;pearson&#39; is deprecated and will be removed in the future. ggplot(res_df, aes(predict_y, std_resid)) + geom_point(size = 0.8) + stat_smooth(se = FALSE) &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; All points are within the -3 to 3 range, so no big issues with outliers. Also, the errors of the residuals have similar variance/spread, so the equal variance assumption is fine. In addition, if there are any systematic pattern between the residuals and the predicted \\(y\\), or any systematic pattern between the residuals and any of the predictors, the model can be misspecified and some additional predictors or nonlinear terms should be included. 8.2.4 Multicollinearity Strictly speaking, multicollinearity is not an assumption of regression. However, especially in frequentist analysis, having predictors that are strongly correlated can increase the uncertainty of the posterior distributions of the regression coefficients. On the other hand, the use of the prior distribution in Bayesian analyses can somewhat come to the rescue, as it makes it less likely for the posteriors to have extremely large posterior mean and standard deviation [which sounds like an important research project]. You can look at the posterior density of the coefficients to see how correlated they are: pairs(m4, pars = &quot;b&quot;, off_diag_args = # arguments of the scatterplots list(size = 0.5, # point size alpha = 0.25)) # transparency If some coefficients are particularly strongly correlated, you may need to think about using a stronger prior or combining some predictors. Principal component and factor analysis are some approaches for that. In the above case it’s still okay. 8.2.5 Robust Models Robustness refers to the degree to which the results of statistical analyses change with assumption violations; a robust method would give similar (consistent) results when certain assumption is violated (to a certain degree). As you already know, there are several assumptions in a linear model; however, unfortunately, often when one says robust methods, it is not always the case that they specify which assumption violation the methods. More commonly than not, a robust method is robust to outliers/influential observations, where outliers are data points that are unusually distant from the majority of the data. For example, both the red and the green dots will be considered outliers in the graph below, as both are away from the majority of the data. The red point will be considered an influential observation because it can have a relatively large influence on the regression line. &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; 8.2.5.1 Student’s \\(t\\) regression One relatively straightforward way to extend the normal linear model to one with a \\(t\\) likelihood: m4t &lt;- brm(kid_score ~ mom_iq_c * mom_hs, data = kidiq100, family = student(), prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), # for interaction prior(normal(0, 0.5), class = &quot;b&quot;, coef = &quot;mom_iq_c:mom_hsyes&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) In brms, the default prior is \\(\\nu \\sim \\mathrm{Gamma}(2, 0.1)\\), with a lower bound of 1: ggplot(tibble(nu = c(1, 50)), aes(x = nu)) + stat_function(fun = function(x) { dgamma(x, 2, 0.1) / pgamma(1, 2, 0.1, lower.tail = TRUE) }) The data did not indicate strong outlier influence, as you can see from the nu parameter: stanplot(m4t, pars = &quot;nu&quot;, type = &quot;areas&quot;) &gt;# Warning: Method &#39;stanplot&#39; is deprecated. Please use &#39;mcmc_plot&#39; instead. which is pretty large. If you compared the results of the normal model and the \\(t\\) model the results appeared similar: source(&quot;extract_brmsfit.R&quot;) texreg::screenreg(map(list(m4, m4t), extract_brmsfit)) &gt;# &gt;# ================================================== &gt;# Model 1 Model 2 &gt;# -------------------------------------------------- &gt;# Intercept 0.85 * 0.85 * &gt;# [ 0.80; 0.89] [ 0.81; 0.89] &gt;# mom_iq_c 0.91 * 0.92 * &gt;# [ 0.64; 1.20] [ 0.64; 1.18] &gt;# mom_hsyes 0.03 0.03 &gt;# [-0.01; 0.08] [-0.01; 0.08] &gt;# mom_iq_c:mom_hsyes -0.42 * -0.43 * &gt;# [-0.75; -0.13] [-0.73; -0.13] &gt;# -------------------------------------------------- &gt;# R^2 0.23 0.23 &gt;# Num. obs. 434 434 &gt;# loo IC -252.03 -250.91 &gt;# WAIC -252.05 -250.93 &gt;# ================================================== &gt;# * Null hypothesis value outside the confidence interval. The LOOIC and the WAIC reported, which we will talk about in the next set of slides, but generally we prefer a model with a smaller value of LOOIC and WAIC. 8.2.5.2 Modeling the variability In the linear model, we have assumed that the \\(\\sigma\\) parameter is the same for every individual, which is the equal variance assumption. However, such an assumption may not always hold, especially in data when some scores represent aggregates of more than one person or time points (e.g., see your homework problem). For example, you can look at the residuals in the following: &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; Check the variability for each group. How are they different? If you suspect that the spreadness (\\(\\sigma\\)) depends on some predictors, you can run the following brms model to see whether the spreadness differs across mom_hs levels: m4h &lt;- brm(bf(kid_score ~ mom_iq_c * mom_hs, sigma ~ mom_hs), data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), # for interaction prior(normal(0, 0.5), class = &quot;b&quot;, coef = &quot;mom_iq_c:mom_hsyes&quot;), prior(normal(0, 1), dpar = &quot;sigma&quot;, class = &quot;b&quot;), prior(student_t(4, 0, 1), dpar = &quot;sigma&quot;, class = &quot;Intercept&quot;)), seed = 2302 ) So not much evidence for heterogeneity of variance across mom_hs in this case. The coefficients were expressed in log unit, so you need to exponentiate the coefficients to get back the \\(\\sigma\\) coefficient. Specifically, \\[\\begin{align*} \\log(\\sigma_i) &amp; = \\beta_0^{[s]} + \\beta_1^{[s]} \\texttt{mom_hs}_i \\end{align*}\\] The estimated error standard deviation was 0.193 when the mother did not have a high school degree, and 0.177 when the mother have a high school degree. However, accounting for the posterior uncertainty, there were not much evidence that \\(\\sigma\\) is different across mom_hs. 8.3 Other Topics There are other topics we have not discussed here for diagnostics of multiple regression, but are just as important as in frequentist analyses. These topics include: Transformation (e.g., logarithm transformation with skewed outcomes and predictors, like income); Leverage points and influential observations (e.g., hat values, Cook’s \\(D\\)) Measurement error of predictors "],
["model-comparison-and-regularization.html", "Chapter 9 Model Comparison and Regularization 9.1 Overfitting and Underfitting 9.2 Kullback-Leibler Divergence 9.3 Information Criteria 9.4 Information Criteria 9.5 Stacking/Model Averaging 9.6 Shrinkage Priors 9.7 Variable Selection", " Chapter 9 Model Comparison and Regularization 9.1 Overfitting and Underfitting In statistical modeling, a more complex model almost always results in a better fit to the data. Roughly speaking, a more complex model means one with more parameters, although as you will see later, in Bayesian analyses, number of parameters is sometimes not straight forward to find out. On the extreme side, if one has 10 observations, one can has a model with 10 parameters that can perfectly predict every single data point (by just having a parameter to predict each data point). However, there are two problems with too complex a model. First, an increasingly complex model also makes it increasingly hard to extract useful information from the data. Instead of describing the relationship between two variables, like mom_iq and kid_score, by a straight line, one ends up with a crazy model that is difficult to make sense. Second, as you will also see, the more complex a model, the more is the risk that it overfit the current data such that it does not work for future observations. For example, let’s randomly sample 10 cases in the kidiq data set, and build some model from it. kidiq &lt;- haven::read_dta(&quot;../data/kidiq.dta&quot;) kidiq100 &lt;- kidiq %&gt;% mutate(mom_iq = mom_iq / 100, # divid mom_iq by 100 kid_score = kid_score / 100, # divide kid_score by 100 mom_iq_c = mom_iq - 1, mom_hs = factor(mom_hs, labels = c(&quot;no&quot;, &quot;yes&quot;))) set.seed(1533) # set the seed for reproducibility # Sample 10 observations train &lt;- sample.int(nrow(kidiq), 10L) kidiq_sub &lt;- kidiq[train, ] base &lt;- ggplot(aes(x = mom_iq, y = kid_score), data = kidiq_sub) + geom_point(size = .9) + coord_cartesian(ylim = c(-120, 180)) + xlim(range(kidiq$mom_iq)) ggplot(aes(x = mom_iq, y = kid_score), data = kidiq) + geom_point(size = .7, col = &quot;lightblue&quot;) + geom_point(size = 1.1, data = kidiq_sub, col = &quot;red&quot;) + xlim(range(kidiq$mom_iq)) When using mom_iq to predict kid_score, we can use beyond a linear regression line by using higher order polynomials. For example, a second-order polynomial assumes a quadratic effect (with one turning point), and it goes to cubic, quartic, and more. The figure below shows the fit from a linear effect of mom_iq, a quadratic effect, and increasingly complex to a six degree polynomial. As you can see, as the model gets more complex, the fitted line tries to capture all the 10 points really well, with an increasing \\(R^2\\). However, also note that the standard error around the fitted line gets, meaning that there are more uncertainty when the model gets more and more complex. r2 &lt;- function(object) { # Function for computing R^2 z &lt;- object f &lt;- z$fitted.values r &lt;- z$residuals mss &lt;- if (attr(z$terms, &quot;intercept&quot;)) sum((f - mean(f))^2) else sum(f^2) rss &lt;- sum(r^2) mss / (mss + rss) } p_list &lt;- map(1:6, function(i) { mod &lt;- lm(kid_score ~ poly(mom_iq, degree = i), data = kidiq_sub) base + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, i), level = .80, fullrange = TRUE) + geom_text(x = 90, y = 170, label = paste0(&quot;italic(R)^2 == &quot;, round(r2(mod), 1)), parse = TRUE) + geom_text(x = 100, y = -100, label = paste0(&quot;RMSE == &quot;, round(sqrt(mean(residuals(mod)^2)), 1)), parse = TRUE) }) do.call(grid.arrange, c(p_list, nrow = 2)) Figure 9.1: Fit of models on the 10 random cases. Top panel: linear, quadratic, and cubic; bottom panel: 4th, 5th, and 6th degree polynomials Another way to look at the accuracy of the model is to look at the Root Mean Squared Error (RMSE), which is defined as the square root of the average squared prediction error. This is a measure of prediction error. The smaller the RMSE, the better the prediction is. As you can see in the above figure, more complex models always reduce the RMSE in the data we use to fit the model (also called training data). However, if I take the estimated regression line/curve based on the subsample of 10 observations, and predict the remaining cases in the data set, things will be different. As you can see in the figure below, whereas prediction error is comparable for the linear and the quadratic model, polynomials of higher degrees predict the data really badly. This is because when you use a complex model in a data set, it tailors the coefficients to any sampling errors and noise in the data such that it will not generalize to new observations. Therefore, our goal in model comparison is to choose a model that is complex enough to capture the essence of the data generation process (and thus avoid underfitting), but avoid overfitting as to make the model useless for predicting new observations. base2 &lt;- ggplot(aes(x = mom_iq, y = kid_score), data = kidiq[-train, ]) + geom_point(size = .6) + coord_cartesian(ylim = c(-120, 180)) + xlim(range(kidiq$mom_iq)) p_list2 &lt;- map(1:6, function(i) { mod &lt;- lm(kid_score ~ poly(mom_iq, degree = i), data = kidiq_sub) f &lt;- predict(mod, newdata = kidiq[-train, ]) y &lt;- kidiq$kid_score[-train] r &lt;- y - f rmse_mod &lt;- sqrt(mean(r^2)) base2 + geom_smooth(data = kidiq_sub, method = &quot;lm&quot;, formula = y ~ poly(x, i), fullrange = TRUE, level = .80) + geom_text(x = 100, y = -100, label = paste0(&quot;RMSE == &quot;, round(rmse_mod, 1)), parse = TRUE) }) do.call(grid.arrange, c(p_list2, nrow = 2)) Figure 9.2: Using the regression lines based on 10 random cases to predict the remaining 424 cases. Top panel: linear, quadratic, and cubic; bottom panel: 4th, 5th, and 6th degree polynomials The goal of statistical modeling is to choose a model that is optimal between the overfitting/underfitting dichotomy. In machine learning, this is also commonly referred to as the bias-variance trade-off, as a model that is too simple tends to produce biased predictions because it does not capture the essence of the data generating process, whereas a model that is overly complex is unbiased but results in a lot of uncertainty in the prediction, because there are too many unnecessary components that can affect predictions, as indicated in the confidence bands around the 6th degree polynomial line. Polynomials of varying degrees are merely one example of comparing simple to complex models. You can think about: models with and without interactions, models with a few predictors versus hundreds of predictors, regression analyses versus multilevel models, etc. This lecture is about finding an optimal model that avoids overfitting and avoids underfitting. Whereas one can always avoid underfitting by fitting a more and more complex model, a more practical problem is to have some tools to refrain oneself from choosing a model that is too complex and predict future observations badly. In this note, you will learn to perform model comparison with information criteria to find a model that has better balance between overfitting and underfitting, while in the next note you will learn additional tools that synthesize multiple models and perform variable selection. 9.2 Kullback-Leibler Divergence When comparing models (e.g., linear vs. quadratic), we prefer models that are closer to the “true” data-generating process. A model that are closer to the “true” model is better than a model that are not as close. Therefore, we need some ways to quantify the degree of “closeness” to the true model. Note that in this context models refer to the distributional family as well as the parameter values. For example, the model \\(y_i \\sim \\mathcal{N}(5, 2)\\) is a different model than \\(y_i \\sim \\mathcal{N}(3, 2)\\), which is a different model than \\(y_i \\sim \\mathrm{Gamma}(2, 2)\\). The first two have the same family but different parameter values (different means, same \\(\\mathit{SD}\\)), whereas the last two have different distributional families (Normal vs. Gamma). To measure the degree of “closeness” between two models, \\(M_0\\) and \\(M_1\\), by far the most popular metric in statistics is the Kullback-Liebler Divergence (or Kullback-Liebler discrepancy; \\(D_\\textrm{KL}\\)). By definition, \\[\\begin{align*} D_\\textrm{KL}(M_0 | M_1) &amp; = \\int_{-\\infty}^\\infty p_{M_0} (\\boldsymbol{\\mathbf{y}}) \\log \\frac{p_{M_0}(\\boldsymbol{\\mathbf{y}})}{p_{M_1}(\\boldsymbol{\\mathbf{y}})} \\; \\mathrm{d}\\boldsymbol{\\mathbf{y}} \\\\ &amp; = \\int_{-\\infty}^\\infty p_{M_0} (\\boldsymbol{\\mathbf{y}}) \\log p_{M_0}(\\boldsymbol{\\mathbf{y}}) \\; \\mathrm{d}\\boldsymbol{\\mathbf{y}} - \\int_{-\\infty}^\\infty p_{M_0} (\\boldsymbol{\\mathbf{y}}) \\log p_{M_1}(\\boldsymbol{\\mathbf{y}}) \\; \\mathrm{d}\\boldsymbol{\\mathbf{y}}. \\end{align*}\\] Note that strictly speaking, \\(D_\\textrm{KL}\\) cannot be called a “distance” between two models because in general, \\(D_\\textrm{KL}(M_0 | M_1) \\neq D_\\textrm{KL}(M_1 | M_0)\\). As an example, assume that the data are generated by a true model \\(M_0\\), and we have two candidate models \\(M_1\\) and \\(M_2\\), where \\(M_0: y \\sim \\mathcal{N}(3, 2)\\) \\(M_1: y \\sim \\mathcal{N}(3.5, 2.5)\\) \\(M_2: y \\sim \\mathrm{Cauchy}(3, 2)\\) ggplot(data.frame(x = c(-3, 9)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 3, sd = 2), aes(col = &quot;M0&quot;), linetype = 1) + stat_function(fun = dnorm, args = list(mean = 3.5, sd = 2.5), aes(col = &quot;M1&quot;), linetype = 2) + stat_function(fun = dcauchy, args = list(location = 3, scale = 2), aes(col = &quot;M2&quot;), linetype = 2) + scale_color_manual(&quot;&quot;, values = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), labels = c(&quot;M0&quot;, &quot;M1&quot;, &quot;M2&quot;)) + labs(x = &quot;y&quot;, y = &quot;density&quot;) &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() Figure 9.3: Density for \\(M_0\\), \\(M_1\\), and \\(M_2\\) One can compute that \\(D_\\textrm{KL}(M_0 | M_1) = 0.063\\) and \\(D_\\textrm{KL}(M_0 | M_1) = 0.259\\), and so \\(M_1\\) is a better model than \\(M_2\\). Note that in the expression of \\(D_\\textrm{KL}\\), when talking about the same target model, the first term is always the same and describes the “true” model, \\(M_0\\). Therefore, it is sufficient to compare models on the second term, \\(\\int_{-\\infty}^\\infty p_{M_0} (\\boldsymbol{\\mathbf{y}}) \\log p_{M_1}(\\boldsymbol{\\mathbf{y}}) \\; \\mathrm{d}\\boldsymbol{\\mathbf{y}}\\), which can also be written as \\(\\mathrm{E}=[\\log p_{M_1} (\\boldsymbol{\\mathbf{y}})]\\), i.e., the expected log predictive density (elpd). In other words, a model with a larger elpd is preferred over a model with a smaller elpd. However, in real data analysis, we don’t know what \\(M_0\\) is. If we knew, then we would just need to choose \\(M_0\\) as our model and there will be no problem about model comparisons. In addition, even if we know that the true model is, e.g., a normal model (which never happens in real data analysis), we still need to estimate the parameter values, and the estimates will not be exactly the same as the true parameter values. However, elpd is defined as the expected value over the true predictive distribution, \\(p_{M_0}(y)\\), which cannot be obtained without knowing what \\(M_0\\) is. So instead, we need to estimate the elpd. A naive way to estimate it would be to assume that the distribution of the data is the true model, but that will lead to an overly optimistic estimate, and computing elpd this way will always favor a more complex model. The best way to estimate elpd is to collect data on a new independent sample that is believed to share the same data generating process as the current sample, and estimate elpd on the new sample. This is called out-of-sample validation. The problem, of course, is we usually do not have the resources to collect a new sample. Therefore, statisticians had worked hard to find ways to estimate elpd from the current sample, and there are two broad approaches: Information criteria: AIC, DIC, and WAIC, which estimate the elpd in the current sample, minus a correction factor Cross validation, which splits the current sample into \\(k\\) parts, estimate the parameters in \\(k - 1\\) parts, and estimate the elpd in the remaining 1 part. A special case is when \\(k\\) = \\(N\\) so that each time one uses \\(N\\) - 1 data points to estimate the model parameters, and estimate the elpd for the observation that was left out. This is called leave-one-out cross-validation (LOO-CV). 9.3 Information Criteria We will illustrate the computation of information criteria with mom_iq predicting kid_score (with centering): # mom_iq with centering m1 &lt;- brm(kid_score ~ mom_iq_c, data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), prior(normal(0, 1), class = &quot;b&quot;, coef = &quot;mom_iq_c&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) Without going too deep into the underlying math, it can be shown that a good estimate of elpd is \\[\\sum_{i = 1}^n \\log p_{M_1}(y_i) - p,\\] where \\(p\\) is some measure of the number of parameters in \\(M_1\\). The first term is the likelihood of the model in the current sample. The second term is an adjustment factor so that the quantity above represents the average likelihood of the model in a new sample. It is more common to work with deviance by multiplying the log-likelihood by \\(-2\\), i.e., \\[D = -2 \\sum_{i = 1}^n \\log p_{M_1}(y_i).\\] 9.3.1 Experiment on Deviance Now, let’s check the in-sample deviance and out-of-sample deviance of our kidiq data with different polynomial functions. Here is a sample function for computing elpd (with frequentist, just for speed purpose) for different degrees of polynomial: # Function for computing deviance with different polynomial deviance_kidiq &lt;- function(degree = 1, train = 10, y = kidiq$kid_score, x = kidiq$mom_iq) { N &lt;- length(y) # get training sample if (length(train) == 1) { train &lt;- sample.int(N, train) } ntrain &lt;- length(train) # Obtain design matrix X &lt;- cbind(1, poly(x, degree, simple = TRUE)) # Get elpd for training sample Xtrain &lt;- X[train, ] ytrain &lt;- y[train] betahat &lt;- qr.solve(Xtrain, ytrain) # estimated betas res_train &lt;- ytrain - Xtrain %*% betahat sigmahat &lt;- sqrt(sum(res_train^2) / (ntrain - 1 - degree)) # estimated sigma deviance_train &lt;- -2 * sum(dnorm(res_train, sd = sigmahat, log = TRUE)) res_test &lt;- y[-train] - X[-train, ] %*% betahat deviance_test &lt;- -2 * sum(dnorm(res_test, sd = sigmahat, log = TRUE)) tibble(degree = degree, sample = c(&#39;in-sample&#39;, &#39;out-of-sample&#39;), deviance = c(deviance_train / ntrain, deviance_test / (N - ntrain)) ) } Below shows the in-sample and out-of-sample elpd for linear model: deviance_kidiq(degree = 1, train = train) &gt;# # A tibble: 2 x 3 &gt;# degree sample deviance &gt;# &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &gt;# 1 1 in-sample 7.92 &gt;# 2 1 out-of-sample 8.88 And for quadratic: deviance_kidiq(degree = 2, train = train) &gt;# # A tibble: 2 x 3 &gt;# degree sample deviance &gt;# &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &gt;# 1 2 in-sample 7.94 &gt;# 2 2 out-of-sample 8.84 As you can see, in general, the deviance is smaller for the current data than for the hold-out data. Note also because the data sets have different size, I divide the deviance by the sample size so that they can be compared. Now let’s run an experiment to check the elpd with different degrees polynomial, with a training sample size of 60: set.seed(1733) # Use the `map` function to run different polynomials, and use the `rerun` # function run the deviance 100 times. The code below runs `deviance_kidiq` by # randomly sampling 30 training samples 100 times, and compute the in-sample and # out-of-sample deviance for each. # rerun(100, deviance_kidiq(degree = 1, train = 30L)) %&gt;% # bind_rows() # Now run 1 to 8 degree polynomial, each 1000 times: dev_df &lt;- map_df(1:6, ~ rerun(1000, deviance_kidiq(degree = .x, train = 60L)) %&gt;% bind_rows) # Plot the results dev_df %&gt;% ggplot(aes(x = degree, y = deviance, col = sample)) + stat_summary() + stat_summary(geom = &quot;line&quot;) &gt;# No summary function supplied, defaulting to `mean_se()` &gt;# No summary function supplied, defaulting to `mean_se()` As you can see, the in-sample deviance (red line) keeps decreasing, indicating that a more complex model fit the data better, which is always the case. So if one were to use deviance to determine what model is optimal, one will always choose a model that is most complex, just like using \\(R^2\\) (indeed, for linear models deviance is basically the same as \\(R^2\\)). Now, look at the blue line, which represents the deviance computed using the coefficients obtained from the training set but applied to the remaining data. As you can see, the deviance achieves its minimum around the linear and the quadratic model, and starts to increase, meaning that more complex model does not fit the hold out data. A statistical model is used to learn something from a data set that can generalize to other observations. Therefore, we should care about the blue line, instead of the red one. The indices you will see in the remaining of this note are all attempts to approximate the blue line. More complex models always fit the current data better, but may not generalize to other data. In other words, models that are too complex are not generalizable. 9.4 Information Criteria 9.4.1 Akaike Information Criteria (AIC) Multiplying the quantity of elpd - \\(p\\) by \\(-2\\), or deviance + 2\\(p\\), with the deviance obtained using the maximum likelihood estimates (MLEs) for the parameters, gives you exactly the formula for AIC: \\[\\textrm{AIC} = D(\\hat \\theta) + 2p,\\] and \\(p\\) in AIC is taken to be just the number of parameters. As we have multiplied by a negative number, maximizing the estimate of elpd is equivalent to minimizing the AIC, so one would prefer a model with the smallest AIC. The approximation of AIC works best when the probability distribution under the \\(M_1\\) is normal, and that the sample size is much larger than the number of parameters. It is nothing Bayesian because there is no posterior distributions used, as \\(D\\) is computed only based on the MLE. Also, it does not take into account any prior information. # Frequentist model m1_freq &lt;- lm(m1$formula, data = m1$data) AIC(m1_freq) &gt;# [1] -240 9.4.2 Deviance Information Criteria (DIC) The definition of AIC assumes that the parameter estimates are known or are maximum likelihood estimates. The DIC, instead, replaces those with the posterior distribution of the parameters. The general formula for DIC is \\[\\textrm{DIC} = \\mathrm{E}(D | \\boldsymbol{\\mathbf{y}}) + 2 p_D,\\] where \\(p_D\\) is the effective number of parameters estimated in the Markov chain. Although DIC does take into account the prior distributions, and \\(\\mathrm{E}(D | \\boldsymbol{\\mathbf{y}})\\) is based on a posterior distribution, it still works best when the posterior distributions are multivariate normal, and that \\(N \\gg p\\). # Function to compute DIC dic_brmsfit &lt;- function(object) { Dbar &lt;- -2 * mean(rowSums(log_lik(object))) coef_pmean &lt;- unname(fixef(m1)[ , &quot;Estimate&quot;]) X &lt;- model.matrix(as.formula(object$formula), object$data) res &lt;- res &lt;- residuals(m1)[ , &quot;Estimate&quot;] N &lt;- length(res) sigma &lt;- posterior_summary(m1, pars = &quot;sigma&quot;)[ , &quot;Estimate&quot;] Dhat &lt;- -2 * sum(dnorm(res, sd = sigma, log = TRUE)) p &lt;- Dbar - Dhat elpd &lt;- Dhat / -2 - p data.frame(elpd_dic = elpd, p_dic = p, dic = Dhat + 2 * p, row.names = &quot;Estimate&quot;) } dic_brmsfit(m1) &gt;# elpd_dic p_dic dic &gt;# Estimate 120 3.01 -240 9.4.3 Watanabe-Akaike Information Criteria (WAIC) A further modification has been proposed to use the log pointwise posterior predictive density, with the effective number of parameters computed using the posterior variance of the likelihood. \\[\\textrm{WAIC} = -2 \\sum_{i = 1}^n \\log \\mathrm{E}[p(y_i | \\boldsymbol{\\mathbf{\\theta}}, \\boldsymbol{\\mathbf{y}})] + 2 p_\\textrm{WAIC},\\] where \\(\\mathrm{E}[p(y_i | \\boldsymbol{\\mathbf{\\theta}}, \\boldsymbol{\\mathbf{y}})]\\) is the posterior mean of the likelihood of the \\(i\\)th observation. The WAIC incorporates prior information, and the use of pointwise likelihood makes it more robust when the posterior distributions deviate from normality. In general, WAIC is a better estimate of the out-of-sample deviance than AIC and DIC. waic(m1) # built-in function in brms &gt;# &gt;# Computed from 4000 by 434 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_waic 120.0 14.5 &gt;# p_waic 2.9 0.3 &gt;# waic -240.1 28.9 9.4.4 Leave-One-Out Cross Validation The idea of cross-validation is to split the sample so that it imitates the scenario of estimating the parameters in part of the data and predicting the remaining part. The part that is used for estimation is called the training set, and the part that is used for prediction is called the validation set. Leave-one-out information criteria (LOO-IC) means that one uses \\(N - 1\\) observations as the training set and 1 observation as the validation sample, repeat the process \\(N\\) times so that each time a different observation is being predicted, and adding up the prediction results will give an estimate of elpd that closely approximates the results that would be obtained by collecting new data and doing the validation. To make it more concrete, we can go back to the kidiq data with mom_iq predicting kid_score. We can do this for case #286, as an example: # Estimate the model without case #286 m1_no286 &lt;- update(m1, newdata = kidiq100[-286, ]) &gt;# Start sampling # The log predictive density for case #286 mean(log_lik(m1_no286, newdata = kidiq100[286, ])) &gt;# [1] -4.2 Because LOO-IC requires fitting the model \\(N\\) times, it is generally very computational intensive. There are, however, shortcuts for some common models that make it computed faster. Otherwise, WAIC can be treated as a fast approximation of LOO-IC, although LOO-IC is more robust and will be a better estimate of out-of-sample deviance. In STAN, it uses the so called Pareto smoothed importance sampling (PSIS) to make the process faster, without having to repeat the process \\(N\\) times. Here is the LOO-IC for the model: loo(m1) &gt;# &gt;# Computed from 4000 by 434 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo 120.0 14.5 &gt;# p_loo 2.9 0.3 &gt;# looic -240.1 28.9 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.0. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. You can save the WAIC and the LOO-IC information to the fitted result: m1 &lt;- add_criterion(m1, c(&quot;loo&quot;, &quot;waic&quot;)) See Vehtari, Gelman, and Gabry (2016) for more discussions on WAIC and LOO-IC. 9.4.5 Example Consider four potential models in predicting kid_score: \\[\\texttt{kidscore}_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\] \\[\\begin{align*} \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq}_i) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq}_i) + \\beta_2 (\\texttt{mom_hs}_i) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq}_i) + \\beta_2 (\\texttt{mom_hs}_i) + \\beta_3 (\\texttt{mom_iq}_i \\times \\texttt{mom_hs}_i) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq}_i) + \\beta_2 (\\texttt{mom_hs}_i) + \\beta_3 (\\texttt{mom_iq}_i \\times \\texttt{mom_hs}_i) + \\beta_4 (\\texttt{mom_age}_i) \\end{align*}\\] The first model only has mom_iq as a predictor, which is equivalent to saying that the coefficients for mom_hs and mom_age are zero. The second model added mom_hs as a predictor. The third model includes an additional interaction term, whereas the fourth model also include mom_age. Now, we can compare the four models: loo_compare(m1, m2, m3, m4) &gt;# elpd_diff se_diff &gt;# m3 0.0 0.0 &gt;# m4 -0.4 1.1 &gt;# m2 -3.4 2.5 &gt;# m1 -6.0 3.9 # m3 is the best # show details loo(m1, m2, m3, m4) &gt;# Output of model &#39;m1&#39;: &gt;# &gt;# Computed from 4000 by 434 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo 120.0 14.5 &gt;# p_loo 2.9 0.3 &gt;# looic -240.1 28.9 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.0. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Output of model &#39;m2&#39;: &gt;# &gt;# Computed from 4000 by 434 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo 122.6 14.2 &gt;# p_loo 4.0 0.4 &gt;# looic -245.2 28.4 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.0. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Output of model &#39;m3&#39;: &gt;# &gt;# Computed from 4000 by 434 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo 126.0 14.3 &gt;# p_loo 4.9 0.5 &gt;# looic -252.0 28.7 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.0. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Output of model &#39;m4&#39;: &gt;# &gt;# Computed from 4000 by 434 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo 125.7 14.4 &gt;# p_loo 5.9 0.6 &gt;# looic -251.3 28.8 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.0. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Model comparisons: &gt;# elpd_diff se_diff &gt;# m3 0.0 0.0 &gt;# m4 -0.4 1.1 &gt;# m2 -3.4 2.5 &gt;# m1 -6.0 3.9 Model 3 has the lowest LOO-IC, although if you compare the difference in LOO-IC between Model 3 and Model 4 and the corresponding standard errors (in the column se_diff), the difference is relatively small. Given that Model 3 achieves the smallest LOO-IC and is simpler than Model 4, one may conclude that Model 3 is the best model among the four. 9.5 Stacking/Model Averaging Sometimes it may not be a good practice to only choose one model with low WAIC or LOO-IC, especially when several models have very similar WAIC/LOO-IC, but they make somewhat different predictions. Instead, we can perform stacking or model averaging by weighting the predictions from multiple models, using weights that are based on their information criteria performance. Stacking approaches this by optimizing the leave-one-out mean squared error in the resulting prediction, whereas model averaging preserves the uncertainty and was not optimized for that task. The technical details can be found in Yao et al. (2018). Note that the conventional Bayesian model averaging used the posterior model probability (Hoeting et al. 1999), which are approximated by the BIC. The discussion in this note is based on more recent discussion in, e.g., Yao et al. (2018). Let’s run four models on some training data by randomly splitting the data into half. First rescale some of the variables: I will run four models, which is from the last note \\[\\texttt{kidscore}_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\] \\[\\begin{align*} \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq}_i) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq}_i) + \\beta_2 (\\texttt{mom_hs}_i) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq}_i) + \\beta_2 (\\texttt{mom_hs}_i) + \\beta_3 (\\texttt{mom_iq}_i \\times \\texttt{mom_hs}_i) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 (\\texttt{mom_iq}_i) + \\beta_2 (\\texttt{mom_hs}_i) + \\beta_3 (\\texttt{mom_iq}_i \\times \\texttt{mom_hs}_i) + \\beta_4 (\\texttt{mom_age}_i) \\end{align*}\\] # mom_iq with centering m1 &lt;- brm(kid_score ~ mom_iq_c, data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302, chains = 2L, cores = 2L ) m1 &lt;- add_criterion(m1, c(&quot;loo&quot;, &quot;waic&quot;)) # Use `update` will sometimes avoid recompiling m2 &lt;- update(m1, kid_score ~ mom_iq_c + mom_hs, newdata = kidiq100) m2 &lt;- add_criterion(m2, c(&quot;loo&quot;, &quot;waic&quot;)) m3 &lt;- update(m2, kid_score ~ mom_iq_c * mom_hs, prior = c(prior(normal(0, 0.5), class = &quot;b&quot;, coef = &quot;mom_iq_c:mom_hsyes&quot;)) ) m3 &lt;- add_criterion(m3, c(&quot;loo&quot;, &quot;waic&quot;)) m4 &lt;- update(m3, kid_score ~ mom_iq_c * mom_hs + mom_age_c, newdata = kidiq100) m4 &lt;- add_criterion(m4, c(&quot;loo&quot;, &quot;waic&quot;)) 9.5.1 Model Weights We have seen that m3 and m4 gave the best LOO-IC and WAIC: loo_compare(m1, m2, m3, m4) &gt;# elpd_diff se_diff &gt;# m3 0.0 0.0 &gt;# m4 -0.6 1.2 &gt;# m1 -6.2 3.9 &gt;# m2 -6.2 3.9 So it makes sense that if we’re to assign weights, m3 should get most weights. Let’s check the following: # Weights based on WAIC waic_wts &lt;- model_weights(m1, m2, m3, m4, weights = &quot;waic&quot;) # Weights based on Pseudo-BMA (with Bayesian bootstrap) pbma_wts &lt;- loo_model_weights(m1, m2, m3, m4, method = &quot;pseudobma&quot;) # Print out the weights round(cbind(waic_wts, pbma_wts), 3) &gt;# waic_wts pbma_wts &gt;# m1 0.001 0.049 &gt;# m2 0.001 0.073 &gt;# m3 0.641 0.522 &gt;# m4 0.356 0.356 You can see m3 would get the highest weight, but it’s only 0.641 and thus less than half of the weights when all four models are considered together. In Bayesian, we want to preserve all the uncertainty in our analyses. Therefore, if we’re not certain which models to use and have tried multiple ones, it would make sense to use all of them to get the best information. So unlike what is commonly done in practice where a researcher would test multiple models and present the best model as if they intended only to test this model, Bayesian analysts should do the honest thing and use all models. The reward is usually better prediction! 9.5.2 Model Averaging I will not go deep into averaging, as there are many ways to do it, but at this moment it requires some programming to perform averaging with STAN and brms. Averaging highlight the Bayesian spirit of incorporating all information for prediction and propagating the uncertainty, which is a key element that unifies a lot of Bayesian methods. In STAN, currently it implements the pseudo-Bayesian Model Averaging (BMA) with Bayesian bootstrap I’ve written a very basic averaging function, bma_brm_lm for brms with linear models. Whereas averaging of predictions can be done for any models, generally it only makes sense to average the coefficients Here is an example of using it for the four models and output posterior draws of the parameters that are weighted averages from the original models. source(&quot;pbma_brm_lm.R&quot;) pbma_draws &lt;- pbma_brm_lm(m1, m2, m3, m4) &gt;# Method: pseudo-BMA+ with Bayesian bootstrap &gt;# ------ &gt;# weight &gt;# m1 0.048 &gt;# m2 0.074 &gt;# m3 0.528 &gt;# m4 0.349 # Coefficients for pseudo-BMA posterior_summary(pbma_draws) &gt;# Estimate Est.Error Q2.5 Q97.5 &gt;# Intercept 0.8442 0.0145 0.81585 0.8718 &gt;# mom_iq_c 0.8791 0.0878 0.71526 1.0589 &gt;# mom_hsyes 0.0311 0.0153 0.00146 0.0612 &gt;# mom_iq_c:mom_hsyes -0.3783 0.0952 -0.56835 -0.1972 &gt;# mom_age_c 0.0120 0.0114 -0.00945 0.0344 # Coefficients for M3 fixef(m3) &gt;# Estimate Est.Error Q2.5 Q97.5 &gt;# Intercept 0.849 0.0213 0.8092 0.8912 &gt;# mom_iq_c 0.914 0.1386 0.6580 1.1873 &gt;# mom_hsyes 0.033 0.0235 -0.0119 0.0763 &gt;# mom_iq_c:mom_hsyes -0.422 0.1524 -0.7179 -0.1390 # Coefficients for M4 fixef(m4) &gt;# Estimate Est.Error Q2.5 Q97.5 &gt;# Intercept 0.8383 0.0246 0.790 0.8860 &gt;# mom_iq_c 0.9312 0.1430 0.662 1.2208 &gt;# mom_hsyes 0.0265 0.0243 -0.022 0.0733 &gt;# mom_age_c 0.0344 0.0325 -0.027 0.0985 &gt;# mom_iq_c:mom_hsyes -0.4448 0.1541 -0.766 -0.1502 As you can see, the coefficients from the pseudo-BMA is smaller (i.e., being shrunk to closer to zero) as compared to m3 and m4. However, we also had a smaller posterior SD of the estimates. Simulation studies have generally shown that the prediction based on BMA tends to outperform many other methods, especially when overfitting is suspected to be a problem. 9.5.3 Stacking Stacking is another way to combine the predictions of different models. The technical details can be found in Yao et al. (2018), but you can obtain the predictions using the pp_average function: # Prediction from stacking by Yao et al. (2018) pred_stacking &lt;- pp_average(m1, m2, m3, m4, method = &quot;predict&quot;) # Prediction from pseudo BMA # 1. Obtain predictions from each model pred_m1234 &lt;- map(list(m1, m2, m3, m4), posterior_predict) # 2. Obtain model weights (pbma_wts as previously obtained) # 3. Obtain weighted predictions pred_pbma &lt;- map2(pred_m1234, pbma_wts, `*`) %&gt;% reduce(`+`) %&gt;% posterior_summary() # Compare the weights ggplot(tibble(stacking = pred_stacking[ , &quot;Estimate&quot;], pbma = pred_pbma[ , &quot;Estimate&quot;]), aes(x = pbma, y = stacking)) + geom_point() + geom_abline(intercept = 0, slope = 1) As can be seen, in this example the two methods give very similar predictions. 9.5.3.1 Prediction example Consider a kid whose mother’s IQ is 120 (mom_iq = .2), mother’s age is 40, (mom_age_c = 2.2), mother does not have a high school degree, and mother did not work in first three years of child’s life (mom_work = 1). Then the prediction based on the various models are: Check out this blog post https://mc-stan.org/loo/articles/loo2-weights.html for more information on stacking and BMA. 9.6 Shrinkage Priors When the number of parameters to be estimated is large relative to the amount of data available, ordinary least square (in frequentist) and estimation using non-informative or weakly informative priors tend to overfit. For example, fitting a 6th degree polynomial (with 8 parameters) on a data set with only 10 observations will severely overfit the data, making the results not generalizable. One way to avoid overfitting is to perform regularization, that is, to shrink some of the parameters to closer to zero. This makes the model fit less well to the existing data, but will be much more generalizable to an independent data set. 9.6.1 Number of parameters In Bayesian analyses, the concept of number of parameters is a little vague. This is because the posterior distribution is a function of both the prior and the data. For non-informative priors, it would make sense to simply count the number of parameters. However, say one put a very strong prior on one of the regression coefficients, which has about 9 times the weights of the information contributed by the data: &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() Then the posterior for the parameter only uses 1/10 of the information from the data! Therefore, it would make more sense to count this as 0.1 parameter, instead of 1 full parameter. The concept of regularization is essentially to introduce a stronger prior so that the posterior is less likely to overfit the data, and the resulting model will have lower effective number of parameters, which, when done appropriately, would find a model that is more likely to generalize to external data sets. In Bayesian methods, regularization can be done by choosing a prior on the coefficient that has a sharp peak at 0, but also has a heavy tail. One such prior is what is called the horseshoe prior. The discussion here is based on the blog pot by Michael Betancourt: https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html It should first be pointed out that these priors were based on the assumption that the predictors and the outcome has been scaled to have a standard deviation of one. So we will do this here: # For variable selection, scale the predictor and outcome to have unit variance kidiq_std &lt;- scale(kidiq) head(kidiq_std) &gt;# kid_score mom_hs mom_iq mom_work mom_age &gt;# [1,] -1.0679 0.522 1.4078 0.9342 1.56 &gt;# [2,] 0.5489 0.522 -0.7092 0.9342 0.82 &gt;# [3,] -0.0881 0.522 1.0295 0.9342 1.56 &gt;# [4,] -0.1860 0.522 -0.0367 0.0878 0.82 &gt;# [5,] 1.3818 0.522 -0.4836 0.9342 1.56 &gt;# [6,] 0.5489 -1.913 0.5268 -1.6051 -1.77 9.6.2 Sparsity-Inducing Priors The horseshoe prior (Carvalho, Polson, and Scott 2009) is a type of hierarchical prior for regression models by introducing a global scale, \\(\\tau\\), and local scale, \\(\\lambda_m\\), parameters on the priors for the regression coefficients. Specifically, with \\(p\\) predictors, \\[\\begin{align*} Y_i &amp; \\sim \\mathcal{N}(\\mu_i, \\sigma^2) \\\\ \\mu_i &amp; = \\beta_0 + \\sum_{m = 1}^p \\beta_m X_m \\\\ \\beta_0 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_m &amp; \\sim \\mathcal{N}(0, \\tau \\lambda_m) \\\\ \\lambda_m &amp; \\sim \\textrm{Cauchy}^+(0, 1) \\\\ \\tau &amp; \\sim \\textrm{Cauchy}^+(0, \\tau_0) \\end{align*}\\] The local scale, \\(\\lambda_m\\), can flexibly shrink the coefficient to close to zero. Below is the implication of the prior on the shrinkage of \\(\\beta\\): &gt;# Warning: Removed 1 row(s) containing missing values (geom_path). The U-shape here means that, for coefficients that are weakly supported by the data, the horseshoe will shrink it to very close to zero, whereas for coefficients that are more strongly supported by the data, the horseshoe will not shrink it much. The red curve in the following is one example for the resulting prior distribution on \\(\\beta\\): dhs &lt;- Vectorize( function(y, df = 1) { ff &lt;- function(lam) dnorm(y, 0, sd = lam) * dt(lam, df) * 2 if (y != 0) integrate(ff, lower = 0, upper = Inf)$value else Inf } ) ggplot(data.frame(x = c(-6, 6)), aes(x = x)) + stat_function(fun = dhs, args = list(df = 3), n = 501, aes(col = &quot;HS&quot;), linetype = 1) + stat_function(fun = dnorm, n = 501, aes(col = &quot;norm&quot;), linetype = 2) + scale_color_manual(&quot;&quot;, values = c(&quot;red&quot;, &quot;black&quot;), labels = c(&quot;horseshoe(3)&quot;, &quot;N(0, 1)&quot;)) + xlab(&quot;y&quot;) + ylab(&quot;density&quot;) + ylim(0, 0.75) &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() Figure 9.4: Density for the Finnish horseshoe prior with 3 degrees of freedom Such a prior has more density at 0, but also more density for extreme values, as compared to a normal distribution. Thus, for coefficients with very weak evidence, the regularizing prior will shrink it to zero, whereas for coefficients with strong evidence, the shrinkage will be very small. This is called a horseshoe prior. In brms, one can specify it with horseshoe(), which is a stabilized version of the original horseshoe prior (Carvalho, Polson, and Scott 2009). 9.6.3 Finnish Horseshoe The Finnish horseshoe (https://projecteuclid.org/euclid.ejs/1513306866) prior is \\[\\begin{align*} \\beta_m &amp; \\sim \\mathcal{N}(0, \\tau \\tilde \\lambda_m) \\\\ \\tilde \\lambda_m &amp; = \\frac{c \\lambda_m}{\\sqrt{c^2 + \\tau^2 \\lambda^2_m}} \\\\ \\lambda_m &amp; \\sim \\textrm{Cauchy}^+(0, 1) \\\\ c^2 &amp; \\sim \\textrm{Inv-Gamma}(\\nu / 2, nu / 2 s^2) \\\\ \\tau &amp; \\sim \\textrm{Cauchy}^+(0, \\tau_0) \\end{align*}\\] The additional parameters are chosen in the code below. First, fit a model without shrinkage: # A model with all main and interaction effects m5 &lt;- brm(kid_score ~ (.)^2, data = kidiq_std, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), iter = 1000, # just to save time chains = 2L, cores = 2L, seed = 2217) # A model with all main and interaction effects m_hs &lt;- brm(kid_score ~ (.)^2, data = kidiq_std, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # Prior guess of 20% of the terms are non-zero prior(horseshoe(par_ratio = 2 / 8), class = &quot;b&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), iter = 1000, # just to save time chains = 2L, cores = 2L, # Need higher adapt_delta control = list(adapt_delta = .99), seed = 2217) We can plot the coefficients: stanplot(m_hs) + # Show the shrinkage as black, transparent dots geom_point(data = posterior_summary(m5) %&gt;% as_tibble(rownames = &quot;parameter&quot;) %&gt;% filter(parameter != &quot;lp__&quot;), aes(x = Estimate, y = parameter), alpha = 0.8) + geom_vline(xintercept = c(-.05, .05), col = &quot;red&quot;) &gt;# Warning: Method &#39;stanplot&#39; is deprecated. Please use &#39;mcmc_plot&#39; instead. An arbitrary cutoff is to select only coefficients with posterior means larger than .05, in which case only mom_iq and mom_hs and their interaction were supported by the data. You can also double check that the regularized version has better LOO-IC: loo(m5, m_hs) &gt;# Output of model &#39;m5&#39;: &gt;# &gt;# Computed from 1000 by 434 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo -567.6 14.4 &gt;# p_loo 13.1 1.4 &gt;# looic 1135.1 28.8 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.1. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Output of model &#39;m_hs&#39;: &gt;# &gt;# Computed from 1000 by 434 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo -565.1 14.5 &gt;# p_loo 8.4 1.0 &gt;# looic 1130.1 28.9 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.1. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Model comparisons: &gt;# elpd_diff se_diff &gt;# m_hs 0.0 0.0 &gt;# m5 -2.5 2.4 And also that the effective number of parameters was smaller in m_hs. 9.7 Variable Selection One way to identify variables that are relevant to predict a certain outcome is to use the projection-based method, as discussed in https://cran.r-project.org/web/packages/projpred/vignettes/quickstart.html and in Piironen and Vehtari (2016). Building from the full model with shrinkage priors, we first identify the importance of various variables in terms of their importance for prediction: library(projpred) &gt;# This is projpred version 1.1.6. # Variable selection: vs &lt;- varsel(m_hs) &gt;# Warning: Method &#39;parse_bf&#39; is deprecated. Please use &#39;brmsterms&#39; instead. &gt;# Warning: posterior_linpred(transform = TRUE) is deprecated. Please use &gt;# posterior_epred() instead, without the &#39;transform&#39; argument. vs$vind # variables ordered as they enter during the search &gt;# mom_iq mom_hs:mom_iq mom_hs:mom_work mom_work:mom_age &gt;# 2 5 6 10 &gt;# mom_hs mom_iq:mom_work mom_hs:mom_age mom_age &gt;# 1 8 7 4 &gt;# mom_iq:mom_age mom_work &gt;# 9 3 # plot predictive performance on training data varsel_plot(vs, stats = c(&quot;elpd&quot;, &quot;rmse&quot;)) We then use the cv_varsel method to perform cross-validation to see how many variables should be included: # With cross-validation cvs &lt;- cv_varsel(m_hs, verbose = FALSE) # not printing progress &gt;# Warning: Method &#39;parse_bf&#39; is deprecated. Please use &#39;brmsterms&#39; instead. &gt;# Warning: posterior_linpred(transform = TRUE) is deprecated. Please use &gt;# posterior_epred() instead, without the &#39;transform&#39; argument. # model size suggested by the program suggest_size(cvs) &gt;# [1] 2 # plot the validation results, this time relative to the full model varsel_plot(cvs, stats = c(&quot;elpd&quot;, &quot;rmse&quot;), deltas = TRUE) Here it suggests to include only mom_iq and its interaction with mom_hs. However, we generally prefers to also include the main effect of mom_hs. 9.7.1 Projection-Based Method The projection-based method will obtain the posterior distributions based on a projection from the full model on the simplified model. In other words, we’re asking the question: If we want a model with only mom_iq, mom_hs, and their interactions in the model, what coefficients should be obtained so that the resulting prediction accuracy is as closed to the full model as possible? Note that the coefficients will be different from if you were to directly estimate the model using the three predictors (i.e., m3). In this case, simulation results showed that the projection-based method will yield a model with better predictive performance. # Fit m3 with the standardized data m3_std &lt;- brm(kid_score ~ mom_hs * mom_iq, data = kidiq_std, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), prior(normal(0, 1), class = &quot;b&quot;), prior(normal(0, 0.5), class = &quot;b&quot;, coef = &quot;mom_hs:mom_iq&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302, chains = 2L, cores = 2L ) # Visualise the projected three most relevant variables proj &lt;- project(vs, vind = vs$vind[c(&quot;mom_iq&quot;, &quot;mom_hs:mom_iq&quot;, &quot;mom_hs&quot;)]) mcmc_intervals(as.matrix(proj)) + # Show the non-projection version as black, transparent dots geom_point(data = fixef(m3_std, pars = c(&quot;Intercept&quot;, &quot;mom_iq&quot;, &quot;mom_hs&quot;, &quot;mom_hs:mom_iq&quot;)) %&gt;% as_tibble(rownames = &quot;parameter&quot;), aes(x = Estimate, y = parameter), alpha = 0.8) &gt;# Warning: Removed 1 rows containing missing values (geom_point). References "],
["hierarchical-multilevel-models.html", "Chapter 10 Hierarchical &amp; Multilevel Models 10.1 ANOVA 10.2 Multilevel Modeling (MLM) 10.3 Varying Coefficients 10.4 Model Comparisons", " Chapter 10 Hierarchical &amp; Multilevel Models In this note we’ll talk about hierarchical models, starting with the Bayesian analogue of ANOVA. While the results of Bayesian regression are usually similar to the frequentist counterparts, at least with weak priors, Bayesian ANOVA is usually represented as a hierarchical model, which corresponds to random-effect ANOVA in frequentist. We’ll then build on that to discuss multilevel regression models with varying intercepts and slopes. 10.1 ANOVA We’ll use some demonstration data that usually corresponds to a typical psychological experiment that uses one-way ANOVA: # From http://personality-project.org/R/datasets/R.appendix1.data alert &lt;- tibble(Dosage = factor(rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 6)), Alertness = c(30, 38, 35, 41, 27, 24, 32, 26, 31, 29, 27, 35, 21, 25, 17, 21, 20, 10)) # Show barplot ggplot(alert, aes(x = Dosage, y = Alertness)) + # Mean + SE stat_summary() &gt;# No summary function supplied, defaulting to `mean_se()` As can be seen, Dosage c has lower mean than others. 10.1.1 “Frequentist” ANOVA In frequentist analyses, we generally first perform an omnibus test: summary(aov(Alertness ~ Dosage, data = alert)) &gt;# Df Sum Sq Mean Sq F value Pr(&gt;F) &gt;# Dosage 2 619 309.5 11.5 0.00094 *** &gt;# Residuals 15 404 26.9 &gt;# --- &gt;# Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And then there will be post hoc comparisons with adjustment on \\(p\\) values # Use Holm&#39;s procedure by default pairwise.t.test(x = alert$Alertness, g = alert$Dosage) &gt;# &gt;# Pairwise comparisons using t tests with pooled SD &gt;# &gt;# data: alert$Alertness and alert$Dosage &gt;# &gt;# a b &gt;# b 0.417 - &gt;# c 0.001 0.005 &gt;# &gt;# P value adjustment method: holm which shows that c was lower than both a and b. 10.1.2 Bayesian ANOVA In Bayesian, it is more common to treat grouping variables, especially with more than three or four categories, as clusters in hierarchical modeling. Specifically, we start with the normal model: \\[\\texttt{Alertness}_{ij} \\sim \\mathcal{N}(\\mu_j, \\sigma)\\] but in the priors, we assume that the \\(\\mu_j\\)s are exchangeable and have a common prior distribution such that \\[\\mu_j \\sim \\mathcal{N}(\\gamma, \\tau)\\] This means that we believe the group means themselves are from a normal distribution with mean \\(\\gamma\\) and SD \\(\\tau\\). \\(\\gamma\\) is the grand mean of Alertness averaged across the conditions, and \\(\\tau\\) is the between-condition SD. They are called hyperparameters, and they also need priors (i.e., hyperpriors). Because the prior for \\(\\mu_j\\) consists of hyperparameters that themselves have prior (hyperprior) distributions, this is also called hierarchical priors. We’ll use: \\[\\begin{align*} \\gamma &amp; \\sim \\mathcal{N}(0, 50) \\\\ \\tau &amp; \\sim \\textrm{Gamma}(2, 1 / 8) \\end{align*}\\] Note that the Gamma prior was recommended in previous papers for hierarchical models, with the 8 in 1/8 being the prior belief of what the maximum value of \\(\\tau\\) can be. m1 &lt;- brm(Alertness ~ 1 + (1 | Dosage), data = alert, prior = c(# for gamma prior(normal(0, 50), class = &quot;Intercept&quot;), # for sigma prior(student_t(4, 0, 10), class = &quot;sigma&quot;), # for tau prior(gamma(2, 0.125), class = &quot;sd&quot;, coef = &quot;Intercept&quot;, group = &quot;Dosage&quot;) ), # Hierarchical models generally require smaller stepsize control = list(adapt_delta = .99)) broom::tidy(m1) %&gt;% knitr::kable() term estimate std.error lower upper b_Intercept 26.37 8.20 13.22 37.49 sd_Dosage__Intercept 11.42 6.79 4.10 24.62 sigma 5.65 1.17 4.11 7.87 r_Dosage[a,Intercept] 5.63 8.32 -5.47 19.22 r_Dosage[b,Intercept] 3.38 8.37 -8.07 17.18 r_Dosage[c,Intercept] -6.71 8.31 -18.71 6.29 lp__ -67.08 2.13 -71.13 -64.33 From the results, the posterior mean for \\(\\gamma\\) is 26.37 (SD = 8.198), which was the grand mean Alertness level. The between-group SD was estimated to be \\(\\tau\\) = 11.416, whereas the within-group SD was estimated to be \\(\\sigma\\) = 5.654. You can get the posterior mean for the mean of each group (i.e., \\(\\mu_j\\)) using coef(m1)$Dosage[ , , &quot;Intercept&quot;] &gt;# Estimate Est.Error Q2.5 Q97.5 &gt;# a 32.0 2.36 27.2 36.6 &gt;# b 29.7 2.29 25.2 34.2 &gt;# c 19.7 2.42 15.0 24.6 10.1.2.1 Shrinkage Note that in the above model, the Bayes estimates of the group means are different from the sample group means, as shown in the following graph: ggplot(alert, aes(x = Dosage, y = Alertness)) + # Mean + SE stat_summary(aes(col = &quot;sample means&quot;)) + geom_pointrange(data = as_tibble(coef(m1)$Dosage[ , , &quot;Intercept&quot;], rownames = &quot;Dosage&quot;), aes(x = Dosage, y = Estimate, ymin = Estimate - Est.Error, ymax = Estimate + Est.Error, col = &quot;Bayes means&quot;), position = position_nudge(x = 0.1)) &gt;# No summary function supplied, defaulting to `mean_se()` If you look more carefully, you can see that the Bayes estimates are closer to the middle. This shrinkage effect may seem odd at first, but it has a good reason. The hierarchical assumes that there are something in common for observations in different groups, so it performs partial pooling by borrowing information from other groups. To illustrate the strength of partial pooling, I went through a thought experiment with my students in my multilevel modeling class. Imagine it’s your first time visiting Macau, my hometown, and you are about to go to a McDonald’s there. You’ve never been to any restaurants in Macau. So what do you expect? You probably will use your experience of eating at McDonald’s in the US as a reference. The Bayesian hierarchical model here is the same: it assumes that even though participants received different Dosage, there are something similar among them, so information from one group should provide some information for another group. And for many of our problems in research, hierarchical models have been shown to make better predictions and inferences, compared to traditional ANOVA. See Kruschke and Liddell (2018) for some more discussion. 10.1.2.2 Notes on multiple comparisons With hierarchical models, the common recommendation is that no further control for multiple comparison is needed (see Gelman, Hill, and Yajima 2012). For one, we don’t use \\(p\\) values in Bayesian. For the other, by shrinking the group means closer to the grand mean in a hierarchical model, the comparisons in some sense have already been adjusted. You can plot the estimated group means by: mcmc_intervals(coef(m1, summary = FALSE)$Dosage[ , , &quot;Intercept&quot;]) And below it shows the posterior of the differences: ranef_draws &lt;- coef(m1, summary = FALSE)$Dosage[ , , &quot;Intercept&quot;] # Find all comparisons: m1_cont &lt;- combn(colnames(ranef_draws), 2, simplify = FALSE) # Compute mean differences m1_cont_draws &lt;- map_dfc(m1_cont, ~ tibble(ranef_draws[, .x[1]] - ranef_draws[, .x[2]]) %&gt;% `names&lt;-`(paste(.x[1], .x[2], sep = &quot;-&quot;))) # Plot the contrasts mcmc_areas(m1_cont_draws, prob = .95, bw = &quot;SJ&quot;) And the results in this example are similar to the post hoc comparisons. 10.2 Multilevel Modeling (MLM) Multilevel modeling is the set of techniques that built on the previous hierarchical model. It is proposed kind of separately in multiple disciplines, including education and other social sciences, and so historically it has been referred to by many different names, such as: Mixed/Mixed-effect models Hierarchical linear models Variance component models It allows us to build models on different groups/clusters, and allows the parameters to be different across clusters. However, it does partial pooling by borrowing information from one cluster to another, which is especially beneficial when some groups have only a few people, where borrowing information from other clusters would help stabilize the parameter estimates. 10.2.1 Examples of clustering There are many different forms of clustering in data across different disciplines. We’ve seen the example of people clustered in experimental conditions. Other examples include: Students in schools Clients nested within therapists within clinics Employees nested within organizations Citizens nested within employees Repeated measures nested within persons They can be represented in network graphs like the following (students within schools): Sometimes there are more than one level of clustering, like students clustered by both middle schools and high schools. This is called a crossed structure as shown in the following, where we say that students are cross-classified by both middle and high schools. Another example commonly happened in psychological experiments is when participants see multiple stimuli, each as an item, so the observations are cross-classified by both persons and items. The repeated measures nested within persons one is particularly relevant as that means essentially all longitudinal data are multilevel data and should be modelled accordingly. It allows one to build individualized model to look at within-person changes, as well as between-person differences of those changes. Techniques such as dependent-sample \\(t\\)-test, repeated-measures ANOVA, growth curve modeling, and time-series analyses, can all be represented in the multilevel modeling framework. Therefore, some authors, such as McElreath (2016), would suggest that MLM should be the default model that we use for analyses, rather than regression. 10.2.2 Data We will use the data set sleepstudy from the lme4 package, which is the package for frequentist multilevel modeling. The data set contains 18 participants, each with 10 observations. It examines the change in average reaction time per day with increasing sleep deprivation. See ?lme4::sleepstudy for more of the description. Here is a plot of the data: data(sleepstudy, package = &quot;lme4&quot;) # call the data psych::pairs.panels(sleepstudy[1:2]) This data set has clustering because it is repeated measures nested within persons. It is more useful to plot the change in the outcome: ggplot(sleepstudy, aes(x = Days, y = Reaction)) + geom_point(size = 0.5) + geom_smooth() + # presented by person facet_wrap(~ Subject, ncol = 6) &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; As you can see, most people experience increases in reaction time, although there are certainly differences across individuals. 10.2.3 Intraclass correlation With multilevel data, the first question to ask is how much variation in the outcome is there at each level. This is quantified by the intraclass correlation, which, for a two-level model, is defined by \\[\\rho = \\frac{\\tau^2}{\\tau^2 + \\sigma^2}\\] where \\(\\tau\\) is the between-level SD, which is the SD of the cluster means (i.e., the variability of mean response time across persons in this example), and \\(\\sigma\\) is the within-level SD (i.e., variability within a person, which is assumed constant across persons). The ICC represents the proportion of variance of the outcome that are due to between-level (e.g., between-group, between-person) differences Here is a graph from my MLM class showing how the data would be like with different ICC levels: As you can see, the higher the ICC, the higher the variations in the cluster means, relative to the within-cluster variations. Below is the graph for the sleepstudy data: ggplot(sleepstudy, aes(x = Subject, y = Reaction)) + geom_jitter(width = 0.1, col = &quot;darkgrey&quot;) + stat_summary(geom = &quot;point&quot;, fun.y = mean, size = 4, shape = 24, fill = &quot;red&quot;) &gt;# Warning: `fun.y` is deprecated. Use `fun` instead. Which has substantial between-person variations. 10.2.3.1 Computing ICC To compute the ICC, we need to first fit a multilevel model, which in this case is the varying intercept model: \\[\\begin{align*} \\texttt{Reaction}_{ij} &amp; \\sim \\mathcal{N}(\\mu_j, \\sigma) \\\\ \\mu_j &amp; \\sim \\mathcal{N}(\\gamma, \\tau) \\end{align*}\\] where \\(\\mu_j\\) is the mean reaction for the \\(j\\)th person, and \\(i\\) indexes measurement occasions. We’ll rescale Reaction by 10: sleepstudy &lt;- sleepstudy %&gt;% mutate(Reaction10 = Reaction / 10) To use weakly informative priors, we will set \\[\\begin{align*} \\gamma &amp; \\sim \\mathcal{N}(0, 50) \\\\ \\sigma &amp; \\sim t^+(4, 0, 5) \\\\ \\tau &amp; \\sim \\textrm{Gamma}(2, 1 / 5) \\end{align*}\\] m2 &lt;- brm(Reaction10 ~ (1 | Subject), data = sleepstudy, prior = c(# for intercept prior(normal(0, 50), class = &quot;Intercept&quot;), # for tau prior(gamma(2, 0.2), class = &quot;sd&quot;), # for sigma prior(student_t(4, 0, 5), class = &quot;sigma&quot;)), control = list(adapt_delta = .95), cores = 2L, seed = 2107) Now use the posterior draws of \\(\\tau\\) and \\(\\sigma\\) to compute the posterior for the ICC: # Computing ICC # 1. Obtain posterior draws of tau and sigma sd_m2 &lt;- VarCorr(m2, summary = FALSE) draws_tau &lt;- sd_m2$Subject$sd[ , &quot;Intercept&quot;] # tau draws_sigma &lt;- sd_m2$residual__$sd[ , 1] #sigma # 2. Compute draws for ICC draws_icc &lt;- draws_tau^2 / (draws_tau^2 + draws_sigma^2) # Plot the ICC qplot(draws_icc, geom = &quot;density&quot;, xlab = &quot;ICC&quot;, bw = &quot;SJ&quot;) # Summarize the ICC distribution psych::describe(draws_icc) &gt;# vars n mean sd median trimmed mad min max range skew kurtosis se &gt;# X1 1 4000 0.42 0.1 0.42 0.42 0.1 0.13 0.78 0.65 0.24 -0.19 0 10.2.3.2 Interpretations broom::tidy(m2, parameters = c(&quot;b_Intercept&quot;, &quot;sd&quot;, &quot;sigma&quot;)) %&gt;% knitr::kable() term estimate std.error lower upper b_Intercept 29.86 0.973 28.25 31.51 sd_Subject__Intercept 3.88 0.824 2.71 5.38 sigma 4.45 0.253 4.06 4.89 The model suggested that the average reaction time across individuals and measurement occasions was 298.629 ms, 95% CI [279.936, 317.770]. It was estimated that 42.477%, 95% CI [23.954%, 63.417%] of the variations in reaction time was attributed to between-person differences. 10.2.4 Is MLM needed? This is a commonly asked question. Based on Lai and Kwok (2015), you can compute the design effect index, which shows the inflation in variability of the estimates due to clustering. It is recommended to account for clustering if the design effect is larger than 1.1. It is defined as: \\[\\mathit{Deff}= 1 + (n - 1) \\rho\\] where \\(n\\) is the (average) number of observations in each cluster, and in our case it is 10. Therefore, the design effect in sleepstudy for Reaction is \\[\\mathit{Deff}= 1 + (10 - 1) (0.425)\\] which is 4.823, so we do need to account for the clustering. 10.3 Varying Coefficients The strength of a multilevel model is that it can allow researchers to build models that allow for cluster-specific coefficients. In our example data this is analogous to fitting separate models for each person, but instead of only using 10 data points for each model, MLM pools information from other people as it believes that we can learn something about one person by looking at data from other people. For example, for each person, we’ll fit a regression model using Days to predict Reaction10. Using our previous notations, \\[\\begin{align} \\texttt{Reaction10}_i &amp; \\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 \\texttt{Days}_i \\end{align}\\] However, because we have more than one person, we’ll use the subscript \\(j\\) to denote the person, so that the model becomes \\[\\begin{align} \\texttt{Reaction10}_{ij} &amp; \\sim \\mathcal{N}(\\mu_{ij}, \\sigma_j) \\\\ \\mu_{ij} &amp; = \\beta_{0j} + \\beta_{1j} \\texttt{Days}_{ij} \\end{align}\\] which suggests that all three of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\) can be different across persons. We’ll first start with varying \\(\\beta_0\\), or varying intercepts. 10.3.1 Varying Intercepts With varying intercepts model, we assumed that only \\(\\beta_0\\) is different across persons, but \\(\\beta_1\\) and \\(\\sigma\\) are common parameters that do not change across persons. This is also referred to as a random intercept model in (frequentist) MLM literature. Specifically, the model and priors are: \\[\\begin{align} \\text{Repeated-measure level:} \\\\ \\texttt{Reaction10}_{ij} &amp; \\sim \\mathcal{N}(\\mu_{ij}, \\sigma) \\\\ \\mu_{ij} &amp; = \\beta_{0j} + \\beta_{1} \\texttt{Days}_{ij} \\\\ \\text{Person level:} \\\\ \\beta_{0j} &amp; \\sim \\mathcal{N}(\\mu^{[\\beta_0]}, \\tau^{[\\beta_0]}) \\\\ \\text{Priors:} \\\\ \\mu^{[\\beta_0]} &amp; \\sim \\mathcal{N}(0, 50) \\\\ \\tau^{[\\beta_0]} &amp; \\sim \\mathrm{Gamma}(2, 0.2) \\\\ \\beta_1 &amp; \\sim \\mathcal{N}(0, 10) \\\\ \\sigma &amp; \\sim t^+(4, 0, 5) \\end{align}\\] where the \\(\\beta_{0j}\\)s follow a common normal distribution with hyperparameters \\(\\mu^{[\\beta_0]}\\) and \\(\\tau^{[\\beta_0]}\\). Thus, \\(\\mu^{[\\beta_0]}\\) is the grand intercept, or the average intercept across persons, and \\(\\tau^{[\\beta_0]}\\) is the SD of those intercepts. The model can be fitted in brms: m3 &lt;- brm(Reaction10 ~ Days + (1 | Subject), data = sleepstudy, prior = c(# for intercept prior(normal(0, 50), class = &quot;Intercept&quot;), # for slope prior(normal(0, 10), class = &quot;b&quot;), # for tau prior(gamma(2, 0.2), class = &quot;sd&quot;), # for sigma prior(student_t(4, 0, 5), class = &quot;sigma&quot;)), control = list(adapt_delta = .95), cores = 2L, seed = 2107) Below is a summary table of the results broom::tidy(m3, parameters = c(&quot;b_&quot;, &quot;sd&quot;, &quot;sigma&quot;)) %&gt;% knitr::kable() term estimate std.error lower upper b_Intercept 25.18 1.052 23.433 26.95 b_Days 1.05 0.080 0.915 1.18 sd_Subject__Intercept 4.03 0.772 2.945 5.46 sigma 3.12 0.176 2.854 3.43 Let’s check the fit of the model to the data, first to the overall data and then to each individual specifically. 10.3.1.1 Fit of Overall data # Posterior mean of slope coef_post &lt;- fixef(m3, summary = FALSE) ggplot(sleepstudy, aes(x = Days, y = Reaction10)) + geom_jitter(size = 0.5, width = 0.1) + geom_abline(data = as_tibble(coef_post), aes(intercept = Intercept, slope = Days), color = &quot;skyblue&quot;, size = 0.2, alpha = 0.01) + geom_smooth(se = FALSE, col = &quot;red&quot;) &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; As can be seen, the estimated coefficient for Days, which was assumed constant for everyone, fit the overall data. However, does it fit each individual? 10.3.1.2 Fit of Individuals # Posterior mean of slope coef_post &lt;- coef(m3, summary = FALSE) df_lines &lt;- tibble(Subject = colnames(coef_post$Subject), Intercept = colMeans(coef_post$Subject[ , , &quot;Intercept&quot;]), Days = colMeans(coef_post$Subject[ , , &quot;Days&quot;])) ggplot(sleepstudy, aes(x = Days, y = Reaction10)) + geom_point(size = 0.5) + geom_abline(data = df_lines, aes(intercept = Intercept, slope = Days), color = &quot;blue&quot;, size = 0.8, alpha = 0.5) + geom_smooth(se = FALSE, col = &quot;red&quot;, size = 0.8, alpha = 0.5) + # Uncomment the following to show the uncertainty on the line # geom_abline(data = as_tibble(coef_post), # aes(intercept = Intercept, slope = Days), # color = &quot;skyblue&quot;, size = 0.2, alpha = 0.01) + # presented by person facet_wrap(~ Subject, ncol = 6) &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Obviously it only fit a few individuals, but not all. So let’s also allow \\(\\beta_1\\) to vary. 10.3.2 Varying Slopes We’ll now also allow \\(\\beta_1\\) to vary across clusters, with the following model: \\[\\begin{align} \\text{Repeated-measure level:} \\\\ \\texttt{Reaction10}_{ij} &amp; \\sim \\mathcal{N}(\\mu_{ij}, \\sigma) \\\\ \\mu_{ij} &amp; = \\beta_{0j} + \\beta_{1j} \\texttt{Days}_{ij} \\\\ \\text{Person level:} \\\\ \\begin{bmatrix} \\beta_{0j} \\\\ \\beta_{1j} \\\\ \\end{bmatrix} &amp; \\sim \\mathcal{N}_2\\left( \\begin{bmatrix} \\mu^{[\\beta_0]} \\\\ \\mu^{[\\beta_1]} \\\\ \\end{bmatrix}, \\boldsymbol{\\mathbf{T}} \\right) \\end{align}\\] where \\[\\boldsymbol{\\mathbf{T}} = \\begin{bmatrix} {\\tau^{[\\beta_0]}}^2 &amp; \\\\ \\tau^{\\beta{10}} &amp; {\\tau^{[\\beta_1]}}^2 \\\\ \\end{bmatrix}\\] Note that \\(\\mathcal{N}_2\\) denotes a bivariate normal (i.e., 2-dimensional multivariate normal) distribution, because now we can talk about how \\(\\beta_0\\) and \\(\\beta_1\\) are associated at the person level. Generally I don’t interpret the covariance between them because it largely depends on how the variables were centered, but nevertheless we should allow them to be correlated. The parameter \\(\\tau^{\\beta{10}}\\) thus denotes the covariance of them. Programs using Gibbs sampling, such as MCMCglmm, uses an inverse-Wishart distribution as a prior for the covariance matrix \\(\\boldsymbol{\\mathbf{T}}\\), but it has been shown to usually leading to biased and inefficient estimates. More recent recommendation is to decompose \\(\\boldsymbol{\\mathbf{T}}\\) into a correlation matrix and the scaling matrices, and use an LKJ prior on the correlation matrix. We’ll explain the LKJ prior below, but first let’s do the decomposition: \\[\\boldsymbol{\\mathbf{T}} = \\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}) \\boldsymbol{\\mathbf{\\Omega }}\\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}),\\] where \\(\\boldsymbol{\\mathbf{T}}\\) = \\([\\tau_1, \\tau_2, \\ldots]\\) is a vector containing the scale parameters (i.e., SD) of the varying coefficients, and \\(\\boldsymbol{\\mathbf{\\Omega}}\\) is the correlation matrix of the varying coefficients. 10.3.2.1 LKJ Prior The LKJ Prior is a probability distribution for correlation matrices. A correlation matrix has 1 on all the diagonal elements. For example, a 2 \\(\\times\\) 2 correlation matrix is \\[\\begin{bmatrix} 1 &amp; \\\\ 0.35 &amp; 1 \\end{bmatrix}\\] where the correlation is 0.35. Therefore, with two variables, there is one correlation; with three or more variables, the number of correlations will be \\(q (q - 1) / 2\\), where \\(q\\) is the number of variables. For a correlation matrix of a given size, the LKJ prior has one shape parameter, \\(\\eta\\), where \\(\\eta = 1\\) corresponds to a uniform distribution of the correlations such that any correlations are equally likely, \\(\\eta \\geq 1\\) favors a matrix closer to an identity matrix so that the correlations are closer to zero, and \\(\\eta \\leq 1\\) favors a matrix with larger correlations. For a 2 \\(\\times\\) 2 matrix, the distribution of the correlation, \\(\\rho\\), with different \\(\\eta\\) values are shown in the graph below: dlkjcorr2 &lt;- function(rho, eta = 1, log = FALSE) { # Function to compute the LKJ density given a correlation out &lt;- (eta - 1) * log(1 - rho^2) - 1 / 2 * log(pi) - lgamma(eta) + lgamma(eta + 1 / 2) if (!log) out &lt;- exp(out) out } ggplot(tibble(rho = c(-1, 1)), aes(x = rho)) + stat_function(fun = dlkjcorr2, args = list(eta = 0.1), aes(col = &quot;0.1&quot;), n = 501) + stat_function(fun = dlkjcorr2, args = list(eta = 0.5), aes(col = &quot;0.5&quot;), n = 501) + stat_function(fun = dlkjcorr2, args = list(eta = 1), aes(col = &quot;1&quot;), n = 501) + stat_function(fun = dlkjcorr2, args = list(eta = 2), aes(col = &quot;2&quot;), n = 501) + stat_function(fun = dlkjcorr2, args = list(eta = 10), aes(col = &quot;10&quot;), n = 501) + stat_function(fun = dlkjcorr2, args = list(eta = 100), aes(col = &quot;100&quot;), n = 501) + labs(col = expression(eta), x = expression(rho), y = &quot;Density&quot;) &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: `mapping` is not used by stat_function() &gt;# Warning: Removed 2 row(s) containing missing values (geom_path). As you can see, when \\(\\eta\\) increases, the correlation is more concentrated to zero. The default in brms is to use \\(\\eta\\) = 1, which is non-informative. If you have a weak but informative belief that the correlations shouldn’t be very large, using \\(\\eta\\) = 2 is reasonable. The resulting model and priors are: \\[\\begin{align} \\text{Repeated-measure level:} \\\\ \\texttt{Reaction10}_{ij} &amp; \\sim \\mathcal{N}(\\mu_{ij}, \\sigma) \\\\ \\mu_{ij} &amp; = \\beta_{0j} + \\beta_{1j} \\texttt{Days}_{ij} \\\\ \\text{Person level:} \\\\ \\begin{bmatrix} \\beta_{0j} \\\\ \\beta_{1j} \\\\ \\end{bmatrix} &amp; \\sim \\mathcal{N}_2\\left( \\begin{bmatrix} \\mu^{[\\beta_0]} \\\\ \\mu^{[\\beta_1]} \\\\ \\end{bmatrix}, \\boldsymbol{\\mathbf{T}} \\right) \\\\ \\boldsymbol{\\mathbf{T}} &amp; = \\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}) \\boldsymbol{\\mathbf{\\Omega }}\\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}) \\\\ \\text{Priors:} \\\\ \\mu^{[\\beta_0]} &amp; \\sim \\mathcal{N}(0, 50) \\\\ \\mu^{[\\beta_1]} &amp; \\sim \\mathcal{N}(0, 10) \\\\ \\tau^{[\\beta_m]} &amp; \\sim \\mathrm{Gamma}(2, 0.2), \\; m = 0, 1 \\\\ \\boldsymbol{\\mathbf{\\Omega }}&amp; \\sim \\mathrm{LKJ}(1) \\\\ \\sigma &amp; \\sim t^+(4, 0, 5) \\end{align}\\] m4 &lt;- brm(Reaction10 ~ Days + (Days | Subject), data = sleepstudy, prior = c(# for intercept prior(normal(0, 50), class = &quot;Intercept&quot;), # for slope prior(normal(0, 10), class = &quot;b&quot;), # for tau_beta0 and tau_beta1 prior(gamma(2, 0.2), class = &quot;sd&quot;, group = &quot;Subject&quot;), # for correlation prior(lkj(1), class = &quot;cor&quot;), # for sigma prior(student_t(4, 0, 5), class = &quot;sigma&quot;)), control = list(adapt_delta = .95), cores = 2L, seed = 2107) Below is a summary table of the results broom::tidy(m4, parameters = c(&quot;b_&quot;, &quot;sd&quot;, &quot;sigma&quot;)) %&gt;% knitr::kable() term estimate std.error lower upper b_Intercept 25.135 0.798 23.811 26.428 b_Days 1.048 0.179 0.754 1.341 sd_Subject__Intercept 2.865 0.752 1.823 4.252 sd_Subject__Days 0.693 0.163 0.467 0.991 sigma 2.583 0.151 2.350 2.846 10.3.2.2 Fit of Individuals # Posterior mean of slope coef_post &lt;- coef(m4, summary = FALSE) df_lines &lt;- tibble(Subject = colnames(coef_post$Subject), Intercept = colMeans(coef_post$Subject[ , , &quot;Intercept&quot;]), Days = colMeans(coef_post$Subject[ , , &quot;Days&quot;])) ggplot(sleepstudy, aes(x = Days, y = Reaction10)) + geom_point(size = 0.5) + geom_abline(data = df_lines, aes(intercept = Intercept, slope = Days), color = &quot;blue&quot;, size = 0.8, alpha = 0.5) + geom_smooth(se = FALSE, col = &quot;red&quot;, size = 0.8, alpha = 0.5) + # presented by person facet_wrap(~ Subject, ncol = 6) &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; You can see that the fit is better. You can also visualize the varying regression lines: ggplot(sleepstudy, aes(x = Days, y = Reaction10, col = Subject)) + geom_jitter(size = 0.5, width = 0.1) + geom_abline(data = df_lines, aes(intercept = Intercept, slope = Days, col = Subject), size = 0.8, alpha = 0.5) + # Suppress legend guides(col = FALSE) Or using the sjPlot package: sjPlot::plot_model(m4, type = &quot;pred&quot;, # Put in the predictor first, and then grouping variable terms = c(&quot;Days&quot;, &quot;Subject&quot;), pred.type = &quot;re&quot;, # Grayscale color colors = &quot;gs&quot;) + guides(col = FALSE) &gt;# Warning: posterior_linpred(transform = TRUE) is deprecated. Please use &gt;# posterior_epred() instead, without the &#39;transform&#39; argument. &gt;# Note: uncertainty of error terms are not taken into account. You may want to use `rstantools::posterior_predict()`. 10.3.2.3 Fixed Effect Model You can compare the previous model with one where have different slopes for different person, which can be modelled by including an interaction with the categorical Subject predictor. This is referred to as the fixed-effect model, as opposed to random-effect model used to describe hierarchical models with partial pooling. Below is an example: m4_fixed &lt;- brm(Reaction10 ~ Days * I(factor(Subject)), data = sleepstudy, prior = c(# for intercept prior(normal(0, 50), class = &quot;Intercept&quot;), # for slope prior(normal(0, 10), class = &quot;b&quot;), # for sigma prior(student_t(4, 0, 5), class = &quot;sigma&quot;)), control = list(adapt_delta = .95), cores = 2L, seed = 2107) You can compare the two models using LOO-IC: loo(m4, m4_fixed) &gt;# Warning: Found 3 observations with a pareto_k &gt; 0.7 in model &#39;m4&#39;. It is &gt;# recommended to set &#39;reloo = TRUE&#39; in order to calculate the ELPD without the &gt;# assumption that these observations are negligible. This will refit the model 3 &gt;# times to compute the ELPDs for the problematic observations directly. &gt;# Warning: Found 4 observations with a pareto_k &gt; 0.7 in model &#39;m4_fixed&#39;. It is &gt;# recommended to set &#39;reloo = TRUE&#39; in order to calculate the ELPD without the &gt;# assumption that these observations are negligible. This will refit the model 4 &gt;# times to compute the ELPDs for the problematic observations directly. &gt;# Output of model &#39;m4&#39;: &gt;# &gt;# Computed from 4000 by 180 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo -446.3 22.5 &gt;# p_loo 34.2 8.4 &gt;# looic 892.5 44.9 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is NA. &gt;# &gt;# Pareto k diagnostic values: &gt;# Count Pct. Min. n_eff &gt;# (-Inf, 0.5] (good) 175 97.2% 899 &gt;# (0.5, 0.7] (ok) 2 1.1% 493 &gt;# (0.7, 1] (bad) 2 1.1% 31 &gt;# (1, Inf) (very bad) 1 0.6% 15 &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Output of model &#39;m4_fixed&#39;: &gt;# &gt;# Computed from 4000 by 180 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo -450.0 23.7 &gt;# p_loo 40.1 9.8 &gt;# looic 900.1 47.5 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is NA. &gt;# &gt;# Pareto k diagnostic values: &gt;# Count Pct. Min. n_eff &gt;# (-Inf, 0.5] (good) 165 91.7% 251 &gt;# (0.5, 0.7] (ok) 11 6.1% 210 &gt;# (0.7, 1] (bad) 1 0.6% 721 &gt;# (1, Inf) (very bad) 3 1.7% 5 &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Model comparisons: &gt;# elpd_diff se_diff &gt;# m4 0.0 0.0 &gt;# m4_fixed -3.8 2.6 As you can see, in this case the hierarchical approach yields a lower LOO (but there was a warning message, so be careful), and estimated less number of parameters. With more clusters and with lower ICC, hierarchical models will have even stronger advantage. So far we have not talked about including person-level predictors. If there are such predictors available, such as gender, we can use those to predict individual differences in intercepts (main effect) and in slopes (i.e., interaction with Days). Just add those predictors to the model by: \\[\\begin{align} \\begin{bmatrix} \\beta_{0j} \\\\ \\beta_{1j} \\\\ \\end{bmatrix} &amp; \\sim \\mathcal{N}_2\\left( \\begin{bmatrix} \\mu_{{\\beta_0}j} \\\\ \\mu_{{\\beta_1}j} \\\\ \\end{bmatrix}, \\boldsymbol{\\mathbf{T}} \\right) \\\\ \\boldsymbol{\\mathbf{T}} &amp; = \\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}) \\boldsymbol{\\mathbf{\\Omega }}\\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}) \\\\ \\mu_{{\\beta_0}j} &amp; = \\gamma_{00} + \\gamma_{01} X_j \\\\ \\mu_{{\\beta_1}j} &amp; = \\gamma_{10} + \\gamma_{11} X_j \\end{align}\\] where \\(X_j\\) is a person-level predictor. 10.3.2.4 Interpretations tau_m4 &lt;- VarCorr(m4)$Subject$sd Based on the model, at Day 0, the average reaction time across individuals was 251.348 ms, 95% CI [235.577, 267.109], and the SD at Day 0 was 28.646ms, 95% CI [16.698ms, 45.883ms]. The average growth rate per day in reaction time across individuals was 10.477 ms, 95% CI [6.938, 14.178], and the SD at Day 0 was 6.93ms, 95% CI [4.371ms, 10.886ms], as shown in the figure. 10.3.3 Varying \\(\\sigma\\) Finally, you can also allow \\(\\sigma\\) to be different across individuals. This is typically used to relax the homogeneity of variance assumption, but recently there is also some interest in treating varying \\(\\sigma\\) as an important outcome. Examples include fluctuations in mood, as two people with the same mean level of mood may fluctuate very differently, and mood swing can be an important outcome to assess. There has been some interesting applications in health research using ecological momentary assessment data. For an overview, see the paper by Hedeker, Mermelstein, and Demirtas (2008). Without going into the details, here is the model and the priors: \\[\\begin{align} \\text{Repeated-measure level:} \\\\ \\texttt{Reaction10}_{ij} &amp; \\sim \\mathcal{N}(\\mu_{ij}, \\sigma_j) \\\\ \\mu_{ij} &amp; = \\beta_{0j} + \\beta_{1j} \\texttt{Days}_{ij} \\\\ \\text{Person level:} \\\\ \\begin{bmatrix} \\beta_{0j} \\\\ \\beta_{1j} \\\\ \\log(\\sigma_j) \\end{bmatrix} &amp; \\sim \\mathcal{N}_2\\left( \\begin{bmatrix} \\mu^{[\\beta_0]} \\\\ \\mu^{[\\beta_1]} \\\\ \\mu^{[s]} \\end{bmatrix}, \\boldsymbol{\\mathbf{T}} \\right) \\\\ \\boldsymbol{\\mathbf{T}} &amp; = \\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}) \\boldsymbol{\\mathbf{\\Omega }}\\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}) \\\\ \\text{Priors:} \\\\ \\mu^{[\\beta_0]} &amp; \\sim \\mathcal{N}(0, 50) \\\\ \\mu^{[\\beta_1]} &amp; \\sim \\mathcal{N}(0, 10) \\\\ \\mu^{[s]} &amp; \\sim t^+(4, 0, 1.6) \\\\ \\tau^{[\\beta_m]} &amp; \\sim \\mathrm{Gamma}(2, 0.2), \\; m = 0, 1 \\\\ \\tau^{[s]} &amp; \\sim \\mathrm{Gamma}(2, 0.625) \\\\ \\boldsymbol{\\mathbf{\\Omega }}&amp; \\sim \\mathrm{LKJ}(1) \\end{align}\\] # Use |c| to estimate the covariance between the sigma and beta random effects m5 &lt;- brm(bf(Reaction10 ~ Days + (Days |c| Subject), sigma ~ (1 |c| Subject)), data = sleepstudy, prior = c(# for intercept prior(normal(0, 50), class = &quot;Intercept&quot;), # for slope prior(normal(0, 10), class = &quot;b&quot;), # for tau_beta0 prior(gamma(2, 0.2), class = &quot;sd&quot;, coef = &quot;Intercept&quot;, group = &quot;Subject&quot;), # for tau_beta1 prior(gamma(2, 0.2), class = &quot;sd&quot;, coef = &quot;Days&quot;, group = &quot;Subject&quot;), # for correlation prior(lkj(1), class = &quot;cor&quot;), # for sigma prior(student_t(4, 0, 1.6), class = &quot;Intercept&quot;, dpar = &quot;sigma&quot;), # for tau_sigma prior(gamma(2, 0.625), class = &quot;sd&quot;, coef = &quot;Intercept&quot;, group = &quot;Subject&quot;, dpar = &quot;sigma&quot;)), control = list(adapt_delta = .95), cores = 2L, seed = 2107) Below is a summary table of the results broom::tidy(m5, parameters = c(&quot;b_&quot;, &quot;sd_Subject&quot;, &quot;cor&quot;)) %&gt;% knitr::kable() term estimate std.error lower upper b_Intercept 25.158 0.838 23.816 26.581 b_sigma_Intercept 0.727 0.137 0.504 0.949 b_Days 1.037 0.186 0.741 1.346 sd_Subject__Intercept 3.140 0.695 2.183 4.418 sd_Subject__Days 0.704 0.159 0.490 0.993 sd_Subject__sigma_Intercept 0.507 0.123 0.338 0.737 cor_Subject__Intercept__Days 0.004 0.270 -0.438 0.451 cor_Subject__Intercept__sigma_Intercept 0.251 0.297 -0.284 0.697 cor_Subject__Days__sigma_Intercept 0.438 0.255 -0.032 0.794 And the posterior predictive check: pp_check(m5, type = &quot;ribbon_grouped&quot;, group = &quot;Subject&quot;, facet_args = list(ncol = 6)) &gt;# Using all posterior samples for ppc type &#39;ribbon_grouped&#39; by default. 10.4 Model Comparisons We can compare the previous models from m3 to m5, with m3 being least complex and m5 being most complex. However, it should be noted that, because of the way how STAN computes LOOIC and WAIC, The LOOIC and WAIC computed in STAN (including brms) generally cannot be used to compare models with different level-2 predictors. The problem is illustrated in this blog post: https://deepthoughtsandsilliness.blogspot.com/2007/12/focus-on-dic.html in the context of DIC. Here’s the table for the several models: source(&quot;extract_brmsfit.R&quot;) ext_m3 &lt;- extract_brmsfit(m3) ext_m3@gof.names[1] &lt;- &quot;SD(Intercept): Subject&quot; ext_m4 &lt;- extract_brmsfit(m4) &gt;# Warning: Found 3 observations with a pareto_k &gt; 0.7 in model &#39;model&#39;. It is &gt;# recommended to set &#39;reloo = TRUE&#39; in order to calculate the ELPD without the &gt;# assumption that these observations are negligible. This will refit the model 3 &gt;# times to compute the ELPDs for the problematic observations directly. ext_m4@gof.names[1:3] &lt;- c(&quot;SD(Intercept): Subject&quot;, &quot;SD(Days): Subject&quot;, &quot;Cor(Intercept,Days): Subject&quot;) ext_m5 &lt;- extract_brmsfit(m5) &gt;# Warning: Found 13 observations with a pareto_k &gt; 0.7 in model &#39;model&#39;. With this &gt;# many problematic observations, it may be more appropriate to use &#39;kfold&#39; with &gt;# argument &#39;K = 10&#39; to perform 10-fold cross-validation rather than LOO. ext_m5@gof.names[1:6] &lt;- c(&quot;SD(Intercept): Subject&quot;, &quot;SD(Days): Subject&quot;, &quot;SD(log[sigma]): Subject&quot;, &quot;Cor(Intercept,Days): Subject&quot;, &quot;Cor(Intercept,log[sigma]): Subject&quot;, &quot;Cor(Days,log[sigma]): Subject&quot;) texreg::htmlreg(list(ext_m3, ext_m4, ext_m5), custom.model.names = c(&quot;Varying Intercepts + Days&quot;, &quot;Varying Intercepts and Slopes&quot;, &quot;Varying Variances&quot;), reorder.gof = c(1, 6, 8, 7, 9, 10, 2:5), doctype = FALSE) Statistical models   Varying Intercepts + Days Varying Intercepts and Slopes Varying Variances Intercept 25.18* 25.13* 25.16*   [23.12; 27.30] [23.59; 26.72] [23.57; 26.86] Days 1.05* 1.05* 1.04*   [ 0.90; 1.21] [ 0.66; 1.38] [ 0.68; 1.42] sigma_Intercept     0.73*       [ 0.47; 1.00] SD(Intercept): Subject 4.03 2.86 3.14 SD(Days): Subject   0.69 0.70 SD(log[sigma]): Subject     0.51 Cor(Intercept,Days): Subject   0.05 0.00 Cor(Intercept,log[sigma]): Subject     0.25 Cor(Days,log[sigma]): Subject     0.44 R2 0.70 0.79 0.80 Num. obs. 180 180 180 loo IC 941.06 892.55 838.70 WAIC 940.73 889.81 827.78 * Null hypothesis value outside the confidence interval. As can be seen, the last model had the best predictive performance. References "],
["generalized-linear-models.html", "Chapter 11 Generalized Linear Models 11.1 Basics of Generalized Linear Models 11.2 Binary Logistic Regression 11.3 Binomial Logistic Regression 11.4 Probit Regression 11.5 Poisson Regression", " Chapter 11 Generalized Linear Models GLM (generalized linear model) is a generalization of the linear model (e.g., multiple regression) we discussed a few weeks ago. Just to be careful, some scholars also use the abbreviation GLM to mean the general linear model, which is actually the same as the linear model we discussed and not the one we will discuss here. Here we discuss GLM (the generalized one) that is especially popular for modeling binary and count outcomes. Like the linear model, the generalized linear model is concerned about the conditional mean of an outcome variable \\(Y\\); the conditional mean is usually denoted as \\(\\mu\\). Indeed, the linear model we have discussed is a special case of GLM. To see what’s different in other GLMs, let’s apply the normal regression model to categorical outcomes, and see what problems there are. We will use an example from Pritschet, Powell, and Horne (2016), which examines how common it was for researchers to report marginal \\(p\\) values (i.e., .05 &lt; \\(p\\) \\(\\leq\\) .10). The outcome is whether a study reported one or more marginal \\(p\\) values (1 = Yes, 0 = No). The researchers also categorized the studies into three subfields (Cognitive Psychology, Developmental Psychology, Social Psychology), and there were a total of 1,535 studies examined from 1970 to 2010. Let’s read in the data and plot the proportions: # readxl package can be used to import excel files marginalp &lt;- readxl::read_excel(&quot;../data/marginals psych science revision_corrections.xlsx&quot;) # Recode `Field` into a factor marginalp &lt;- marginalp %&gt;% # Filter out studies without any experiments filter(`Number of Experiments` &gt;= 1) %&gt;% mutate(Field = factor(Field, labels = c(&quot;Cognitive Psychology&quot;, &quot;Developmental Psychology&quot;, &quot;Social Psychology&quot;))) %&gt;% # Rename the outcome rename(marginal_p = `Marginals Yes/No`) # Proportion of marginal p in each subfield across Years marginalp %&gt;% ggplot(aes(x = Year, y = marginal_p)) + stat_summary(aes(fill = Field), geom = &quot;ribbon&quot;, alpha = 0.3) + stat_summary(aes(col = Field), geom = &quot;line&quot;) + stat_summary(aes(col = Field), geom = &quot;point&quot;) + coord_cartesian(ylim = c(0, 0.7)) + facet_wrap(~ Field) &gt;# No summary function supplied, defaulting to `mean_se()` &gt;# No summary function supplied, defaulting to `mean_se()` &gt;# No summary function supplied, defaulting to `mean_se()` &gt;# No summary function supplied, defaulting to `mean_se()` &gt;# No summary function supplied, defaulting to `mean_se()` &gt;# No summary function supplied, defaulting to `mean_se()` &gt;# No summary function supplied, defaulting to `mean_se()` &gt;# No summary function supplied, defaulting to `mean_se()` &gt;# No summary function supplied, defaulting to `mean_se()` The error bar is pretty high for Cognitive Psychology, because only 5.922% of the articles in the data set were from that field. We can first try a normal linear model to use Year to predict marginal_p. First recode year into 0 to 4 so that every unit corresponds to 10 years, and 0 means 1970: marginalp &lt;- marginalp %&gt;% mutate(Year10 = (Year - 1970) / 10) # Check the recode distinct(marginalp, Year, Year10) &gt;# # A tibble: 5 x 2 &gt;# Year Year10 &gt;# &lt;dbl&gt; &lt;dbl&gt; &gt;# 1 1980 1 &gt;# 2 1970 0 &gt;# 3 2000 3 &gt;# 4 1990 2 &gt;# 5 2010 4 Now fit a normal linear model: m1_norm &lt;- brm(marginal_p ~ Year10, data = marginalp, prior = c(# for intercept prior(normal(0, 1), class = &quot;Intercept&quot;), # for slope prior(normal(0, 1), class = &quot;b&quot;), # for sigma prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 1340) Now let’s check the model. First a posterior predictive check: pp_check(m1_norm, nsamples = 100) (#fig:ppc_normal_hist)Posterior predictive graphical check with the normal regression model of Year predicting whether a marginal \\(p\\) value was reported. Then the marginal model plot: mmp_brm(m1_norm, x = &quot;Year10&quot;, plot_pi = TRUE, jitter = TRUE, smooth_method = &quot;loess&quot;) &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; (#fig:pi_m1)Marginal model plot of the normal regression model of Year predicting whether a marginal \\(p\\) value was reported. There are two problems with a normal regression model for a binary outcome. First, clearly the outcome cannot be normally distributed as it can only take two values. Second, the predicted regression line is unbounded, meaning that it can go from \\(-\\infty\\) to \\(\\infty\\). It is probably not a problem for this example, as the fitted line is not too way off from the non-parametric smoother, and the predicted mean is still positive. However, it is easy to imagine situations where the predicted mean is smaller than 0 or greater than 1, which is clearly undesirable and leads to insensible predictions. 11.1 Basics of Generalized Linear Models Now, consider how we can fix the two problems previously identified concerning fitting a normal regression model to a binary outcome. First, we want to allow for the possibility of conditional distributions other than normal for \\(Y\\). As you recall, for binary outcome we may consider the Bernoulli distribution. Second, we want to model \\(\\mu\\) as a non-linear function of the predictors, as modeling \\(\\mu\\) as a linear function of the predictors will imply that \\(\\mu\\) can go from \\(-\\infty\\) to \\(\\infty\\), which does not work for categorical outcomes. Note, however, even when the outcome can only take two values, 0 and 1, the mean across multiple observations can be a fraction. In a Bernoulli model, \\(\\mu\\) is the probability of getting a “1”. Similar discrepancy (i.e., mean can be continuous even though the outcome is discrete) is also present in some other GLMs. Under the GLM framework, we have models in the form \\[\\begin{align*} Y_i &amp; \\sim \\mathrm{Dist}(\\mu_i, \\tau) \\\\ g(\\mu_i) &amp; = \\eta_i \\\\ \\eta_i &amp; = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots \\end{align*}\\] where one can choose distributions like Dist = Normal, Poisson, Binomial, Bernoulli, etc. Strictly speaking, GLM requires distributions that are in the exponential family, which will not include distributions like the \\(t\\) distribution. However, this is relatively minor detail as the exponential family is broad and subsumes most of the commonly used models. The distributions will have a mean parameter \\(\\mu_i\\) and may have a dispersion parameter, \\(\\tau\\). An intermediate step in GLM is to transform \\(\\mu_i\\) to \\(\\eta_i\\). \\(\\eta_i\\) is called the linear predictor, which is the linear function of the predictors. In linear models we directly models the conditional mean, \\(\\mu_i\\) as the same as \\(\\eta_i\\). However, to allow for the possibility of \\(\\mu_i\\) being a non-linear function of the predictors, in GLM we transform \\(\\mu_i\\) by applying a link function, \\(g(\\cdot)\\), so that, even though we \\(\\eta_i\\) to be linear in the coefficients, \\(\\mu_i = g^{-1}(\\eta_i)\\) will be a non-linear function of the coefficients as long as the link function is not linear. This step is needed to make sure that the predicted values will not be out of range. 11.2 Binary Logistic Regression To make things more concrete, consider a binary logistic regression example for our example of predicting marginal \\(p\\) being reported using Year of publication. First, we need a model equation of the conditional mean of \\(Y\\), which will be \\[\\texttt{marginal_p}_i \\sim \\mathrm{Bernoulli}(\\mu_i).\\] As you should know, the mean of a Bernoulli distribution is also the probability of a “success” (e.g., if one flips a coin 100 times, and gets 50 heads, the mean of the data is 50 / 100 = 0.5, which is also the probability of getting a head). 11.2.1 The logit link With Year10 being the predictor, we have a linear equation \\(\\eta_i = \\beta_0 + \\beta_1 \\texttt{Year10}_i\\). Now, we will need to specify the link function to map \\(\\mu_i\\) to \\(\\eta_i\\). There are many possible choices, but for binary outcomes a common choice is the logit link, meaning \\(\\eta_i = g(\\mu_i) = \\operatorname{logit}(\\mu_i) = \\log [\\mu_i / (1 - \\mu_i)]\\). The logit link convert probabilities to log odds. You can see the logit link in the left figure below: p1 &lt;- ggplot(data.frame(x = c(0, 1)), aes(x)) + stat_function(fun = qlogis, n = 501) + xlab(expression(mu)) + ylab(expression(eta)) p2 &lt;- ggplot(data.frame(x = c(-6, 6)), aes(x)) + stat_function(fun = plogis, n = 501) + ylab(expression(mu)) + xlab(expression(eta)) grid.arrange(p1, p2, nrow = 1) (#fig:logit_link)(Left) Logit link of \\(\\eta = \\operatorname{logit}(\\mu)\\); (Right) inverse logit link of \\(\\mu = \\operatorname{logit}^{-1}(\\eta)\\). For example, when \\(\\mu = .50\\), \\(\\eta = 0\\); when \\(\\mu = 0.9\\), \\(\\eta = 2.197\\). Therefore, even when the predicted value for \\(\\eta\\) is very large or small, one can always ensure that \\(\\mu\\) is between 0 and 1. The right hand side shows the same relationship, but with \\(\\eta\\) in the x-axis and \\(\\mu\\) in the y-axis, so that we are plotting \\(\\mu_i = g^{-1}(\\eta_i)\\). \\(g^{-1}(\\cdot)\\) is called the inverse link function. For a logit link, the inverse link function is called the standard logistic function, and thus the name logistic regression. The logistic function is defined as \\[ \\mu_i = g^{-1}(\\eta_i) = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}. \\] Generally, people either present the model using the link function or the inverse link function. For our example I can also write \\[\\begin{align*} \\texttt{marginal_p}_i &amp; \\sim \\mathrm{Bernoulli}(\\mu_i) \\\\ \\mu_i &amp; = \\mathrm{logistic}(\\beta_0 + \\beta_1 \\texttt{Year10}_i). \\end{align*}\\] 11.2.2 Choice of Priors There has been previous literature on what choices of prior on the \\(\\beta\\) coefficients for logistic regressions would be appropriate (see Gelman et al. 2008). \\(\\beta\\) coefficients in logistic regression can be relatively large, unlike in normal regression. Therefore, it’s pointed out that a heavy tail distribution, like Cauchy and \\(t\\), would be more appropriate. Recent discussions have settled on priors such as \\(t\\) distributions with a small degrees of freedom as a good balance between heavy tails and efficiency for MCMC sampling, so I will use \\(t(4, 0, .875)\\), where .875 is chosen to be the 1.25 divided by the standard deviation of the predictor, for the priors for logistic regression. To run this model, we can specify the family argument in stan_glm: m1_bern &lt;- brm(marginal_p ~ Year10, data = marginalp, family = bernoulli(link = &quot;logit&quot;), prior = prior(student_t(4, 0, .875), class = &quot;b&quot;), # Note: no sigma seed = 1340) plot(m1_bern) Note that in brms we used family = bernoulli(). In other R functions, such as glm, they do not distinguish between bernoulli and Binomial and only recognize family = binomial(), as a Bernoulli variable is a binomial variable with \\(n = 1\\). 11.2.3 Interpreting the coefficients Any non-linear relationships will involve more work in interpretations, and the coefficients in logistic regressions are no exceptions. 11.2.3.1 Intercept From the equation, when all predictors are zero, we have \\[\\operatorname{logit}(\\mu_i) = \\beta_0.\\] Therefore, the intercept is the log odds that a study reported a marginally significant \\(p\\) value when Year10 = 0 (i.e, in 1970), which was estimated to be -1.348 , 95% CI [-1.552, -1.150]. As log odds are not as intuitive as probability, it is common to instead interpret \\(\\hat{\\mu} = \\operatorname{logistic}(\\beta_0)\\), which is the conditional probability of being marginal_p = 1 in 1970. For Bayesian, that means obtaining the posterior distribution of \\(\\operatorname{logistic}(\\beta_0)\\), which can be done by draws_beta0 &lt;- as.matrix(m1_bern, pars = &quot;b_Intercept&quot;) logistic_beta0 &lt;- plogis(draws_beta0) # Summarize the posterior distribution psych::describe(logistic_beta0) &gt;# vars n mean sd median trimmed mad min max range skew kurtosis se &gt;# X1 1 4000 0.21 0.02 0.21 0.21 0.02 0.15 0.27 0.12 0.13 0 0 The bayesplot package allow you to plot transformed parameters quickly: mcmc_areas(m1_bern, pars = &quot;b_Intercept&quot;, transformations = list(&quot;b_Intercept&quot; = &quot;plogis&quot;), bw = &quot;SJ&quot;) A simpler but not precise method is to directly obtain an estimate of \\(\\operatorname{logistic}(\\beta_0)\\) as \\(\\operatorname{logistic}(-1.348) = 0.206\\) by directly transforming the posterior mean and the credible interval of \\(\\beta_0\\). This usually is okay for large sample sizes and probability close to 0.5, but can give big discrepancies otherwise. Generally I recommend directly obtaining the posterior distribution of \\(\\operatorname{logistic}(\\beta_0)\\) instead. Below illustrates the discrepancy: # Directly from the posterior of logistic(beta0): quantile(logistic_beta0, probs = c(.05, .95)) &gt;# 5% 95% &gt;# 0.180 0.235 # Transform the credible interval limits of beta0: plogis(posterior_interval(m1_bern, pars = &quot;Intercept&quot;)) &gt;# 2.5% 97.5% &gt;# b_Intercept 0.175 0.24 11.2.3.2 Interpreting \\(\\exp(\\beta_1)\\) As Odds Ratio The slope, \\(\\beta_1\\), represents the difference in the predicted log odds between two observations with 1 unit difference in the predictor of interest, while having matching values on all other predictors. For example, for two individuals with 1 unit difference in Year10 (i.e., 10 years), we have \\[\\operatorname{logit}(\\mu_{\\textrm{marginal_p} = 1}) - \\operatorname{logit}(\\mu_{\\textrm{marginal_p} = 0}) = \\beta_1.\\] Again, difference in log odds are hard to interpret, and so we will exponentiate to get \\[\\frac{\\mathrm{odds}_{\\textrm{marginal_p} = 1}} {\\mathrm{odds}_{\\textrm{marginal_p} = 0}} = \\exp(\\beta_1).\\] The fraction on the left hand side is the odds ratio of reporting a marginal \\(p\\) value associated with a one unit difference in Year10 (i.e., 10 years). An odds of 1.0 means that the probability of success and failure is equal; an odds \\(&gt; 1\\) means success is more likely than failures; and an odds \\(&lt; 1\\) means success is less likely than failures. Again, for Bayesian, we need to obtain the posterior distribution of \\(\\exp(\\beta_1)\\) by draws_beta1 &lt;- as.matrix(m1_bern, pars = &quot;b_Year10&quot;) exp_beta1 &lt;- exp(draws_beta1) # Summarize the posterior distribution psych::describe(exp_beta1) &gt;# vars n mean sd median trimmed mad min max range skew kurtosis se &gt;# X1 1 4000 1.43 0.06 1.43 1.43 0.06 1.21 1.63 0.41 0.1 -0.03 0 Using the posterior mean, we predict that the odds of reporting a marginal \\(p\\) value for a study that is 10 years later is multiplied by 1.428 times. And we can again compute a 90% credible interval by: # Directly from the posterior of exp(beta): quantile(exp_beta1, probs = c(.05, .95)) &gt;# 5% 95% &gt;# 1.34 1.53 # Or you can exponentiate the credible interval of beta: exp(posterior_interval(m1_bern, pars = &quot;Year10&quot;)) &gt;# 2.5% 97.5% &gt;# b_Year10 1.32 1.54 Odds ratio (OR) is popular as the multiplicative effect is constant, thus making interpretations easier. Also, in medical research and some other research areas, OR can be an excellent approximation of the relative risk, which is the probability ratio of two groups, of some rare disease or events. However, odds and odds ratio are never intuitive metrics for people, and in many situations a large odds ratio may be misleading as it corresponds to a very small effect. Therefore, in general I would recommend you to interpret coefficients in probability unit, even though that means more work. 11.2.3.3 Interpreting Coefficients in Probability Units Another way to interpret the results of logistic regression that requires more work but is more intuitive is to examine the change in probability. Because the predicted probability is a non-linear function of the predictors, a one unit difference in the predictor has different meanings depending on the values on \\(X\\) you chose for interpretations. Take a look on the predicted values based on our model below: mmp_brm(m1_bern, x = &quot;Year10&quot;, plot_pi = FALSE, jitter = TRUE, smooth_method = &quot;loess&quot;) &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; (#fig:pi_m1_bern)Marginal model plot of the binary logistic regression model of Year predicting whether a marginal \\(p\\) value was reported. Now, consider the change in the predicted probability of reporting a marginal \\(p\\) value with Year10 = 0 (1970) and Year10 = 1 (1980) respectively: When Year10 = 0, \\(P\\)(marginal_p = 1) = \\(\\operatorname{logistic}(\\beta_0)\\), posterior mean = 0.207. When Year10 = 1, \\(P\\)(marginal_p = 1) = \\(\\operatorname{logistic}(\\beta_0 + \\beta_1)\\), posterior mean = 0.271. So between 1970 to 1980, 10 years of time is associated with an increase in the predicted probability of reporting marginal \\(p\\) by 0.064. On the other hand, if we look at the change in predicted probability for Year10 = \\(3\\) and Year10 = \\(4\\), When Year10 = \\(3\\), \\(P\\)(marginal_p = 1) = \\(logistic(\\beta_0 + 3 \\beta_1)\\), posterior mean = 0.43. When Year10 = \\(4\\), \\(P\\)(marginal_p = 1) = \\(\\operatorname{logistic}(\\beta_0 + 4 \\beta_1)\\), posterior mean = 0.518. So for higher values of , one unit increase in Year10 is associated with an increase in the predicted probability of reporting marginal \\(p\\) by 0.088. 11.2.3.3.1 The “divide by 4 rule” A quick approximation is to divide the coefficient by 4 to get an upper bound on the change in probability associated with a one unit change in the predictor. In our example, this corresponds to 0.356 / 4 = 0.089, which is very close to the predicted increase in probability from Year10 = 3 to Year10 = 4 (i.e., 0.088. 11.2.3.4 Model Comparison 11.2.3.4.1 Identity Link Although logistic regression is popular, it is generally hard to interpret. Also, the logit link is not the only link function that is available. There is also the probit link which is commonly used in some areas, which works mostly similar to the logit link. Another possibility is to use the identity link, which just means that \\[\\eta_i = g(\\mu_i) = \\mu_i,\\] so that the probability is directly a linear function of the predictor(s). This will be problematic because it may lead to out of bound probabilities, but when the probabilities are mostly within 20% to 80%, it is generally a pretty good approximation. The nice thing is it gives direct interpretation on probability unit. Without going into details of the priors, below is how it’s fitted: m1_bern_id &lt;- brm(marginal_p ~ Year10, data = marginalp, family = bernoulli(link = &quot;identity&quot;), prior = c(prior(normal(0, 0.5), class = &quot;b&quot;, lb = -1, ub = 1), prior(uniform(0, 1), class = &quot;Intercept&quot;)), # Note: no sigma seed = 1340) loo(m1_bern, m1_bern_id) &gt;# Output of model &#39;m1_bern&#39;: &gt;# &gt;# Computed from 4000 by 1469 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo -910.0 13.8 &gt;# p_loo 2.0 0.1 &gt;# looic 1820.1 27.6 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.0. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Output of model &#39;m1_bern_id&#39;: &gt;# &gt;# Computed from 4000 by 1469 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo -908.1 14.1 &gt;# p_loo 2.0 0.1 &gt;# looic 1816.3 28.2 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.0. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. &gt;# &gt;# Model comparisons: &gt;# elpd_diff se_diff &gt;# m1_bern_id 0.0 0.0 &gt;# m1_bern -1.9 0.7 As you can see, the identity link actually fits better. The model estimates are summary(m1_bern_id) &gt;# Family: bernoulli &gt;# Links: mu = identity &gt;# Formula: marginal_p ~ Year10 &gt;# Data: marginalp (Number of observations: 1469) &gt;# Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; &gt;# total post-warmup samples = 4000 &gt;# &gt;# Population-Level Effects: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# Intercept 0.19 0.02 0.16 0.23 1.00 4013 2924 &gt;# Year10 0.08 0.01 0.06 0.10 1.00 2917 2479 &gt;# &gt;# Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS &gt;# and Tail_ESS are effective sample size measures, and Rhat is the potential &gt;# scale reduction factor on split chains (at convergence, Rhat = 1). So it suggested that every 10 years, the proportion of studies reporting marginal \\(p\\) values increases by 8.078%. If this model fits close to or better than the logistic model, it may be useful for interepretation as opposed to odds ratio or non-linear effects. Here is the model implied relationship: mmp_brm(m1_bern_id, x = &quot;Year10&quot;, plot_pi = FALSE, jitter = TRUE, smooth_method = &quot;loess&quot;) &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; (#fig:pi_m1_id)Marginal model plot of the Bernoulli regression model with identity link of Year predicting whether a marginal \\(p\\) value was reported. Also using marginal effects in brms: marginal_effects(m1_bern_id) &gt;# Warning: Method &#39;marginal_effects&#39; is deprecated. Please use &gt;# &#39;conditional_effects&#39; instead. It should be pointed out that, whereas you can compare Bernoulli models with different link functions, you CANNOT compare the normal model with the logistic model using AIC/WAIC/LOO-IC. Even though they use the same outcome, they treat the outcome variables differently: a normal model treats it as an unbounded continous variable, whereas logistic model treats it a binary discrete variable. In mathematical term, they have densitites with respect to different dominating measures. Generally, you cannot use ICs to compare models with respect to different families, except for some special cases. 11.2.4 Model Checking The assumptions of GLM is similar to the normal GLM, with correct specification (of the predictors and \\(\\eta\\)), linearity (on the \\(\\eta\\) scale), independence, and correct specification of the distribution of the outcome. For normal GLM, the last one involves equal error variance, which is not needed for logistic regression, because error variance is not a parameter for the Bernoulli distribution. Instead, logistic regression assumes constant probability (\\(\\mu\\)) given the same predictor values. To check for the correct specification and linearity assumptions, one can look at the marginal model plots above. For logistic regression, there are a few specific diagnostics one should examine. The following shows three binned residual plots, with each point showing \\(\\tilde y - y\\), where \\(\\tilde y\\) is based on simulated data from the posterior predictive distributions, and \\(y\\) is the observed data. Note that we need the binned residuals or predictive errors, as the prediction error is either 0 or 1, as shown in the Figure below. pp_check(m1_bern_id, type = &quot;error_binned&quot;, nsamples = 9) (#fig:ppc_m1_bern_id)Binned residual plots in replicated data of \\(\\tilde y - y\\). In the above, the binned margins were based on the observed data, whereas the dots were predictive errors from replicated data. As can be seen, with the replicated data, it generally had an underestimate of the proportions in Year 2000 but an oeverestimate of the proportions in Year 2010. It might be reasonable to consider non-linear models of including Year. 11.2.4.1 Classification Error Another way to evaluate a logistic regression model is to look at classification error, which is analogous to \\(R^2\\) for normal regression. (Note: there are also other pseudo \\(R^2\\) measures that can be computed with Bayesian logistic regression, which I will leave for your own study on other sources.) The simplest measure is to assign observations with predicted probabilities larger than \\(\\pi_0\\) to have a value of 1, and to assign observations with predicted probabilities smaller than \\(\\pi_0\\) to have a value of 0, where \\(\\pi_0\\) is some chosen cutoff, and is usually chosen as the proportion of 1s in the sample. For example, for our model, m1_pred &lt;- predict(m1_bern_id, type = &quot;response&quot;)[ , &quot;Estimate&quot;] m1_pred &lt;- as.numeric(m1_pred &gt; mean(marginalp$marginal_p)) # Classification table (classtab_m1 &lt;- table(predicted = m1_pred, observed = marginalp$marginal_p)) &gt;# observed &gt;# predicted 0 1 &gt;# 0 522 157 &gt;# 1 436 354 # Average classification accuracy (acc_m1 &lt;- sum(diag(classtab_m1)) / sum(classtab_m1)) &gt;# [1] 0.596 # so the classification is not particularly impressive. So from a \\(2 \\times 2\\) contingency table, the prediction is correct when the predicted and the observed values are the same (i.e., the two cells in the diagonal), and the prediction is incorrect otherwise. In this example, the classification accuracy is 59.632%. We can consider adding more predictors and re-evaluate the classification error again: # We can add more predictors (interaction with Field), and use a hierarchical # model: m2_bern &lt;- brm(marginal_p ~ (1 | Field) + (1 | Year10), data = marginalp, family = bernoulli(link = &quot;logit&quot;), # Just default priors for quickness control = list(adapt_delta = .995, max_treedepth = 12), seed = 1340) m2_pred &lt;- predict(m2_bern, type = &quot;response&quot;)[ , &quot;Estimate&quot;] m2_pred &lt;- as.numeric(m2_pred &gt; mean(marginalp$marginal_p)) # Classification table (classtab_m2 &lt;- table(predicted = m2_pred, observed = marginalp$marginal_p)) &gt;# observed &gt;# predicted 0 1 &gt;# 0 603 177 &gt;# 1 355 334 # Average classification accuracy (acc_m2 &lt;- sum(diag(classtab_m2)) / sum(classtab_m2)) &gt;# [1] 0.638 Note that unlike information criteria, as the classification error is evaluated on the same sample, you should not be choosing models based on which model has the smallest classification error, as that would generally choose the most complex one that is most prone to overfitting (just like using \\(R^2\\) to choose models in normal regression). One should use information criteria to compare models instead: # Classification now improves; however, one should be interested in # out-of-sample prediction. Let&#39;s check the LOO-IC too: loo(m2_bern) &gt;# &gt;# Computed from 4000 by 1469 log-likelihood matrix &gt;# &gt;# Estimate SE &gt;# elpd_loo -885.8 15.1 &gt;# p_loo 7.1 0.2 &gt;# looic 1771.7 30.3 &gt;# ------ &gt;# Monte Carlo SE of elpd_loo is 0.0. &gt;# &gt;# All Pareto k estimates are good (k &lt; 0.5). &gt;# See help(&#39;pareto-k-diagnostic&#39;) for details. # So adding predictors helps Finally, one can consider varying the cutoff \\(\\pi_0\\), and plots the true positive and true negative rates to obtain an ROC curve. This is beyond the scope of this note, but see, for example, https://www.r-bloggers.com/roc-curves-in-two-lines-of-r-code/. Below I will show you the code for getting the ROC curve: # Using the pROC package pROC::roc(response = marginalp$marginal_p, predictor = predict(m2_bern, type = &quot;response&quot;)[ , &quot;Estimate&quot;], plot = TRUE, print.auc = TRUE) &gt;# Setting levels: control = 0, case = 1 &gt;# Setting direction: controls &lt; cases &gt;# &gt;# Call: &gt;# roc.default(response = marginalp$marginal_p, predictor = predict(m2_bern, type = &quot;response&quot;)[, &quot;Estimate&quot;], plot = TRUE, print.auc = TRUE) &gt;# &gt;# Data: predict(m2_bern, type = &quot;response&quot;)[, &quot;Estimate&quot;] in 958 controls (marginalp$marginal_p 0) &lt; 511 cases (marginalp$marginal_p 1). &gt;# Area under the curve: 0.683 11.2.5 Complete Separation Another issue in logistic regression is separation, i.e., when all cases with \\(y = 1\\) have certain \\(X\\) values that do not overlap with those \\(X\\) values when \\(y = 0\\). In the following example, when \\(X_1 \\geq 4\\) or when \\(X_1 = 3\\) and \\(X_2 = 1\\), all of the \\(Y\\) values are 1. If you run frequentist logistic regression, you will get warning messages with crazy coefficients and extremely large standard errors: sep_dat &lt;- tibble(y = c(0, 0, 0, 0, 1, 1, 1, 1, 1, 1), x1 = c(1, 2, 3, 3, 3, 4, 5, 6, 10, 11), x2 = c(3, 0, -1, 4, 1, 0, 2, 7, 3, 4)) # Scale the predictors to SD 1 sep_dat &lt;- sep_dat %&gt;% mutate_at(vars(x1, x2), ~ . / sd(.)) m_freq &lt;- glm(y ~ x1 + x2, data = sep_dat, family = binomial) &gt;# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(m_freq) &gt;# &gt;# Call: &gt;# glm(formula = y ~ x1 + x2, family = binomial, data = sep_dat) &gt;# &gt;# Deviance Residuals: &gt;# Min 1Q Median 3Q Max &gt;# -1.0042 -0.0001 0.0000 0.0000 1.4689 &gt;# &gt;# Coefficients: &gt;# Estimate Std. Error z value Pr(&gt;|z|) &gt;# (Intercept) -58.08 17511.90 0.0 1.00 &gt;# x1 63.80 19418.72 0.0 1.00 &gt;# x2 -0.29 1.47 -0.2 0.84 &gt;# &gt;# (Dispersion parameter for binomial family taken to be 1) &gt;# &gt;# Null deviance: 13.4602 on 9 degrees of freedom &gt;# Residual deviance: 3.7792 on 7 degrees of freedom &gt;# AIC: 9.779 &gt;# &gt;# Number of Fisher Scoring iterations: 21 On the other hand, use of priors, even relatively weak priors, can regularize the coefficients so that they are more sensible. See the results below, and note the more reasonable coefficients with smaller posterior standard deviations: # Weakly informative priors can help in Bayesian m_bayes &lt;- brm(y ~ x1 + x2, data = sep_dat, family = bernoulli, prior = prior(student_t(4, 0, 1.25), class = &quot;b&quot;), seed = 1340) &gt;# Compiling the C++ model &gt;# Start sampling m_bayes &gt;# Family: bernoulli &gt;# Links: mu = logit &gt;# Formula: y ~ x1 + x2 &gt;# Data: sep_dat (Number of observations: 10) &gt;# Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; &gt;# total post-warmup samples = 4000 &gt;# &gt;# Population-Level Effects: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# Intercept -3.00 2.27 -8.67 0.49 1.00 2053 1491 &gt;# x1 2.94 2.11 0.21 8.28 1.00 1701 1254 &gt;# x2 0.15 0.79 -1.34 1.77 1.00 2033 1996 &gt;# &gt;# Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS &gt;# and Tail_ESS are effective sample size measures, and Rhat is the potential &gt;# scale reduction factor on split chains (at convergence, Rhat = 1). 11.3 Binomial Logistic Regression In binary logistic regression, the outcome for each observation is either 0 or 1. However, if your predictors are discrete so that there are multiple observations with each predictor value, then an equivalent but more concise way of analyzing the data is to do binomial logistic regression. For example, consider using Year10 as a predictor for marginal_p. If we keep the original data, we have the binary logistic model: \\[\\begin{align*} \\texttt{marginal_p}_i &amp; \\sim \\mathrm{Bernoulli}(\\mu_i) \\\\ \\mu_i &amp; = \\mathrm{logistic}(\\beta_0 + \\beta_1 \\texttt{Year10}_i). \\end{align*}\\] However, sometimes the data may come in the form: marginalp_agg &lt;- summarise(group_by(marginalp, Year10), marginal_p = sum(marginal_p), n = n()) &gt;# `summarise()` ungrouping output (override with `.groups` argument) head(marginalp_agg) &gt;# # A tibble: 5 x 3 &gt;# Year10 marginal_p n &gt;# &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &gt;# 1 0 56 321 &gt;# 2 1 76 304 &gt;# 3 2 129 318 &gt;# 4 3 120 229 &gt;# 5 4 130 297 The above data set is grouped. So instead of writing out each observation, one actually only has the possible values of each predictor with the corresponding counts of “successes.” Such form of data is not uncommon, and you may check out built-in data sets like UCBAdmissions for more examples. With data in this form, the outcome is no longer 0 or 1, but instead counts with a maximum of \\(n_j\\), where \\(j\\) is the index for the group. Therefore, we will be using a binomial distribution for the outcome, instead of Bernoulli: \\[\\begin{align*} \\texttt{marginal_p}_j &amp; \\sim \\mathrm{Bin}(n_j, \\mu_j) \\\\ \\mu_j &amp; = \\mathrm{logistic}(\\beta_0 + \\beta_1 \\texttt{Year10}_j). \\end{align*}\\] m1_bin &lt;- brm(marginal_p | trials(n) ~ Year10, data = marginalp_agg, family = binomial(link = &quot;logit&quot;), prior = prior(student_t(4, 0, .875), class = &quot;b&quot;), # Note: no sigma seed = 1340) The results are essentially the same, as in this case the binomial and the Bernoulli model are the same model. And we can look at the good at the goodness-of-fit by comparing the predicted and observed counts in groups. Now, because there are multiple observations for each level of , we can naturally use the five groups: pp_check(m1_bin, type = &quot;intervals&quot;) &gt;# Using all posterior samples for ppc type &#39;intervals&#39; by default. (#fig:ppc-m1_bin)Posterior predictive check using the predicted and observed counts. As can be seen, the fit wasn’t particularly good in this case. 11.4 Probit Regression Just note that the logit link is not the only choice for binary/binomial data. You’ve seen the identity link: \\(g(\\mu) = \\mu\\). Another popular choice is \\(g(\\mu) = \\Phi^{-1}(\\mu)\\), where \\(\\Phi^{-1}(\\cdot)\\) is the normal quantile function (or inverse cumulative density function,which can be obtained with qnorm in R). You can simply change the link to family = binomial(link = \"probit\") in R. Practically, using the logit link or the probit gives basically identical results, so it’s a matter of ease of interpretations and conventions between the two. Note that because of the use of different link functions, the coefficients will be on different scales that differ by a factor of approximately 1.7. You are encouraged to try out the probit link yourself. 11.5 Poisson Regression The Poisson GLM is used to model count outcomes. Count outcomes are non-negative discrete integers. Remember in GLM, we’re modeling the mean of the outcome, \\(\\mu\\). Therefore, we need to make sure \\(\\mu\\) is non-negative, so we need a link function that can map \\(\\eta\\) from the whole real line to non-negative numbers; by far the most commonly used link function is the logarithmic transformation, \\(g(\\mu) = \\log(\\mu)\\) (what’s the inverse link function?): ggplot(data.frame(x = c(0, 100)), aes(x)) + stat_function(fun = log, n = 501) + xlab(expression(mu)) + ylab(expression(eta)) (#fig:log_link)Link of \\(\\eta = \\log(\\mu)\\). redcard_dat &lt;- readr::read_csv(&quot;../data/redcard_data.zip&quot;) Let’s use the example you’ve seen on studying the distribution of number of red cards, which we first learned the Poisson distribution. Here, however, we’re interested in what predicts the number of red cards a referee will give. In the data, there are 3147 referees. We are interested in whether referees give more or less red cards depending on the skin color of the players, and whether the referee’s country of origin makes a difference. We aggregated the data to the referee level, with two predictor variables: player_dark, which is the average skin rating of the players that the referee had in the data (0 = very light skin to 1 = very dark skin), and meanExp, the mean explicit bias score (using a racial thermometer task) for referee country. redcard_ref &lt;- redcard_dat %&gt;% group_by(refNum) %&gt;% summarise(redCards = sum(redCards), player_dark = (weighted.mean(rater1, games) + weighted.mean(rater2, games)) / 2, refCountry = refCountry[1], meanExp = meanExp[1], games = sum(games)) &gt;# `summarise()` ungrouping output (override with `.groups` argument) First check the distribution of the counts: ggplot(redcard_ref, aes(x = redCards)) + geom_bar() You can see a lot of zeros. Below are the scatterplots against the predictors: ggplot(redcard_ref, aes(x = player_dark, y = redCards)) + geom_jitter(width = 0.05, height = 0.5, alpha = 0.1) + geom_smooth() &gt;# `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; &gt;# Warning: Removed 1855 rows containing non-finite values (stat_smooth). &gt;# Warning: Removed 1855 rows containing missing values (geom_point). ggplot(redcard_ref, aes(x = meanExp, y = redCards)) + geom_jitter(width = 0.05, height = 0.5, alpha = 0.1) + geom_smooth() &gt;# `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; &gt;# Warning: Removed 11 rows containing non-finite values (stat_smooth). &gt;# Warning: Removed 11 rows containing missing values (geom_point). It doesn’t appear that there are strong relationships. Let’s fit a Poisson model. First, because the referees are nested within countries, we need to account for that clustering. Second, the number of red cards depends on how many games a referee has called, so we need to adjust for that by including an offset term. The following is the equation of the Poisson model: \\[\\begin{align*} \\texttt{redCards}_{ij} &amp; \\sim \\mathrm{Pois}(\\mu_{ij}) \\\\ \\log(\\mu_{ij}) &amp; = \\log(\\texttt{games}_{ij}) + \\beta_{0j} + \\beta_{1j} \\texttt{player_dark}_{ij} \\\\ \\begin{bmatrix} \\beta_{0j} \\\\ \\beta_{1j} \\\\ \\end{bmatrix} &amp; \\sim \\mathcal{N}_2\\left( \\begin{bmatrix} \\mu^{[\\beta_0]}_j \\\\ \\mu^{[\\beta_1]}_j \\\\ \\end{bmatrix}, \\boldsymbol{\\mathbf{T}} \\right) \\\\ \\boldsymbol{\\mathbf{T}} &amp; = \\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}) \\boldsymbol{\\mathbf{\\Omega }}\\operatorname{diag}(\\boldsymbol{\\mathbf{\\tau}}) \\\\ \\mu^{[\\beta_0]}_j &amp; = \\gamma_{00} + \\gamma_{01} \\texttt{meanIAT}_j \\\\ \\mu^{[\\beta_1]}_j &amp; = \\gamma_{10} + \\gamma_{11} \\texttt{meanIAT}_j \\end{align*}\\] And I will use these priors: \\[\\begin{align*} \\gamma_{00} &amp; \\sim t^+(4, 0, 5) \\\\ \\gamma_{10} &amp; \\sim t^+(4, 0, 2.5) \\\\ \\gamma_{01} &amp; \\sim t^(4, 0, 2.5) \\\\ \\gamma_{11} &amp; \\sim t^(4, 0, 2.5) \\\\ \\tau^{[\\beta_m]} &amp; \\sim \\mathrm{Gamma}(2, 0.2), \\; m = 0, 1 \\\\ \\boldsymbol{\\mathbf{\\Omega }}&amp; \\sim \\mathrm{LKJ}(1) \\end{align*}\\] m4 &lt;- brm(redCards ~ player_dark * meanExp + offset(log(games)) + (player_dark | refCountry), family = poisson(), data = redcard_ref, prior = c(# for intercept prior(student_t(4, 0, 5), class = &quot;Intercept&quot;), # for slope prior(student_t(4, 0, 2.5), class = &quot;b&quot;), # for tau_beta0 prior(gamma(2, 0.2), class = &quot;sd&quot;, group = &quot;refCountry&quot;), # for correlation prior(lkj(1), class = &quot;cor&quot;)), cores = 2L, control = list(adapt_delta = .95), seed = 1340) &gt;# Warning: Rows containing NAs were excluded from the model. The coefficients were shown in the graph below: stanplot(m4, type = &quot;areas&quot;, bw = &quot;SJ&quot;) &gt;# Warning: Method &#39;stanplot&#39; is deprecated. Please use &#39;mcmc_plot&#39; instead. 11.5.1 Interpretations Because of the nonlinear link function, one needs to be careful in interpreting the coefficients. Consider if there is only one predictor, we have \\(\\log(\\mu) = \\beta_0 + \\beta_1 X_1 \\Rightarrow \\mu = \\exp(\\beta_0) \\exp(\\beta_1 X_1)\\), so every unit increase in \\(X_1\\) is associated with the predicted mean counts being multiplied by \\(\\exp(\\beta_1)\\) times. With the interaction the results are even harder to directly interpret. Therefore it is best to plot the associations based on the model: plot(marginal_effects(m4), ask = FALSE) &gt;# Warning: Method &#39;marginal_effects&#39; is deprecated. Please use &gt;# &#39;conditional_effects&#39; instead. As can be seen, there are a lot of uncertainty in the associations, and there was not much evidence for a positive association between player_dark and number of red cards, even though the association was trending more to the positive side for referees coming from a country with higher explicit bias score. 11.5.2 Model Checking One should look at the posterior predictive graphical check, marginal model plots, proportion of zeros, maximum values, and the standardized residuals against the fitted values. From the plots below, it is obvious that the Poisson model does not fit the data, as the variability of the data is much larger than what can be predicted by the Poisson model, with excessive zeros. The predictive intervals failed to cover a large number of data. ppc_dens_pois &lt;- pp_check(m4) &gt;# Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default. ppc_max_pois &lt;- pp_check(m4, type = &quot;stat&quot;, stat = &quot;max&quot;) &gt;# Using all posterior samples for ppc type &#39;stat&#39; by default. var_mean_ratio &lt;- function(y) var(y) / mean(y) ppc_vm_ratio_pois &lt;- pp_check(m4, type = &quot;stat&quot;, stat = &quot;var_mean_ratio&quot;) &gt;# Using all posterior samples for ppc type &#39;stat&#39; by default. prop_zero &lt;- function(y) mean(y == 0) ppc_pzero_pois &lt;- pp_check(m4, type = &quot;stat&quot;, stat = &quot;prop_zero&quot;) &gt;# Using all posterior samples for ppc type &#39;stat&#39; by default. grid.arrange(ppc_dens_pois, ppc_max_pois, ppc_vm_ratio_pois, ppc_pzero_pois, nrow = 2) &gt;# `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. &gt;# `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. &gt;# `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. As shown above, the fit wasn’t terrible but also wasn’t very good for the Poisson model. One major problem is that the number of red cards for each player is limited to only 1, so the counts are actually bounded to be the total number of games. Also we aggregated the data by referees, but because many different referees also gave red cards to the same players, the player-level clustering hasn’t been accounted for. And as we saw in Chapter 3, we can also look at the rootogram: pp_check(m4, type = &quot;rootogram&quot;, style = &quot;hanging&quot;) &gt;# Using all posterior samples for ppc type &#39;rootogram&#39; by default. 11.5.3 Other models in GLM There are other models that are in GLM or related to GLM. First, there are infinitely many possible link functions for each choice of distribution. For example, you can use an identity link, \\(g(\\mu) = \\mu\\), for logistic and Poisson regressions, although such link functions are not conventional. Check ?brmsfamily in R for available link functions. Also, there are models that handle overdispersion, which happened when there are some hidden heterogeneity not accounted for by the predictors, which leads to models such as beta-binomial for binomial models and negative binomial for Poisson models. There are also other types of distributions, such as gamma, multinomial, depending on the types of outcomes. A related model is the ordinal regression model, which is available in brms. You may refer to this paper: https://psyarxiv.com/x8swp/ for more information. References "],
["missing-data.html", "Chapter 12 Missing Data 12.1 Missing Data Mechanisms 12.2 Bayesian Approaches for Missing Data", " Chapter 12 Missing Data Missing data are common in many research problems. Sometimes missing data arise from design, but more often data are missing for reasons that are beyond researchers’ control. I will first provide some conceptual discussion on the types of missing data, and then talk about the Bayesian approach for handling missing data by treating missing data as parameters with some prior information. I will then give a brief introduction of multiple imputation and its Bayesian origin. A regression with missing data problem will be used to illustrate two Bayesian approaches to handle missing data. 12.1 Missing Data Mechanisms To simplify the discussion, assume that missing values are only present in the outcome \\(Y\\) in a hypothetical regression problem of using people’s age (\\(X\\)) to predict their voting intention (\\(Y\\)). Let \\(R\\) be an indicator variable that denotes whether \\(Y\\) is missing (\\(R = 0\\)) or not (\\(R = 1\\)). For example, if \\(Y\\) looks like set.seed(3) N &lt;- 30 x &lt;- sort(sample(18:80, N, replace = TRUE)) y &lt;- 0.5 * x / sd(x) + rnorm(N, sd = 0.8) y &lt;- round(y * 1.4 + 2, 1) ifelse(y[1:10] &gt; 4, NA, y[1:10]) &gt;# [1] 2.4 2.7 2.4 2.0 2.0 3.9 3.2 2.3 1.7 NA then \\(R\\) will be &gt;# [1] 1 1 1 1 1 1 1 1 1 0 Assume our data look like the first scatter plot below if there are no missing data: demo_data &lt;- tibble(y = y, x = x, r_mcar = rep(c(TRUE, FALSE, TRUE), 10), r_mar = 1:N &gt;= 11, r_nmar = rank(0.2 * x / sd(x) + 0.8 * y / sd(y)) &gt;= 11) &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; &gt;# `geom_smooth()` using formula &#39;y ~ x&#39; Figure 12.1: Scatter plots for different types of missing data Missing data can be related to the predictor \\(X\\) (e.g., older people are more likely to give a missing response), the outcome \\(Y\\) itself (e.g., people with lower voting intention are less likely to respond), and some other unmeasured factors that relate to neither \\(X\\) nor \\(Y\\), which I summarize as \\(Z\\). Depending on what causes missing data, the three missing data algorithms are MCAR (missing completely at random), MAR (missing at random), and NMAR (not missing at random), as summarized in the figures below, which I will further explain. 12.1.1 MCAR (Missing Completely at Random) MCAR means that the probability of a missing response (denoted as \\(R\\)) is unrelated to anything of interest in the research question. For example, for the left graph in Figure 2, \\(Z\\) maybe some haphazard events such as interviewers accidentally erase responses for some people, which we believe to be unrelated to participants’ ages or voting intentions. The plot on the top right panel of Figure 1 is an example, with the missing cases being grayed out. One quick-and-dirty method to check for MCAR is to check whether the distribution of \\(X\\) is similar for cases with or without missing data on \\(Y\\), and as you can see in the above graph the means and variances of \\(X\\) for the group with missing data and for the group without are highly similar. This method can be generalized to data with missing data on multiple variables, and one can check whether missing data on every variable affect all other variables. As you can see, the regression line barely changes with or without the missing data. In general, under MCAR, using only cases with no missing value still give valid inferences and unbiased estimations. However, for more complex models complete case analyses (also called listwise deletion) can greatly reduce the sample size for analysis, as it throws away information from cases with partial information. 12.1.2 MAR (Missing At Random) It’s probably not the most intuitive naming, but MAR refers to the condition that the probability of a missing observation (\\(R\\)) can be explained by the observed data (i.e., \\(X\\) in this case). In other words, missing data does not relate to the values that would have been observed (which is denoted as \\(Y_\\textrm{mis}\\)), once we considered the observed data. For example, for the middle graph in Figure 2, some missing data on voting intentions can be explained by some random factor \\(Z\\), but for some cases data are missing because, for instance, younger people tend to be less motivated to complete the survey. The plot on the bottom left panel of Figure 1 is an example, with the missing cases being grayed out. As can be seen, when data are MAR, the distributions of \\(X\\) are different for groups with and without missing \\(Y\\) values. Also, the distributions of the observed \\(Y\\) values differ systematically from the complete data. Under MAR, using only the cases without missing values still produces an unbiased estimate of the regression coefficient, if missing data is only present in \\(Y\\). However, for more complex models and with missing data in \\(X\\), more advanced methods generally give more accurate coefficient estimates and standard errors. 12.1.3 NMAR (Not Missing At Random) NMAR is sometimes called missing not at random or non-ignorable missingness, and as the name suggested it refers to conditions where MAR does not hold. In other words, NMAR happens when, after considering all the observed data, the probability of a missing value (\\(R\\)) still depends on the value of \\(Y\\) that would have been observed. For example, if we consider people in the same age group and still find those with lower voting intentions tend not to give their responses, the situation can be described as NMAR. The plot on the bottom right panel of Figure 1, where people with lowing voting intentions are more likely to miss. The example looks very similar to the one for MAR, including the fact that the distributions of \\(X\\) are different for the group with and without missing \\(Y\\). Indeed, there are no statistical procedures that can distinguish between MAR in general and NMAR. If there are evidence for MCAR then one can be more confident in ruling out NMAR, and there have been recent efforts to establish procedures for testing some special cases of MAR. However, for many real data problems one has to rely on reasoning, judgments, and perhaps some educated guessing to decide whether the data is MAR or NMAR. On the other hand, if one has variables that potentially relates to the probability of missing but are not part of the model of interest (e.g., gender, SES, etc), these can be included in the imputation model (discussed later) so that the missing data mechanism better resembles MAR. Including these auxiliary variables is equivalent to changing them from unmeasured to measured, and generally can weaken the associations between the unobserved \\(Y\\) and \\(R\\), thus making the estimates less biased. With NMAR, valid statistical inferences can only be obtained by correctly modeling the mechanism for the missing data. Including variables that help explain probability of missing data makes MAR more reasonable. 12.1.4 Ignorable Missingness* Let \\(Y_\\textrm{obs}\\) be the part of the multivariate data \\(Y\\) that is observed (i.e., not missing), and \\(Y_\\textrm{mis}\\) be the part that would have been observed. The likelihood now concerns both \\(Y_\\textrm{obs}\\) and \\(R\\), that is, \\(P(Y_\\textrm{obs}, R)\\). Let \\(\\boldsymbol{\\mathbf{\\phi}}\\) be the set of parameters that determine the probability of missing in addition to the observed data, which can be written as \\(P(R | Y_\\textrm{obs}, \\boldsymbol{\\mathbf{\\phi}})\\). Note it is assumed that \\(\\boldsymbol{\\mathbf{\\phi}}\\) is distinct from the model parameters \\(\\boldsymbol{\\mathbf{\\theta}}\\). For a case \\(i\\) with \\(r_i = 1\\), the joint likelihood of \\((x_i, y_i, r_i = 1)\\) is \\[P(x_i, y_{\\textrm{obs}, i}, r_i = 1; \\boldsymbol{\\mathbf{\\theta}}, \\boldsymbol{\\mathbf{\\phi}}) = P(r_i = 1 | x_i, y_{\\textrm{obs}, i}; \\boldsymbol{\\mathbf{\\phi}}) P(y_{\\textrm{obs}, i} | x_i; \\boldsymbol{\\mathbf{\\theta}}) P(x_i).\\] For a case with \\(r_i = 0\\), \\(y_i\\) is missing. Assume first we know the missing value \\(y_{\\textrm{mis}, i}\\), and the complete likelihood \\((x_i, y_{\\textrm{mis}, i}, r_i = 0)\\) is \\[P(x_i, y_{\\textrm{mis}, i}, r_i = 0; \\boldsymbol{\\mathbf{\\theta}}, \\boldsymbol{\\mathbf{\\phi}}) = P(r_i = 0 | x_i, y_{\\textrm{mis}, i}; \\boldsymbol{\\mathbf{\\phi}}) P(y_{\\textrm{mis}, i} | x_i; \\boldsymbol{\\mathbf{\\theta}}) P(x_i)\\] But because \\(y\\) is missing, we need to integrate out the missing value to obtain the observed likelihood of \\((x_i, r_i = 0)\\) \\[\\begin{align*} P(x_i, r_i = 0; \\boldsymbol{\\mathbf{\\theta}}, \\boldsymbol{\\mathbf{\\phi}}) &amp; = \\int P(r_i = 0 | x_i, y_{\\textrm{mis}, i}; \\boldsymbol{\\mathbf{\\phi}}) P(y_{\\textrm{mis}, i} | x_i; \\boldsymbol{\\mathbf{\\theta}}) P(x_i) \\; \\mathrm{d}y_{\\textrm{mis}, i} \\\\ &amp; = P(x_i) \\int P(r_i = 0 | x_i, y_{\\textrm{mis}, i}; \\boldsymbol{\\mathbf{\\phi}}) P(y_{\\textrm{mis}, i} | x_i; \\boldsymbol{\\mathbf{\\theta}}) \\; \\mathrm{d}y_{\\textrm{mis}, i} \\end{align*}\\] Because the likelihood depends on \\(R\\) and cannot be separated from \\(\\boldsymbol{\\mathbf{\\phi}}\\), correct inference on \\(\\boldsymbol{\\mathbf{\\theta}}\\) can be obtained only by correct modeling the missing data mechanism. 12.1.4.1 If MCAR Holds However, if the condition for MCAR is satisfied such that \\[P(r_i = 0 | x_i, y_{\\textrm{mis}, i}; \\boldsymbol{\\mathbf{\\phi}}) = P(r_i = 0; \\boldsymbol{\\mathbf{\\phi}}),\\] that is, \\(R\\) is related to neither \\(X\\) and \\(Y\\) Then the observed likelihood is \\[\\begin{align*} P(x_i, r_i = 0; \\boldsymbol{\\mathbf{\\theta}}, \\boldsymbol{\\mathbf{\\phi}}) &amp; = P(x_i) \\int P(r_i = 0; \\boldsymbol{\\mathbf{\\phi}}) P(y_{\\textrm{mis}, i} | x_i; \\boldsymbol{\\mathbf{\\theta}}) \\; \\mathrm{d}y_{\\textrm{mis}, i} \\\\ &amp; = P(x_i) P(r_i = 0; \\boldsymbol{\\mathbf{\\phi}}) \\times \\int P(y_{\\textrm{mis}, i} | x_i; \\boldsymbol{\\mathbf{\\theta}}) \\; \\mathrm{d}y_{\\textrm{mis}, i} \\\\ &amp; = P(x_i) P(r_i = 0; \\boldsymbol{\\mathbf{\\phi}}) \\end{align*}\\] So inference of \\(\\boldsymbol{\\mathbf{\\theta}}\\) does not depend on the missing data mechanism \\(P(r_i = 0; \\boldsymbol{\\mathbf{\\phi}})\\), and missingness is ignorable. 12.1.4.2 If MAR Holds Similarly, if the condition for MAR is satisfied such that \\[P(r_i = 0 | x_i, y_{\\textrm{mis}, i}; \\boldsymbol{\\mathbf{\\phi}}) = P(r_i = 0 | x_i, ; \\boldsymbol{\\mathbf{\\phi}}),\\] that is, \\(R\\) is not related to \\(Y\\) after taking into account \\(X\\). Then the observed likelihood is \\[\\begin{align*} P(x_i, r_i = 0; \\boldsymbol{\\mathbf{\\theta}}, \\boldsymbol{\\mathbf{\\phi}}) &amp; = P(x_i) \\int P(r_i = 0 | x_i; \\boldsymbol{\\mathbf{\\phi}}) P(y_{\\textrm{mis}, i} | x_i; \\boldsymbol{\\mathbf{\\theta}}) \\; \\mathrm{d}y_{\\textrm{mis}, i} \\\\ &amp; = P(x_i) P(r_i = 0 | x_i; \\boldsymbol{\\mathbf{\\phi}}) \\times \\int P(y_{\\textrm{mis}, i} | x_i; \\boldsymbol{\\mathbf{\\theta}}) \\; \\mathrm{d}y_{\\textrm{mis}, i} \\\\ &amp; = P(x_i) P(r_i = 0 | x_i; \\boldsymbol{\\mathbf{\\phi}}) \\end{align*}\\] So inference of \\(\\boldsymbol{\\mathbf{\\theta}}\\) does not depend on the missing data mechanism \\(P(r_i = 0 | x_i; \\boldsymbol{\\mathbf{\\phi}})\\), and missingness is ignorable. On the other hand, if \\(r_i\\) depends on \\(y_\\textrm{mis}\\) (i.e., NMAR) so that \\(P(r_i = 0 | x_i, y_{\\textrm{mis}, i}; \\boldsymbol{\\mathbf{\\phi}})\\) cannot be written outside of the integral, inference of \\(\\boldsymbol{\\mathbf{\\theta}}\\) depends on the missing data mechanism, so missingness is non-ignorable. The discussion generalizes to missing data on multiple variables. 12.2 Bayesian Approaches for Missing Data We will be using the kidiq data set we discussed in Chapter 7. I’ll do the same rescaling and coding mom_hs as a factor variable: kidiq &lt;- haven::read_dta(&quot;../data/kidiq.dta&quot;) kidiq100 &lt;- kidiq %&gt;% mutate(mom_iq = mom_iq / 100, # divid mom_iq by 100 kid_score = kid_score / 100, # divide kid_score by 100 mom_iq_c = mom_iq - 1, mom_hs = factor(mom_hs, labels = c(&quot;no&quot;, &quot;yes&quot;))) %&gt;% select(- mom_iq) In R, the package mice can be used to perform multiple imputation (to be discussed soon), as well as to create missing data. First, let’s generate some missing completely at random (MCAR) data by randomly removing up to 50% of the data: library(mice) set.seed(1955) kidiq100_mcar &lt;- ampute(kidiq100, prop = 0.5, pattern = data.frame(kid_score = c(0, 0, 0), mom_hs = c(1, 1, 0), mom_work = c(1, 1, 1), mom_age = c(1, 1, 1), mom_iq_c = c(1, 0, 1)), freq = c(.2, .4, .4), mech = &quot;MCAR&quot;) kidiq100_mcar &lt;- kidiq100_mcar$amp The second time, I’ll generate some missing at random (MAR) data: set.seed(1955) kidiq100_mar &lt;- ampute(kidiq100, prop = 0.5, pattern = data.frame(kid_score = c(0, 0, 0), mom_hs = c(1, 1, 0), mom_work = c(1, 1, 1), mom_age = c(1, 1, 1), mom_iq_c = c(1, 0, 1)), freq = c(.2, .4, .4), mech = &quot;MAR&quot;) &gt;# Warning: Data is made numeric because the calculation of weights requires &gt;# numeric data kidiq100_mar &lt;- kidiq100_mar$amp And finally, some not missing at random (NMAR) data: set.seed(1955) kidiq100_nmar &lt;- ampute(kidiq100, prop = 0.5, pattern = data.frame(kid_score = c(0, 0, 0), mom_hs = c(1, 1, 0), mom_work = c(1, 1, 1), mom_age = c(1, 1, 1), mom_iq_c = c(1, 0, 1)), freq = c(.2, .4, .4), # mice call it MNAR mech = &quot;MNAR&quot;) &gt;# Warning: Data is made numeric because the calculation of weights requires &gt;# numeric data kidiq100_nmar &lt;- kidiq100_nmar$amp Let’s check the distributions of the resulting data: p1 &lt;- ggplot(kidiq100_mcar, aes(x = mom_iq_c, y = kid_score)) + geom_point(data = kidiq100, color = &quot;grey92&quot;) + geom_point() + geom_smooth() p2 &lt;- p1 %+% kidiq100_mar p3 &lt;- p1 %+% kidiq100_nmar grid.arrange(p1, p2, p3, nrow = 2) &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; &gt;# Warning: Removed 230 rows containing non-finite values (stat_smooth). &gt;# Warning: Removed 230 rows containing missing values (geom_point). &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; &gt;# Warning: Removed 215 rows containing non-finite values (stat_smooth). &gt;# Warning: Removed 215 rows containing missing values (geom_point). &gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; &gt;# Warning: Removed 239 rows containing non-finite values (stat_smooth). &gt;# Warning: Removed 239 rows containing missing values (geom_point). When eyeballing it doesn’t appear that the data are very different, but the regression slopes are affected by the different missing data mechanisms. We’ll look at the simple regression model of using mom_iq_c to predict kid_score, using the MAR data set. In that data set, the missingness of kid_score actually depends on both mom_iq_c and mom_hs, but when the regression does not include mom_hs in the model, the resulting situation will actually be NMAR. The missing data pattern of the kidiq100_mar data set is: # Recode mom_hs to factor kidiq100_mar$mom_hs &lt;- factor(kidiq100_mar$mom_hs, labels = c(&quot;no&quot;, &quot;yes&quot;)) md.pattern(kidiq100_mar, rotate.names = TRUE) &gt;# mom_work mom_age mom_hs mom_iq_c kid_score &gt;# 219 1 1 1 1 1 0 &gt;# 49 1 1 1 1 0 1 &gt;# 94 1 1 1 0 0 2 &gt;# 72 1 1 0 1 0 2 &gt;# 0 0 72 94 215 381 Which shows that only 219 observations had full data, and most were missing the kid_score variable. 12.2.1 Complete Case Analysis/Listwise Deletion By default, brms uses only cases with no missing data. For example, m3_ld &lt;- brm(kid_score ~ mom_iq_c + mom_hs, data = kidiq100_mar, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 2302 ) &gt;# Warning: Rows containing NAs were excluded from the model. m3_ld &gt;# Family: gaussian &gt;# Links: mu = identity; sigma = identity &gt;# Formula: kid_score ~ mom_iq_c + mom_hs &gt;# Data: kidiq100_mar (Number of observations: 219) &gt;# Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; &gt;# total post-warmup samples = 4000 &gt;# &gt;# Population-Level Effects: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# Intercept 0.81 0.03 0.76 0.86 1.00 4281 3286 &gt;# mom_iq_c 0.71 0.11 0.51 0.91 1.00 4192 3222 &gt;# mom_hsyes 0.07 0.03 0.01 0.13 1.00 4361 3233 &gt;# &gt;# Family Specific Parameters: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# sigma 0.20 0.01 0.18 0.22 1.00 4354 3116 &gt;# &gt;# Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS &gt;# and Tail_ESS are effective sample size measures, and Rhat is the potential &gt;# scale reduction factor on split chains (at convergence, Rhat = 1). Notice that the number of observations is only 219. As previously explained, this analysis is only valid when data are missing completely at random or missing at random (i.e., missingness of the outcome only depends on mom_iq_c and factors unrelated to Ozone). If you recall in Chapter 7, the coefficient using the full data should be: m3 &lt;- brm(kid_score ~ mom_iq_c + mom_hs, data = kidiq100, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 1955 ) m3 &gt;# Family: gaussian &gt;# Links: mu = identity; sigma = identity &gt;# Formula: kid_score ~ mom_iq_c + mom_hs &gt;# Data: kidiq100 (Number of observations: 434) &gt;# Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; &gt;# total post-warmup samples = 4000 &gt;# &gt;# Population-Level Effects: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# Intercept 0.82 0.02 0.78 0.86 1.00 4140 2905 &gt;# mom_iq_c 0.56 0.06 0.44 0.68 1.00 4044 3195 &gt;# mom_hsyes 0.06 0.02 0.02 0.10 1.00 3884 3034 &gt;# &gt;# Family Specific Parameters: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# sigma 0.18 0.01 0.17 0.19 1.00 3902 3132 &gt;# &gt;# Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS &gt;# and Tail_ESS are effective sample size measures, and Rhat is the potential &gt;# scale reduction factor on split chains (at convergence, Rhat = 1). So the listwise approach overestimated the regression coefficient. We can do better. Now, take a look on whether missingness in kid_score is related to other variables. # Compute the missingness indicator (you can use the `within` function too) kidiq100_mar_R &lt;- transform(kidiq100_mar, kid_score_R = factor(as.numeric(!is.na(kid_score)), labels = c(&quot;Missing&quot;, &quot;Observed&quot;))) # Plot distributions of variables against missingness indicator qplot(kid_score_R, mom_iq_c, data = kidiq100_mar_R, geom = &quot;boxplot&quot;) &gt;# Warning: Removed 94 rows containing non-finite values (stat_boxplot). ggplot(data = kidiq100_mar_R, aes(x = kid_score_R)) + geom_bar(aes(fill = factor(mom_hs)), position = position_dodge()) As we already knew, missingness of kid_score is related to both mom_iq_c and mom_hs, in that those with higher mom_iq_c and those whose mother had high school degree were more likely to be missing. 12.2.2 Treat Missing Data as Parameters A fully Bayesian approach to handle missing data is to treat the missing kid_score values just as parameters, and assign priors to them. When the missing data mechanism is ignorable (MCAR or MAR), we can assume that the missing and observed kid_score values are exchangeable, conditioning on the predictors (i.e., whether kid_score is missing or not does not add information to the kid_score values). Therefore, if kid_score is missing, we use the likelihood as the prior for the missing values: \\[\\begin{align*} \\mathtt{kid_score}_{\\textrm{obs}, i}&amp; \\sim \\mathcal{N}(\\beta_0 + \\beta_1 \\mathtt{mom_iq_c}_i, \\sigma) \\\\ \\mathtt{kid_score}_{\\textrm{mis}, i}&amp; \\sim \\mathcal{N}(\\beta_0 + \\beta_1 \\mathtt{mom_iq_c}_i, \\sigma) \\\\ \\beta_0 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_1 &amp; \\sim \\mathcal{N}(0, 1) \\\\ \\beta_2 &amp; \\sim \\mathcal{N}(0, 1) \\end{align*}\\] library(rstan) rstan_options(auto_write = TRUE) data { int&lt;lower=0&gt; N; // number of observations vector[N] y; // response variable (including missing values); int&lt;lower=0, upper=1&gt; y_obs[N]; // missingness indicator for Y int&lt;lower=0&gt; p; // number of predictor variables (exclude intercept) matrix[N, p] X; // predictor variable matrix } transformed data { int n_obs = sum(y_obs); // number of observed cases int ns[n_obs]; // indices of observed cases int ny = 1; for (n in 1:N) { if (y_obs[n]) { ns[ny] = n; ny += 1; } } } parameters { real beta_0; // intercept vector[2] beta; // 2 slopes real&lt;lower=0&gt; sigma; // error standard deviation } model { // likelihood for observed Y y[ns] ~ normal_id_glm(X[ns, ], beta_0, beta, sigma); // prior beta_0 ~ normal(0, 1); beta ~ normal(0, 1); sigma ~ student_t(4, 0, 1); } generated quantities { real yrep[N]; // simulated data based on model vector[N] yhat = beta_0 + X * beta; // used to compute R-squared effect size for (i in 1:N) { yrep[i] = normal_rng(yhat[i], sigma); } } # Data with no missing X kidiq100_mar_obsX &lt;- drop_na(kidiq100_mar, mom_iq_c, mom_hs) m3_stan &lt;- stan(&quot;../codes/normal_regression_missing.stan&quot;, data = list(N = nrow(kidiq100_mar_obsX), y = replace_na(kidiq100_mar_obsX$kid_score, 99), y_obs = as.numeric( !is.na(kidiq100_mar_obsX$kid_score) ), p = 2, X = model.matrix(~ mom_iq_c + mom_hs, data = kidiq100_mar_obsX)[ , -1]), seed = 1234) Note that the results are basically identical to the complete case analyses, and the posterior distributions of the missing \\(Y\\) values are essentially the predictive intervals given the \\(X\\) values. For example, for the first 10 observations with missing kid_score values, draws_ymis &lt;- as.matrix(m3_stan, pars = &quot;yrep&quot;)[ , is.na(kidiq100_mar_obsX$kid_score)] mcmc_areas_ridges(draws_ymis[ , 1:10], bw = &quot;SJ&quot;) Figure 12.2: Posterior density plots of the first two missing values of \\texttt{kid_score} The posterior distributions of the missing values are highly related to the missing data handling technique called multiple imputation, which we will discuss next. Indeed, each posterior sample can be considered an imputed data set. The posterior draws of the missing values are also called plausible values. 12.2.3 Multiple Imputation Multiple imputation is one of the modern techniques for missing data handling, and is general in that it has a very broad application. It uses the observed data and the observed associations to predict the missing values, and captures the uncertainty involved in the predictions by imputing multiple data sets. That’s a bit abstract, but with your Bayesian knowledge, that just means getting samples from the posterior distributions of the missing values, and then substitute them to the missing holes to form an imputed data set. The difference is that, instead of using all posterior samples, we usually obtain 20 or 30 imputed data sets, which can be saved and used for almost any kind of analyses, Bayesian or frequentist. 12.2.3.1 Multiple imputation has several advantages It provides valid results when data is MAR It reduces biases when data is NMAR by incorporating covariates that help explain the missing data mechanism (e.g., mom_work and mom_age) It is very flexible and can impute continuous and categorical variables 12.2.3.2 Example of multiple imputation Although in theory one can use the Bayesian procedures with Stan to account for missing data or to do multiple imputations, there are some limitations. First, when the goal is to impute missing data instead of making inferences on the model parameters, the algorithm in Stan may not be as efficient as specialized programs for multiple imputation. Second, the Hamiltonian Monte Carlo sampler in Stan requires the use of derivatives, so it is not (yet) well-equipped to handle categorical parameters. Thus, it is hard or not possible to handle categorical missing data. Third, when the number of variables with missing data is large, it is tedious to specify the missing data mechanism for all variables. Instead, as Gelman et al. (2013) recommended, we can handle missing data using a two-step process: Do multiple imputation using a specialized program Use brms or rstan (or other Bayesian methods) to analyze each imputed data set 12.2.3.3 R packages for multiple imputation There are several packages in R for multiple imputation (e.g., Amelia, jomo, mi, mice, missForest, norm, pan). Although these packages differ in terms of their underlying algorithms, my experience and also evidence from the literature suggested that they usually gave similar performance for continuous missing data, but several packages have specialized functionality for specific models and data types (e.g., categorical missing data, multilevel data). I will illustrate the use of mice below. I strongly encourage you to take a look on the vignettes found on the website of the package: https://github.com/stefvanbuuren/mice. Also, the author of the package has a nice book on multiple imputation (Van Buuren 2018), which is freely available at https://stefvanbuuren.name/fimd/ and I encourage you to read if you are interested. Note that the example discussed here is simple so not much fine tuning for the imputation is needed. For your own analyses multiple imputation can be complex, and you should consult statisticians or other resources to set up a reasonable imputation model. Let’s continue with the kidiq example. We can use the whole data set for imputation. In general it’s recommended to include covariates that have even minor associations with the probability of missing. The bias introduced by ignoring an important covariate usually is higher than the bias introduced by including a inappropriate covariate. However, see Thoemmes and Rose (2014) for a cautionary note. In planning a study, if high missing rate on a variable is anticipated, one can collect covariates that can help explain the missing data mechanism. This helps recover missing information in the analyses. 12.2.3.3.1 1. Setting up and run the imputation With binary and continuous missing variables, it can be as simple as running the following: library(mice) # Using mice to impute 20 data sets kidiq100_imp &lt;- mice(kidiq100_mar, m = 20, maxit = 35, printFlag = FALSE) # set to false only for knitting to Rmd Of course this oversimplifies the complexity of multiple imputation. By default it uses the method called “predictive mean matching” to replace missing data with a randomly chosen value from several similar cases (see https://stefvanbuuren.name/fimd/sec-pmm.html). Things will get more complicated when you have more variables and complex data types. Typing kidiq100_imp$imp will show the imputed missing values. Check ?mice for more information. The complete function fills the missing values to the missing holes to form data sets with no missing data. kidiq100_imp1 &lt;- complete(kidiq100_imp, 1) head(kidiq100_imp1, 10) &gt;# kid_score mom_hs mom_work mom_age mom_iq_c &gt;# 1 1.21 yes 4 27 -0.1883 &gt;# 2 1.26 yes 4 25 0.2754 &gt;# 3 0.85 yes 4 27 0.1544 &gt;# 4 0.41 yes 3 25 -0.1785 &gt;# 5 0.42 yes 4 27 -0.0725 &gt;# 6 0.98 no 1 18 0.0790 &gt;# 7 0.69 yes 4 20 0.3889 &gt;# 8 1.16 yes 3 23 0.2515 &gt;# 9 1.02 yes 1 24 -0.1838 &gt;# 10 0.95 yes 1 19 -0.0493 Compared to the original data: head(kidiq100, 10) &gt;# # A tibble: 10 x 5 &gt;# kid_score mom_hs mom_work mom_age mom_iq_c &gt;# &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &gt;# 1 0.65 yes 4 27 0.211 &gt;# 2 0.98 yes 4 25 -0.106 &gt;# 3 0.85 yes 4 27 0.154 &gt;# 4 0.83 yes 3 25 -0.00550 &gt;# 5 1.15 yes 4 27 -0.0725 &gt;# 6 0.98 no 1 18 0.0790 &gt;# 7 0.69 yes 4 20 0.389 &gt;# 8 1.06 yes 3 23 0.251 &gt;# 9 1.02 yes 1 24 -0.184 &gt;# 10 0.95 yes 1 19 -0.0493 12.2.3.3.2 2. Check for Convergence We should also look at convergence: plot(kidiq100_imp) These are basically Markov chains in regular Bayesian analyses. So if you see some chains are constantly above or below others then it’s problematic. See https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html for additional steps to check for convergence. 12.2.3.3.3 3. Run brm_multiple on imputed data sets brms directly supports multiply imputed data sets. Simply use the brm_multiple function and supply the multiply imputed data object to it. Also, for computational efficiency using two chains for each imputed data set would be faster. m3_imp &lt;- brm_multiple(kid_score ~ mom_iq_c + mom_hs, data = kidiq100_imp, prior = c(prior(normal(0, 1), class = &quot;Intercept&quot;), # set for all &quot;b&quot; coefficients prior(normal(0, 1), class = &quot;b&quot;), prior(student_t(4, 0, 1), class = &quot;sigma&quot;)), seed = 1955, chains = 2L, cores = 2L ) See this vignette: https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html#compatibility-with-other-multiple-imputation-packages for more information. If you look at the results: m3_imp &gt;# Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be &gt;# careful when analysing the results! We recommend running more iterations and/or &gt;# setting stronger priors. &gt;# Family: gaussian &gt;# Links: mu = identity; sigma = identity &gt;# Formula: kid_score ~ mom_iq_c + mom_hs &gt;# Data: kidiq100_imp (Number of observations: 434) &gt;# Samples: 40 chains, each with iter = 2000; warmup = 1000; thin = 1; &gt;# total post-warmup samples = 40000 &gt;# &gt;# Population-Level Effects: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# Intercept 0.80 0.02 0.76 0.85 1.15 170 783 &gt;# mom_iq_c 0.67 0.10 0.48 0.86 1.40 84 328 &gt;# mom_hsyes 0.07 0.03 0.02 0.13 1.18 144 653 &gt;# &gt;# Family Specific Parameters: &gt;# Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS &gt;# sigma 0.19 0.01 0.18 0.21 1.20 135 246 &gt;# &gt;# Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS &gt;# and Tail_ESS are effective sample size measures, and Rhat is the potential &gt;# scale reduction factor on split chains (at convergence, Rhat = 1). You will see that there are 40 chains in the results. The Rhat value will be much higher than 1, as the chains are from different data sets and will never converge. Instead, you should investigate the Rhat for each data set by m3_imp$rhats &gt;# b_Intercept b_mom_iq_c b_mom_hsyes sigma lp__ &gt;# 1 1.002 1.000 1.002 1.003 1.002 &gt;# 2 0.999 1.001 0.999 1.000 1.000 &gt;# 3 1.000 0.999 1.000 1.002 0.999 &gt;# 4 0.999 0.999 0.999 1.001 1.004 &gt;# 5 1.001 1.001 1.002 1.003 1.001 &gt;# 6 1.000 1.001 1.001 1.001 0.999 &gt;# 7 1.000 1.000 1.000 1.001 1.000 &gt;# 8 1.000 1.000 1.000 1.000 1.002 &gt;# 9 1.000 0.999 1.001 0.999 1.002 &gt;# 10 0.999 1.000 0.999 1.000 1.003 &gt;# 11 0.999 1.001 0.999 1.000 1.000 &gt;# 12 0.999 1.000 0.999 0.999 1.000 &gt;# 13 1.000 1.000 1.000 1.000 1.000 &gt;# 14 1.001 1.000 1.000 0.999 1.001 &gt;# 15 1.001 1.000 1.001 1.000 1.000 &gt;# 16 0.999 1.000 0.999 0.999 1.001 &gt;# 17 1.001 1.000 1.003 0.999 1.000 &gt;# 18 1.000 1.000 1.000 0.999 0.999 &gt;# 19 1.000 0.999 1.001 0.999 1.000 &gt;# 20 1.001 1.000 1.001 1.002 1.003 So the chains have converged for each individual data set. Now, put the results together: source(&quot;extract_brmsfit.R&quot;) texreg::screenreg(map(list(m3, m3_ld, m3_imp), extract_brmsfit, # LOO-IC and WAIC not meaningful for comparing different # data include.loo.ic = FALSE, include.waic = FALSE), custom.model.names = c(&quot;Full data&quot;, &quot;Complete case&quot;, &quot;MI&quot;)) &gt;# Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be &gt;# careful when analysing the results! We recommend running more iterations and/or &gt;# setting stronger priors. &gt;# Warning: Using only the first imputed data set. Please interpret the results &gt;# with caution until a more principled approach has been implemented. &gt;# &gt;# ==================================================== &gt;# Full data Complete case MI &gt;# ---------------------------------------------------- &gt;# Intercept 0.82 * 0.81 * 0.80 * &gt;# [0.78; 0.86] [0.76; 0.86] [0.76; 0.85] &gt;# mom_iq_c 0.56 * 0.71 * 0.67 * &gt;# [0.44; 0.68] [0.51; 0.91] [0.49; 0.86] &gt;# mom_hsyes 0.06 * 0.07 * 0.07 * &gt;# [0.01; 0.10] [0.01; 0.13] [0.02; 0.13] &gt;# ---------------------------------------------------- &gt;# R^2 0.21 0.22 0.22 &gt;# Num. obs. 434 219 434 &gt;# ==================================================== &gt;# * Null hypothesis value outside the confidence interval. You can see that the coefficients for mom_iq_c is closer to the original data with multiple imputation, and the credible intervals are slightly shorter than complete case analyses. For data with more variables, choices of missing data handling method can make a substantial difference. Therefore, researchers need to be thoughtful in choosing imputation models that best reflect the missing data mechanism. Missing data is an active research area, and this note only covers a very small fraction of the issues discussed in the literature. References "],
["references.html", "References", " References Bürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01. Carvalho, Carlos M, Nicholas G Polson, and James G Scott. 2009. “Handling Sparsity via the Horseshoe.” In Artificial Intelligence and Statistics, 73–80. Frank, Avi, Sena Biberci, and Bruno Verschuere. 2019. “The language of lies: a preregistered direct replication of Suchotzki and Gamer (2018; Experiment 2).” Cognition and Emotion 33 (6): 1310–5. https://doi.org/10.1080/02699931.2018.1553148. Gelman, Andrew. 2006. “Prior distributions for variance parameters in hierarchical models (Comment on Article by Browne and Draper).” Bayesian Analysis 1 (3): 515–34. https://doi.org/10.1214/06-BA117A. Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald Rubin. 2013. Bayesian Data Analysis. 3rd ed. London, UK: CRC Press. Gelman, Andrew, Jennifer Hill, and Masanao Yajima. 2012. “Why we (usually) don’t have to worry about multiple comparisons.” Journal of Research on Educational Effectiveness 5 (2): 189–211. https://doi.org/10.1080/19345747.2011.618213. Gelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” The Annals of Applied Statistics, 1360–83. Gelman, Andrew, Xiao-Li Meng, and Hal Stern. 1996. “Posterior Predictive Assessment of Model Fitness via Realized Discrepancies.” Statistica Sinica, 733–60. Gigerenzer, Gerd. 2004. “Mindless statistics.” The Journal of Socio-Economics 33 (5): 587–606. https://doi.org/10.1016/j.socec.2004.09.033. Heathcote, Andrew, Scott Brown, and Denis Cousineau. 2004. “QMPE: Estimating Lognormal, Wald, and Weibull Rt Distributions with a Parameter-Dependent Lower Bound.” Behavior Research Methods, Instruments, &amp; Computers 36 (2): 277–90. Hedeker, Donald, Robin J. Mermelstein, and Hakan Demirtas. 2008. “An application of a mixed-effects location scale model for analysis of ecological momentary assessment (EMA) data.” Biometrics 64 (2): 627–34. https://doi.org/10.1111/j.1541-0420.2007.00924.x. Hoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science, 382–401. Kruschke, John K. 2013. “Bayesian estimation supersedes the t test.” Journal of Experimental Psychology: General 142 (2): 573–603. https://doi.org/10.1037/a0029146. ———. 2015. Doing Bayesian Data Analysis: Tutorial with R, JAGS, and Stan. 2nd ed. London, UK: Academic Press. Kruschke, John K, and Torrin M Liddell. 2018. “The Bayesian new statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective.” Psychonomic Bulletin &amp; Review 25 (1): 178–206. https://doi.org/10.3758/s13423-016-1221-4. Lai, Mark H. C., and Oi-man Kwok. 2015. “Examining the Rule of Thumb of Not Using Multilevel Modeling: The ‘Design Effect Smaller Than Two’ Rule.” The Journal of Experimental Education 83: 423–38. https://doi.org/10.1080/00220973.2014.907229. Lambert, Ben. 2018. A student’s guide to Bayesian statistics. https://bookshelf.vitalsource.com. McElreath, Richard. 2016. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Vol. 122. CRC Press. Piironen, Juho, and Aki Vehtari. 2016. “Comparison of Bayesian Predictive Methods for Model Selection.” Statistics and Computing. Pritschet, Laura, Derek Powell, and Zachary Horne. 2016. “Marginally Significant Effects as Evidence for Hypotheses: Changing Attitudes over Four Decades.” Psychological Science 27 (7): 1036–42. Silberzahn, Raphael, Eric L Uhlmann, Daniel P Martin, Pasquale Anselmi, Frederik Aust, Eli Awtrey, Štěpán Bahnı́k, et al. 2018. “Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results.” Advances in Methods and Practices in Psychological Science 1 (3): 337–56. Thoemmes, Felix, and Norman Rose. 2014. “A Cautious Note on Auxiliary Variables That Can Increase Bias in Missing Data Problems.” Multivariate Behavioral Research 49 (5): 443–59. Van Buuren, Stef. 2018. Flexible Imputation of Missing Data. 2nd ed. Boca Raton, FL: CRC Press. https://stefvanbuuren.name/fimd/. van de Schoot, Rens, Sonja D. Winter, Oisín Ryan, Mariëlle Zondervan-Zwijnenburg, and Sarah Depaoli. 2017. “A systematic review of Bayesian articles in psychology: The last 25 years.” Psychological Methods 22 (2): 217–39. https://doi.org/10.1037/met0000100. Vehtari, Aki, Andrew Gelman, and Jonah Gabry. 2016. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and Waic.” Statistics and Computing 27 (5): 1413–32. https://doi.org/10.1007/s11222-016-9696-4. Yao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. “Using stacking to average bayesian predictive distributions (with discussion).” Bayesian Analysis 13 (3): 917–1007. https://doi.org/10.1214/17-BA1091. "]
]

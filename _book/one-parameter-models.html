<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 One-Parameter Models | Course Handouts for Bayesian Data Analysis Class</title>
  <meta name="description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Spring semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 One-Parameter Models | Course Handouts for Bayesian Data Analysis Class" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Spring semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 One-Parameter Models | Course Handouts for Bayesian Data Analysis Class" />
  
  <meta name="twitter:description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Spring semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  

<meta name="author" content="Mark Lai" />


<meta name="date" content="2019-12-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-inference.html"/>
<link rel="next" href="brief-introduction-to-stan.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.1/grViz.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PSYC 621 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#history-of-bayesian-statistics"><i class="fa fa-check"></i><b>1.1</b> History of Bayesian Statistics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#thomas-bayes-17011762"><i class="fa fa-check"></i><b>1.1.1</b> Thomas Bayes (1701–1762)</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#pierre-simon-laplace-17491827"><i class="fa fa-check"></i><b>1.1.2</b> Pierre-Simon Laplace (1749–1827)</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction.html"><a href="introduction.html#th-century"><i class="fa fa-check"></i><b>1.1.3</b> 20th Century</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#motivations-for-using-bayesian-methods"><i class="fa fa-check"></i><b>1.2</b> Motivations for Using Bayesian Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#problem-with-classical-frequentist-statistics"><i class="fa fa-check"></i><b>1.2.1</b> Problem with classical (frequentist) statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#probability"><i class="fa fa-check"></i><b>1.3</b> Probability</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#classical-interpretation"><i class="fa fa-check"></i><b>1.3.1</b> Classical Interpretation</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#frequentist-interpretation"><i class="fa fa-check"></i><b>1.3.2</b> Frequentist Interpretation</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#problem-of-the-single-case"><i class="fa fa-check"></i><b>1.3.3</b> Problem of the single case</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#subjectivist-interpretation"><i class="fa fa-check"></i><b>1.3.4</b> Subjectivist Interpretation</a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#basics-of-probability"><i class="fa fa-check"></i><b>1.3.5</b> Basics of Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#bayess-theorem"><i class="fa fa-check"></i><b>1.4</b> Bayes’s Theorem</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#example-1-base-rate-fallacy-from-wikipedia"><i class="fa fa-check"></i><b>1.4.1</b> Example 1: Base rate fallacy (From Wikipedia)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#bayesian-statistics"><i class="fa fa-check"></i><b>1.5</b> Bayesian Statistics</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#example-2-locating-a-plane"><i class="fa fa-check"></i><b>1.5.1</b> Example 2: Locating a Plane</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#comparing-bayesian-and-frequentist-statistics"><i class="fa fa-check"></i><b>1.6</b> Comparing Bayesian and Frequentist Statistics</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#software-for-bayesian-statistics"><i class="fa fa-check"></i><b>1.7</b> Software for Bayesian Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>2</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Steps of Bayesian Data Analysis</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#real-data-example"><i class="fa fa-check"></i><b>2.2</b> Real Data Example</a></li>
<li class="chapter" data-level="2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#choosing-a-model"><i class="fa fa-check"></i><b>2.3</b> Choosing a Model</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#exchangeability"><i class="fa fa-check"></i><b>2.3.1</b> Exchangeability*</a></li>
<li class="chapter" data-level="2.3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#probability-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="2.3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-likelihood"><i class="fa fa-check"></i><b>2.3.3</b> The Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#specifying-priors"><i class="fa fa-check"></i><b>2.4</b> Specifying Priors</a><ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#beta-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#obtain-the-posterior-distributions"><i class="fa fa-check"></i><b>2.5</b> Obtain the Posterior Distributions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#grid-approximation"><i class="fa fa-check"></i><b>2.5.1</b> Grid Approximation</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#using-conjugate-priors"><i class="fa fa-check"></i><b>2.5.2</b> Using Conjugate Priors</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#laplace-approximation-with-maximum-a-posteriori-estimation"><i class="fa fa-check"></i><b>2.5.3</b> Laplace Approximation with Maximum A Posteriori Estimation</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#markov-chain-monte-carlo-mcmc"><i class="fa fa-check"></i><b>2.5.4</b> Markov Chain Monte Carlo (MCMC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summarizing-the-posterior-distribution"><i class="fa fa-check"></i><b>2.6</b> Summarizing the Posterior Distribution</a><ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-mean-median-and-mode"><i class="fa fa-check"></i><b>2.6.1</b> Posterior Mean, Median, and Mode</a></li>
<li class="chapter" data-level="2.6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#uncertainty-estimates"><i class="fa fa-check"></i><b>2.6.2</b> Uncertainty Estimates</a></li>
<li class="chapter" data-level="2.6.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>2.6.3</b> Credible Intervals</a></li>
<li class="chapter" data-level="2.6.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#probability-of-theta-higherlower-than-a-certain-value"><i class="fa fa-check"></i><b>2.6.4</b> Probability of <span class="math inline">\(\theta\)</span> Higher/Lower Than a Certain Value</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#model-checking"><i class="fa fa-check"></i><b>2.7</b> Model Checking</a><ul>
<li class="chapter" data-level="2.7.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>2.7.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.8</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="2.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summary"><i class="fa fa-check"></i><b>2.9</b> Summary</a><ul>
<li class="chapter" data-level="2.9.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#key-concepts"><i class="fa fa-check"></i><b>2.9.1</b> Key Concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="one-parameter-models.html"><a href="one-parameter-models.html"><i class="fa fa-check"></i><b>3</b> One-Parameter Models</a><ul>
<li class="chapter" data-level="3.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#binomialbernoulli-data"><i class="fa fa-check"></i><b>3.1</b> Binomial/Bernoulli data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#reparameterization"><i class="fa fa-check"></i><b>3.1.1</b> Reparameterization*</a></li>
<li class="chapter" data-level="3.1.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#posterior-predictive-check-1"><i class="fa fa-check"></i><b>3.1.2</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="3.1.3" data-path="one-parameter-models.html"><a href="one-parameter-models.html#comparison-to-frequentist-results"><i class="fa fa-check"></i><b>3.1.3</b> Comparison to frequentist results</a></li>
<li class="chapter" data-level="3.1.4" data-path="one-parameter-models.html"><a href="one-parameter-models.html#sensitivity-to-different-priors"><i class="fa fa-check"></i><b>3.1.4</b> Sensitivity to different priors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#poisson-data"><i class="fa fa-check"></i><b>3.2</b> Poisson Data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#example-2"><i class="fa fa-check"></i><b>3.2.1</b> Example 2</a></li>
<li class="chapter" data-level="3.2.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#choosing-a-model-1"><i class="fa fa-check"></i><b>3.2.2</b> Choosing a model</a></li>
<li class="chapter" data-level="3.2.3" data-path="one-parameter-models.html"><a href="one-parameter-models.html#choosing-a-prior"><i class="fa fa-check"></i><b>3.2.3</b> Choosing a prior</a></li>
<li class="chapter" data-level="3.2.4" data-path="one-parameter-models.html"><a href="one-parameter-models.html#model-equations-and-diagram"><i class="fa fa-check"></i><b>3.2.4</b> Model Equations and Diagram</a></li>
<li class="chapter" data-level="3.2.5" data-path="one-parameter-models.html"><a href="one-parameter-models.html#getting-the-posterior"><i class="fa fa-check"></i><b>3.2.5</b> Getting the posterior</a></li>
<li class="chapter" data-level="3.2.6" data-path="one-parameter-models.html"><a href="one-parameter-models.html#posterior-predictive-check-2"><i class="fa fa-check"></i><b>3.2.6</b> Posterior Predictive Check</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html"><i class="fa fa-check"></i><b>4</b> Brief Introduction to STAN</a><ul>
<li class="chapter" data-level="4.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#stan"><i class="fa fa-check"></i><b>4.1</b> <code>STAN</code></a><ul>
<li class="chapter" data-level="4.1.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#stan-code"><i class="fa fa-check"></i><b>4.1.1</b> <code>STAN</code> code</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#rstan"><i class="fa fa-check"></i><b>4.2</b> <code>RStan</code></a><ul>
<li class="chapter" data-level="4.2.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#assembling-data-list-in-r"><i class="fa fa-check"></i><b>4.2.1</b> Assembling data list in R</a></li>
<li class="chapter" data-level="4.2.2" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#call-rstan"><i class="fa fa-check"></i><b>4.2.2</b> Call <code>rstan</code></a></li>
<li class="chapter" data-level="4.2.3" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#summarize-the-results"><i class="fa fa-check"></i><b>4.2.3</b> Summarize the results</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#resources"><i class="fa fa-check"></i><b>4.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="group-comparisons.html"><a href="group-comparisons.html"><i class="fa fa-check"></i><b>5</b> Group Comparisons</a><ul>
<li class="chapter" data-level="5.1" data-path="group-comparisons.html"><a href="group-comparisons.html#data"><i class="fa fa-check"></i><b>5.1</b> Data</a></li>
<li class="chapter" data-level="5.2" data-path="group-comparisons.html"><a href="group-comparisons.html#between-subject-comparisons"><i class="fa fa-check"></i><b>5.2</b> Between-Subject Comparisons</a><ul>
<li class="chapter" data-level="5.2.1" data-path="group-comparisons.html"><a href="group-comparisons.html#plots"><i class="fa fa-check"></i><b>5.2.1</b> Plots</a></li>
<li class="chapter" data-level="5.2.2" data-path="group-comparisons.html"><a href="group-comparisons.html#independent-sample-t-test"><i class="fa fa-check"></i><b>5.2.2</b> Independent sample t-test</a></li>
<li class="chapter" data-level="5.2.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesian-normal-model"><i class="fa fa-check"></i><b>5.2.3</b> Bayesian Normal Model</a></li>
<li class="chapter" data-level="5.2.4" data-path="group-comparisons.html"><a href="group-comparisons.html#robust-model"><i class="fa fa-check"></i><b>5.2.4</b> Robust Model</a></li>
<li class="chapter" data-level="5.2.5" data-path="group-comparisons.html"><a href="group-comparisons.html#shifted-lognormal-model"><i class="fa fa-check"></i><b>5.2.5</b> Shifted Lognormal Model*</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="group-comparisons.html"><a href="group-comparisons.html#notes-on-model-comparison"><i class="fa fa-check"></i><b>5.3</b> Notes on Model Comparison</a></li>
<li class="chapter" data-level="5.4" data-path="group-comparisons.html"><a href="group-comparisons.html#within-subject-comparisons"><i class="fa fa-check"></i><b>5.4</b> Within-Subject Comparisons</a><ul>
<li class="chapter" data-level="5.4.1" data-path="group-comparisons.html"><a href="group-comparisons.html#plots-1"><i class="fa fa-check"></i><b>5.4.1</b> Plots</a></li>
<li class="chapter" data-level="5.4.2" data-path="group-comparisons.html"><a href="group-comparisons.html#independent-sample-t-test-1"><i class="fa fa-check"></i><b>5.4.2</b> Independent sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="5.4.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesian-normal-model-1"><i class="fa fa-check"></i><b>5.4.3</b> Bayesian Normal Model</a></li>
<li class="chapter" data-level="5.4.4" data-path="group-comparisons.html"><a href="group-comparisons.html#using-brms"><i class="fa fa-check"></i><b>5.4.4</b> Using <code>brms</code>*</a></li>
<li class="chapter" data-level="5.4.5" data-path="group-comparisons.html"><a href="group-comparisons.html#region-of-practical-equivalence-rope"><i class="fa fa-check"></i><b>5.4.5</b> Region of Practical Equivalence (ROPE)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="6.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#monte-carlo-simulation-with-one-unknown"><i class="fa fa-check"></i><b>6.1</b> Monte Carlo Simulation With One Unknown</a></li>
<li class="chapter" data-level="6.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo-mcmc-with-one-parameter"><i class="fa fa-check"></i><b>6.2</b> Markov Chain Monte Carlo (MCMC) With One Parameter</a><ul>
<li class="chapter" data-level="6.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> The Metropolis algorithm</a></li>
<li class="chapter" data-level="6.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-metropolis-hastings-algorithm"><i class="fa fa-check"></i><b>6.2.2</b> The Metropolis-Hastings Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain"><i class="fa fa-check"></i><b>6.3</b> Markov Chain</a></li>
<li class="chapter" data-level="6.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#effective-sample-size-n_texteff"><i class="fa fa-check"></i><b>6.4</b> Effective Sample Size (<span class="math inline">\(n_\text{eff}\)</span>)</a></li>
<li class="chapter" data-level="6.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mc-error"><i class="fa fa-check"></i><b>6.5</b> MC Error</a></li>
<li class="chapter" data-level="6.6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#burn-inwarmup"><i class="fa fa-check"></i><b>6.6</b> Burn-in/Warmup</a><ul>
<li class="chapter" data-level="6.6.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#thinning"><i class="fa fa-check"></i><b>6.6.1</b> Thinning</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#diagnostics-of-mcmc"><i class="fa fa-check"></i><b>6.7</b> Diagnostics of MCMC</a><ul>
<li class="chapter" data-level="6.7.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mixing"><i class="fa fa-check"></i><b>6.7.1</b> Mixing</a></li>
<li class="chapter" data-level="6.7.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#acceptance-rate"><i class="fa fa-check"></i><b>6.7.2</b> Acceptance Rate</a></li>
<li class="chapter" data-level="6.7.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#diagnostics-using-multiple-chains"><i class="fa fa-check"></i><b>6.7.3</b> Diagnostics Using Multiple Chains</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#multiple-parameters"><i class="fa fa-check"></i><b>6.8</b> Multiple Parameters</a></li>
<li class="chapter" data-level="6.9" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>6.9</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>7</b> Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="linear-models.html"><a href="linear-models.html#what-is-regression"><i class="fa fa-check"></i><b>7.1</b> What is Regression?</a></li>
<li class="chapter" data-level="7.2" data-path="linear-models.html"><a href="linear-models.html#one-predictor"><i class="fa fa-check"></i><b>7.2</b> One Predictor</a><ul>
<li class="chapter" data-level="7.2.1" data-path="linear-models.html"><a href="linear-models.html#a-continuous-predictor"><i class="fa fa-check"></i><b>7.2.1</b> A continuous predictor</a></li>
<li class="chapter" data-level="7.2.2" data-path="linear-models.html"><a href="linear-models.html#centering"><i class="fa fa-check"></i><b>7.2.2</b> Centering</a></li>
<li class="chapter" data-level="7.2.3" data-path="linear-models.html"><a href="linear-models.html#a-categorical-predictor"><i class="fa fa-check"></i><b>7.2.3</b> A categorical predictor</a></li>
<li class="chapter" data-level="7.2.4" data-path="linear-models.html"><a href="linear-models.html#predictors-with-multiple-categories"><i class="fa fa-check"></i><b>7.2.4</b> Predictors with multiple categories</a></li>
<li class="chapter" data-level="7.2.5" data-path="linear-models.html"><a href="linear-models.html#stan-4"><i class="fa fa-check"></i><b>7.2.5</b> STAN</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="linear-models.html"><a href="linear-models.html#multiple-regression"><i class="fa fa-check"></i><b>7.3</b> Multiple Regression</a><ul>
<li class="chapter" data-level="7.3.1" data-path="linear-models.html"><a href="linear-models.html#two-predictor-example"><i class="fa fa-check"></i><b>7.3.1</b> Two Predictor Example</a></li>
<li class="chapter" data-level="7.3.2" data-path="linear-models.html"><a href="linear-models.html#interactions"><i class="fa fa-check"></i><b>7.3.2</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="linear-models.html"><a href="linear-models.html#tabulating-the-models"><i class="fa fa-check"></i><b>7.4</b> Tabulating the Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Model Diagnostics</a><ul>
<li class="chapter" data-level="8.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#assumptions-of-linear-models"><i class="fa fa-check"></i><b>8.1</b> Assumptions of Linear Models</a></li>
<li class="chapter" data-level="8.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#diagnostic-tools"><i class="fa fa-check"></i><b>8.2</b> Diagnostic Tools</a><ul>
<li class="chapter" data-level="8.2.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#posterior-predictive-check-7"><i class="fa fa-check"></i><b>8.2.1</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="8.2.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#marginal-model-plots"><i class="fa fa-check"></i><b>8.2.2</b> Marginal model plots</a></li>
<li class="chapter" data-level="8.2.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#residual-plots"><i class="fa fa-check"></i><b>8.2.3</b> Residual plots</a></li>
<li class="chapter" data-level="8.2.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#multicollinearity"><i class="fa fa-check"></i><b>8.2.4</b> Multicollinearity</a></li>
<li class="chapter" data-level="8.2.5" data-path="model-diagnostics.html"><a href="model-diagnostics.html#robust-models"><i class="fa fa-check"></i><b>8.2.5</b> Robust Models</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#other-topics"><i class="fa fa-check"></i><b>8.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html"><i class="fa fa-check"></i><b>9</b> Model Comparison and Regularization</a><ul>
<li class="chapter" data-level="9.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>9.1</b> Overfitting and Underfitting</a></li>
<li class="chapter" data-level="9.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>9.2</b> Kullback-Leibler Divergence</a></li>
<li class="chapter" data-level="9.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#information-criteria"><i class="fa fa-check"></i><b>9.3</b> Information Criteria</a><ul>
<li class="chapter" data-level="9.3.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#experiment-on-deviance"><i class="fa fa-check"></i><b>9.3.1</b> Experiment on Deviance</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#information-criteria-1"><i class="fa fa-check"></i><b>9.4</b> Information Criteria</a><ul>
<li class="chapter" data-level="9.4.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#akaike-information-criteria-aic"><i class="fa fa-check"></i><b>9.4.1</b> Akaike Information Criteria (AIC)</a></li>
<li class="chapter" data-level="9.4.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#deviance-information-criteria-dic"><i class="fa fa-check"></i><b>9.4.2</b> Deviance Information Criteria (DIC)</a></li>
<li class="chapter" data-level="9.4.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#watanabe-akaike-information-criteria-waic"><i class="fa fa-check"></i><b>9.4.3</b> Watanabe-Akaike Information Criteria (WAIC)</a></li>
<li class="chapter" data-level="9.4.4" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>9.4.4</b> Leave-One-Out Cross Validation</a></li>
<li class="chapter" data-level="9.4.5" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#example"><i class="fa fa-check"></i><b>9.4.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#stackingmodel-averaging"><i class="fa fa-check"></i><b>9.5</b> Stacking/Model Averaging</a><ul>
<li class="chapter" data-level="9.5.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#model-weights"><i class="fa fa-check"></i><b>9.5.1</b> Model Weights</a></li>
<li class="chapter" data-level="9.5.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#model-averaging"><i class="fa fa-check"></i><b>9.5.2</b> Model Averaging</a></li>
<li class="chapter" data-level="9.5.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#stacking"><i class="fa fa-check"></i><b>9.5.3</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#shrinkage-priors"><i class="fa fa-check"></i><b>9.6</b> Shrinkage Priors</a><ul>
<li class="chapter" data-level="9.6.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#number-of-parameters"><i class="fa fa-check"></i><b>9.6.1</b> Number of parameters</a></li>
<li class="chapter" data-level="9.6.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#sparsity-inducing-priors"><i class="fa fa-check"></i><b>9.6.2</b> Sparsity-Inducing Priors</a></li>
<li class="chapter" data-level="9.6.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#finnish-horseshoe"><i class="fa fa-check"></i><b>9.6.3</b> Finnish Horseshoe</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#variable-selection"><i class="fa fa-check"></i><b>9.7</b> Variable Selection</a><ul>
<li class="chapter" data-level="9.7.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#projection-based-method"><i class="fa fa-check"></i><b>9.7.1</b> Projection-Based Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html"><i class="fa fa-check"></i><b>10</b> Hierarchical &amp; Multilevel Models</a><ul>
<li class="chapter" data-level="10.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#anova"><i class="fa fa-check"></i><b>10.1</b> ANOVA</a><ul>
<li class="chapter" data-level="10.1.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#frequentist-anova"><i class="fa fa-check"></i><b>10.1.1</b> “Frequentist” ANOVA</a></li>
<li class="chapter" data-level="10.1.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#bayesian-anova"><i class="fa fa-check"></i><b>10.1.2</b> Bayesian ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#multilevel-modeling-mlm"><i class="fa fa-check"></i><b>10.2</b> Multilevel Modeling (MLM)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#examples-of-clustering"><i class="fa fa-check"></i><b>10.2.1</b> Examples of clustering</a></li>
<li class="chapter" data-level="10.2.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#data-1"><i class="fa fa-check"></i><b>10.2.2</b> Data</a></li>
<li class="chapter" data-level="10.2.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#intraclass-correlation"><i class="fa fa-check"></i><b>10.2.3</b> Intraclass correlation</a></li>
<li class="chapter" data-level="10.2.4" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#is-mlm-needed"><i class="fa fa-check"></i><b>10.2.4</b> Is MLM needed?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-coefficients"><i class="fa fa-check"></i><b>10.3</b> Varying Coefficients</a><ul>
<li class="chapter" data-level="10.3.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-intercepts"><i class="fa fa-check"></i><b>10.3.1</b> Varying Intercepts</a></li>
<li class="chapter" data-level="10.3.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-slopes"><i class="fa fa-check"></i><b>10.3.2</b> Varying Slopes</a></li>
<li class="chapter" data-level="10.3.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-sigma"><i class="fa fa-check"></i><b>10.3.3</b> Varying <span class="math inline">\(\sigma\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#model-comparisons"><i class="fa fa-check"></i><b>10.4</b> Model Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>11</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="11.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#basics-of-generalized-linear-models"><i class="fa fa-check"></i><b>11.1</b> Basics of Generalized Linear Models</a></li>
<li class="chapter" data-level="11.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>11.2</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#the-logit-link"><i class="fa fa-check"></i><b>11.2.1</b> The logit link</a></li>
<li class="chapter" data-level="11.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#choice-of-priors"><i class="fa fa-check"></i><b>11.2.2</b> Choice of Priors</a></li>
<li class="chapter" data-level="11.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interpreting-the-coefficients"><i class="fa fa-check"></i><b>11.2.3</b> Interpreting the coefficients</a></li>
<li class="chapter" data-level="11.2.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#model-checking-1"><i class="fa fa-check"></i><b>11.2.4</b> Model Checking</a></li>
<li class="chapter" data-level="11.2.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#complete-separation"><i class="fa fa-check"></i><b>11.2.5</b> Complete Separation</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binomial-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Binomial Logistic Regression</a></li>
<li class="chapter" data-level="11.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#probit-regression"><i class="fa fa-check"></i><b>11.4</b> Probit Regression</a></li>
<li class="chapter" data-level="11.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>11.5</b> Poisson Regression</a><ul>
<li class="chapter" data-level="11.5.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interpretations-2"><i class="fa fa-check"></i><b>11.5.1</b> Interpretations</a></li>
<li class="chapter" data-level="11.5.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#model-checking-2"><i class="fa fa-check"></i><b>11.5.2</b> Model Checking</a></li>
<li class="chapter" data-level="11.5.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#other-models-in-glm"><i class="fa fa-check"></i><b>11.5.3</b> Other models in GLM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="missing-data.html"><a href="missing-data.html"><i class="fa fa-check"></i><b>12</b> Missing Data</a><ul>
<li class="chapter" data-level="12.1" data-path="missing-data.html"><a href="missing-data.html#missing-data-mechanisms"><i class="fa fa-check"></i><b>12.1</b> Missing Data Mechanisms</a><ul>
<li class="chapter" data-level="12.1.1" data-path="missing-data.html"><a href="missing-data.html#mcar-missing-completely-at-random"><i class="fa fa-check"></i><b>12.1.1</b> MCAR (Missing Completely at Random)</a></li>
<li class="chapter" data-level="12.1.2" data-path="missing-data.html"><a href="missing-data.html#mar-missing-at-random"><i class="fa fa-check"></i><b>12.1.2</b> MAR (Missing At Random)</a></li>
<li class="chapter" data-level="12.1.3" data-path="missing-data.html"><a href="missing-data.html#nmar-not-missing-at-random"><i class="fa fa-check"></i><b>12.1.3</b> NMAR (Not Missing At Random)</a></li>
<li class="chapter" data-level="12.1.4" data-path="missing-data.html"><a href="missing-data.html#ignorable-missingness"><i class="fa fa-check"></i><b>12.1.4</b> Ignorable Missingness*</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="missing-data.html"><a href="missing-data.html#bayesian-approaches-for-missing-data"><i class="fa fa-check"></i><b>12.2</b> Bayesian Approaches for Missing Data</a><ul>
<li class="chapter" data-level="12.2.1" data-path="missing-data.html"><a href="missing-data.html#complete-case-analysislistwise-deletion"><i class="fa fa-check"></i><b>12.2.1</b> Complete Case Analysis/Listwise Deletion</a></li>
<li class="chapter" data-level="12.2.2" data-path="missing-data.html"><a href="missing-data.html#treat-missing-data-as-parameters"><i class="fa fa-check"></i><b>12.2.2</b> Treat Missing Data as Parameters</a></li>
<li class="chapter" data-level="12.2.3" data-path="missing-data.html"><a href="missing-data.html#multiple-imputation"><i class="fa fa-check"></i><b>12.2.3</b> Multiple Imputation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Handouts for Bayesian Data Analysis Class</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="one-parameter-models" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> One-Parameter Models</h1>
<p>This lecture surveys some very simple models with one parameter for some real
research data. In your research it is rare to study only one variable with one
parameter, but the concepts you learn in this lecture will naturally generalize
to more regular statistical models, like regression and multilevel models.
Specifically we will consider two models:</p>
<ul>
<li>Binomial/Bernoulli data</li>
<li>Poisson data</li>
</ul>
<div id="binomialbernoulli-data" class="section level2">
<h2><span class="header-section-number">3.1</span> Binomial/Bernoulli data</h2>
<p>You’ve already seen the binomial model in the AIDS example. When you have one
event with two outcomes, like flipping a coin or sampling a respondent and
asking whether he or she has a college degree, the variable is called a
<em>Bernoulli</em> variable. Mathematically we will denote one outcome as success and
coded as 1, and the other as failure and coded as 0 (poor terminology may be,
but that’s by convention). For example, if the variable is coin flipping, we
have 1 = tail, 0 = head. If the variable is whether the respondent has a college
degree, it can be 1 = college or above, 0 = no college. Therefore, in the AIDS
example, each observation is considered a “Bernoulli” outcome (Alive vs. Dead).
An additional thing to note for the Bernoulli/binomial model is that, instead of
setting the prior on <span class="math inline">\(\theta\)</span>, sometimes we’re more interested in setting the
prior for a transformed parameter that has values between <span class="math inline">\(-\infty\)</span> and
<span class="math inline">\(\infty\)</span>, such as one on the <em>logit</em> scale (as related to logistic regression).</p>
<p>Instead of the subsample, here we will use the full sample in the <code>Aids2</code> data
set, with the same informative prior of Beta(46, 34). In the full data, we have</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;Aids2&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;MASS&quot;</span>)</a>
<a class="sourceLine" id="cb34-2" data-line-number="2"><span class="kw">summary</span>(Aids2<span class="op">$</span>status)</a></code></pre></div>
<pre><code>&gt;#    A    D 
&gt;# 1082 1761</code></pre>
<p>So based on the conjugacy, the posterior of <span class="math inline">\(\theta\)</span> is Beta(1,807, 1,116).</p>
<div id="reparameterization" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Reparameterization*</h3>
<p>A problem in choosing an uninformative prior is that the prior is not invariant
to <em>reparameterization</em>. Reparameterization means, instead of estimating the
parameter in one way, we use another parameter that can be expressed as a
function of the parameter we originally have. The easiest example would be for a
normal distribution, you can estimate the variance, or you can estimate the
standard deviation, which is the square root of the variance. Therefore, the
<em>SD</em> can be considered a reparameterization of the variance, and vice versa. In
a Bernoulli/Binomial model, one common reparameterization is to define <span class="math inline">\(\varphi = \log[\theta / (1 - \theta)]\)</span>, which is the <em>logit</em> of <span class="math inline">\(\theta\)</span>.</p>
<p>Here is the relationship between <span class="math inline">\(\varphi\)</span> and <span class="math inline">\(\theta\)</span>:</p>
<p><img src="03_one_parameter_models_files/figure-html/phi-theta-1.png" width="288" /></p>
<p>This reparameterization is popular because it is commonly used in logistic
regression, and is actually a naturally parameter for a binomial distribution
for some theoretical reason. Instead of having a support of [0, 1], it has
a support of (<span class="math inline">\(-\infty\)</span>, <span class="math inline">\(\infty\)</span>), or the whole real number line.</p>
<p>However, a uniform prior on <span class="math inline">\(\theta\)</span> will not transform to a uniform prior
on <span class="math inline">\(\varphi\)</span>. If we take a large sample of <span class="math inline">\(\theta\)</span> from a uniform distribution:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1">th_samples &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="fl">1e5</span>, <span class="dv">0</span>, <span class="dv">1</span>)</a></code></pre></div>
<p>and transform each value to <span class="math inline">\(\varphi\)</span>:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1">phi_samples &lt;-<span class="st"> </span><span class="kw">log</span>(th_samples <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>th_samples))</a></code></pre></div>
<p>The resulting prior on <span class="math inline">\(\varphi\)</span> will be</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">ggplot</span>(<span class="kw">tibble</span>(<span class="dt">phi =</span> phi_samples), <span class="kw">aes</span>(<span class="dt">x =</span> phi)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">bw =</span> <span class="st">&quot;SJ&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-3" data-line-number="3"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(varphi))</a></code></pre></div>
<p><img src="03_one_parameter_models_files/figure-html/prior-phi-1.png" width="288" /></p>
<p>which assigns more density to <span class="math inline">\(\varphi = 0\)</span>. Although generally you will get
very similar results using either a non-informative prior on <span class="math inline">\(\theta\)</span> or
on the logit <span class="math inline">\(\varphi\)</span>, this non-invariance is one major critique on Bayesian
from frequentist, especially the use of non-informative prior.</p>
<p>The graph below shows, in black, the posterior of <span class="math inline">\(\varphi\)</span> for a uniform prior
on <span class="math inline">\(\theta\)</span> for the subsample of size 10, and, in red, the posterior of
<span class="math inline">\(\varphi\)</span> for a <span class="math inline">\(\mathcal{N}(0, 10)\)</span> prior on <span class="math inline">\(\varphi\)</span>. They match pretty well.</p>
<p><img src="03_one_parameter_models_files/figure-html/m1-density-1.png" width="480" /></p>
<hr />
</div>
<div id="posterior-predictive-check-1" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Posterior Predictive Check</h3>
<p>Now, we need to know whether the model fit the data well. If we only have the
<code>status</code> variable we don’t have much to check for a binomial model. However, as
there is information for other variables, we can use them to check the
exchangeability assumption. For example, we can ask whether the data from
different state categories are exchangeable. The death rate across the 4 state
categories are</p>
<pre><code>&gt;#        status
&gt;# state      A    D
&gt;#   NSW    664 1116
&gt;#   Other  107  142
&gt;#   QLD     78  148
&gt;#   VIC    233  355</code></pre>
<pre><code>&gt;#        status
&gt;# state       A     D
&gt;#   NSW   0.373 0.627
&gt;#   Other 0.430 0.570
&gt;#   QLD   0.345 0.655
&gt;#   VIC   0.396 0.604</code></pre>
<p>Now, we can generate some predictions from our posterior distribution and our
model.</p>
<p><img src="03_one_parameter_models_files/figure-html/ppd-1-1.png" width="672" /></p>
<p>So the observed data (the first subgraph) look similar to the simulated data. We
can also conduct a posterior predictive check by defining a test statistic, here
we will be using the difference between the highest death rate and the lowest
death rate across the 4 state categories:</p>
<pre><code>&gt;# `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="03_one_parameter_models_files/figure-html/ppc-max-min-1.png" width="384" /></p>
<p>The posterior predictive <em>p</em>-value (<em>ppp</em>) using a test statistic of the ratio
between the highest death rate and the lowest death rate among the 4 state
categories is 0.094.
<em>ppp</em> is defined as the probability of drawing a sample
from the posterior predictive distribution with a test statistic that is as high
as the observed sample. Mathematically, if we define the test statistic
as <span class="math inline">\(T(y)\)</span>, it is represented as <span class="math inline">\(\mathit{ppp} = P(T(\tilde y) &gt; T(y) | y)\)</span>},
and it’s conditional on <span class="math inline">\(y\)</span> because it is based on the posterior distribution.</p>
<p>We generally do not apply the same cutoff of <span class="math inline">\(p &lt; .05\)</span> as in frequentist
statistics, which is a convention under strong criticism. Instead, what
Bayesian strives for would be to get a posterior predictive <span class="math inline">\(p\)</span>-value close to
.50 for the key test statistic. Therefore, given the check, we should consider
modeling the data as four different states, or better by using a hierarchical
model.</p>
</div>
<div id="comparison-to-frequentist-results" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Comparison to frequentist results</h3>
<p>Using maximum likelihood, the estimated death rate would be <span class="math inline">\(\hat \theta = 1761 / 2843 = 0.619\)</span>, with a standard error (<em>SE</em>) of
<span class="math inline">\(\sqrt{0.619 (1 - 0.619) / n} = 0.009\)</span>, with a 90% confidence interval of <span class="math inline">\([0.604, 0.634]\)</span>, which is similar to the interval with
Bayesian inference.</p>
</div>
<div id="sensitivity-to-different-priors" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Sensitivity to different priors</h3>
<p><img src="03_one_parameter_models_files/figure-html/sensitivity-1.png" width="672" /></p>
<p>You can see one needs a very strong prior (equivalent to 600 data points)
and with the prior and the data not agreeing to get a substantially different
conclusion.</p>
<hr />
<blockquote>
<p>The <span class="math inline">\(\mathrm{Beta}(1 / 2, 1 / 2)\)</span> distribution is called a
<em>Jeffreys prior</em> (<a href="https://en.wikipedia.org/wiki/Jeffreys_prior" class="uri">https://en.wikipedia.org/wiki/Jeffreys_prior</a>),
which is derived according to some statistical principles for different models.
One big advantage of a Jeffreys prior is that it is <strong>invariant</strong>,
meaningful that the prior will stay the same even under reparameterization.
However, like conjugate priors, Jeffreys prior limit the choice of prior even
when there are true prior information available.</p>
</blockquote>
<hr />
</div>
</div>
<div id="poisson-data" class="section level2">
<h2><span class="header-section-number">3.2</span> Poisson Data</h2>
<p>A Poisson distribution is suitable for count data in a fixed interval, when
the average rate of the event happening is constant, and when the time from
one occurrence of the event to the next is independent. For example, think about
the number of e-mails for a person each day. If we know on average a person has
20 emails a day, and we assume that emails from different sources arrive
independently, then we can model the number of emails in different days as
a Poisson distribution.</p>
<p>The Poisson distribution has one rate parameter, usually denote as <span class="math inline">\(\lambda\)</span>,
which is also the mean of a Poisson distribution. Below are a few examples of
Poisson distributions with different <span class="math inline">\(\lambda\)</span>:</p>
<p><img src="03_one_parameter_models_files/figure-html/Pois-dist-1.png" width="432" /></p>
<p>As you can see, the larger <span class="math inline">\(\lambda\)</span> is, the closer the distribution is to
a normal distribution.</p>
<p>One important property of a Poisson distribution is that the mean and the
variance is always the same. Therefore, if you have a Poisson distribution
with <span class="math inline">\(\lambda = 3\)</span>, its variance must be <span class="math inline">\(3\)</span>, and so its <em>SD</em> is <span class="math inline">\(\sqrt{3} = 1.732\)</span>. This will be an important property to check the fit of a
Poisson model.</p>
<div id="example-2" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Example 2</h3>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="co"># Download data file</span></a>
<a class="sourceLine" id="cb42-2" data-line-number="2"><span class="kw">download.file</span>(<span class="st">&quot;https://files.osf.io/v1/resources/47tnc/providers/osfstorage/553e51f98c5e4a21991987e7?action=download&amp;version=1&amp;direct&quot;</span>, <span class="st">&quot;../data/redcard_data.zip&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1">redcard_dat &lt;-<span class="st"> </span>readr<span class="op">::</span><span class="kw">read_csv</span>(<span class="st">&quot;../data/redcard_data.zip&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb43-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(leagueCountry <span class="op">==</span><span class="st"> &quot;England&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb43-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(player) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb43-4" data-line-number="4"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">rater_dark =</span> (<span class="kw">mean</span>(rater1) <span class="op">+</span><span class="st"> </span><span class="kw">mean</span>(rater2)) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb43-5" data-line-number="5">            <span class="dt">yellowCards =</span> <span class="kw">sum</span>(yellowCards), </a>
<a class="sourceLine" id="cb43-6" data-line-number="6">            <span class="dt">redCards =</span> <span class="kw">sum</span>(redCards))</a></code></pre></div>
<p>We will use a data set by <span class="citation">(Silberzahn et al. <a href="#ref-silberzahn2018many">2018</a>)</span>, which the authors sent out to
29 research teams for analyses (read the paper for more information on the
context). We will use a subset of the data, which contains information of
564 soccer players playing in England, and the total number of
red cards (which happens when they committed a foul deemed serious by the
referee of the match and will be sent off the match). Below is a summary and a
plot of the data, and a theoretical Poisson distribution that matches the mean
of the data (i.e., 0.9.</p>
<pre><code>&gt;#     player            rater_dark   yellowCards       redCards  
&gt;#  Length:564         Min.   :0.0   Min.   :  0.0   Min.   :0.0  
&gt;#  Class :character   1st Qu.:0.0   1st Qu.:  9.0   1st Qu.:0.0  
&gt;#  Mode  :character   Median :0.2   Median : 22.0   Median :0.0  
&gt;#                     Mean   :0.3   Mean   : 27.8   Mean   :0.9  
&gt;#                     3rd Qu.:0.5   3rd Qu.: 41.0   3rd Qu.:1.0  
&gt;#                     Max.   :1.0   Max.   :146.0   Max.   :7.0  
&gt;#                     NA&#39;s   :178</code></pre>
<p><img src="03_one_parameter_models_files/figure-html/plot-redcard_dat-1.png" width="480" /></p>
<p>As you can see, the Poisson distribution describes the skewness of the data
reasonably well, but there are more zeros in the data than what the Poisson
would expect. However, we will proceed with the Poisson model first in this
lecture, and we may deal with the extra zeros later in the cocurse.</p>
</div>
<div id="choosing-a-model-1" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Choosing a model</h3>
<p>With exchangeability, the model will be</p>
<p><span class="math display">\[\mathtt{redCards}_i \sim \mathrm{Pois}(\lambda)\]</span></p>
</div>
<div id="choosing-a-prior" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Choosing a prior</h3>
<p>A conjugate prior for the Poisson model is the Gamma distribution family, which
has two parameters: <span class="math inline">\(a\)</span> is the “shape” parameter, and <span class="math inline">\(b\)</span> is the “scale”
parameter. Notice that <span class="math inline">\(\lambda\)</span> has a support (i.e., acceptable values) of <span class="math inline">\([0, \infty)\)</span>. <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> have (roughly) the following interpretations:</p>
<ul>
<li><span class="math inline">\(b\)</span>: Number of prior observations</li>
<li><span class="math inline">\(a\)</span>: Sum of the counts in prior observations</li>
</ul>
<p>However, with the improvement in computational speed, the need to use a
conjugate prior becomes relatively small, and so the for this example I will
show you the use of a non-conjugate prior. A general strategy to using a weakly
informative prior is to first transform the parameter into one with a support
of <span class="math inline">\([-\infty, \infty]\)</span>, and the natural log transformation is common for the
Poisson rate parameter.</p>
</div>
<div id="model-equations-and-diagram" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Model Equations and Diagram</h3>
<p>Our Poisson model and prior are thus</p>
<p><span class="math display">\[\begin{align}
  \mathtt{redCards}_i &amp; \sim \mathrm{Pois}(\lambda) \\
  \log(\lambda) &amp; \sim \mathcal{N}(0, \sigma_{\log(\lambda)}),
\end{align}\]</span></p>
<p>where <span class="math inline">\(\sigma_{\log(\lambda)}\)</span> is the standard deviation (not variance) of the
normal prior, which controls the informativeness of our prior. In this case,
<span class="math inline">\(\log(\lambda)\)</span> = 2.5 would correspond to <span class="math inline">\(\lambda\)</span> = 12.182 and
<span class="math inline">\(\log(\lambda)\)</span> = -2.5 would correspond to <span class="math inline">\(\lambda\)</span> = 0.082, and it
seems unlikely for <span class="math inline">\(\lambda\)</span> to be outside of this range. Therefore, I will
choose <span class="math inline">\(\sigma_{\log(\lambda)}\)</span> = 2.5.</p>
<p><font color="red">The priors are plotted below:</font></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1">p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">tibble</span>(<span class="dt">log_lambda =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)), <span class="kw">aes</span>(<span class="dt">x =</span> log_lambda)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb45-2" data-line-number="2"><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">2.5</span>)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb45-3" data-line-number="3"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(<span class="kw">log</span>(lambda)), <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>)</a>
<a class="sourceLine" id="cb45-4" data-line-number="4">p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">tibble</span>(<span class="dt">lambda =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">20</span>)), <span class="kw">aes</span>(<span class="dt">x =</span> lambda)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb45-5" data-line-number="5"><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> <span class="cf">function</span>(x) <span class="kw">dnorm</span>(<span class="kw">log</span>(x), <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">2.5</span>), </a>
<a class="sourceLine" id="cb45-6" data-line-number="6">                <span class="dt">n =</span> <span class="dv">501</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb45-7" data-line-number="7"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(lambda), <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>)</a>
<a class="sourceLine" id="cb45-8" data-line-number="8">gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2, <span class="dt">nrow =</span> <span class="dv">1</span>)</a></code></pre></div>
<p><img src="03_one_parameter_models_files/figure-html/gamma-dist-1.png" width="75%" /></p>
<p>Here is a graphical sketch of the model and the prior using the R package
<code>DiagrammeR</code>:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1">DiagrammeR<span class="op">::</span><span class="kw">grViz</span>(<span class="st">&quot;</span></a>
<a class="sourceLine" id="cb46-2" data-line-number="2"><span class="st">digraph boxes_and_circles {</span></a>
<a class="sourceLine" id="cb46-3" data-line-number="3"></a>
<a class="sourceLine" id="cb46-4" data-line-number="4"><span class="st">  # a &#39;graph&#39; statement</span></a>
<a class="sourceLine" id="cb46-5" data-line-number="5"><span class="st">  graph [overlap = true, fontsize = 10]</span></a>
<a class="sourceLine" id="cb46-6" data-line-number="6"></a>
<a class="sourceLine" id="cb46-7" data-line-number="7"><span class="st">  # data</span></a>
<a class="sourceLine" id="cb46-8" data-line-number="8"><span class="st">  node [shape = box, fixedsize = true]</span></a>
<a class="sourceLine" id="cb46-9" data-line-number="9"><span class="st">  Y [label = &#39;y@_{i}&#39;]</span></a>
<a class="sourceLine" id="cb46-10" data-line-number="10"><span class="st">  </span></a>
<a class="sourceLine" id="cb46-11" data-line-number="11"><span class="st">  # parameters</span></a>
<a class="sourceLine" id="cb46-12" data-line-number="12"><span class="st">  node [shape = circle]</span></a>
<a class="sourceLine" id="cb46-13" data-line-number="13"><span class="st">  loglambda [label = &#39;log(&amp;lambda;)&#39;]</span></a>
<a class="sourceLine" id="cb46-14" data-line-number="14"><span class="st">  </span></a>
<a class="sourceLine" id="cb46-15" data-line-number="15"><span class="st">  # transformed parameters</span></a>
<a class="sourceLine" id="cb46-16" data-line-number="16"><span class="st">  node [shape = circle, peripheries = 2]</span></a>
<a class="sourceLine" id="cb46-17" data-line-number="17"><span class="st">  lambda [label = &#39;&amp;lambda;&#39;]</span></a>
<a class="sourceLine" id="cb46-18" data-line-number="18"><span class="st">  </span></a>
<a class="sourceLine" id="cb46-19" data-line-number="19"><span class="st">  # fixed values in prior</span></a>
<a class="sourceLine" id="cb46-20" data-line-number="20"><span class="st">  node [shape = circle, peripheries = 1]</span></a>
<a class="sourceLine" id="cb46-21" data-line-number="21"><span class="st">  mu_loglambda [label = &#39;&amp;mu;@_{log(&amp;lambda;)}&#39;];</span></a>
<a class="sourceLine" id="cb46-22" data-line-number="22"><span class="st">  sigma_loglambda [label = &#39;&amp;sigma;@_{log(&amp;lambda;)}&#39;]</span></a>
<a class="sourceLine" id="cb46-23" data-line-number="23"></a>
<a class="sourceLine" id="cb46-24" data-line-number="24"><span class="st">  # paths</span></a>
<a class="sourceLine" id="cb46-25" data-line-number="25"><span class="st">  mu_loglambda -&gt; loglambda;</span></a>
<a class="sourceLine" id="cb46-26" data-line-number="26"><span class="st">  sigma_loglambda -&gt; loglambda;</span></a>
<a class="sourceLine" id="cb46-27" data-line-number="27"><span class="st">  loglambda -&gt; lambda;</span></a>
<a class="sourceLine" id="cb46-28" data-line-number="28"><span class="st">  lambda -&gt; Y;</span></a>
<a class="sourceLine" id="cb46-29" data-line-number="29"><span class="st">}</span></a>
<a class="sourceLine" id="cb46-30" data-line-number="30"><span class="st">&quot;</span>)</a></code></pre></div>
<div id="htmlwidget-a4f1c79fa0875741aeb2" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-a4f1c79fa0875741aeb2">{"x":{"diagram":"\ndigraph boxes_and_circles {\n\n  # a \"graph\" statement\n  graph [overlap = true, fontsize = 10]\n\n  # data\n  node [shape = box, fixedsize = true]\n  Y [label = <y<FONT POINT-SIZE=\"8\"><SUB>i<\/SUB><\/FONT>>]\n  \n  # parameters\n  node [shape = circle]\n  loglambda [label = \"log(&lambda;)\"]\n  \n  # transformed parameters\n  node [shape = circle, peripheries = 2]\n  lambda [label = \"&lambda;\"]\n  \n  # fixed values in prior\n  node [shape = circle, peripheries = 1]\n  mu_loglambda [label = <&mu;<FONT POINT-SIZE=\"8\"><SUB>log(&lambda;)<\/SUB><\/FONT>>];\n  sigma_loglambda [label = <&sigma;<FONT POINT-SIZE=\"8\"><SUB>log(&lambda;)<\/SUB><\/FONT>>]\n\n  # paths\n  mu_loglambda -> loglambda;\n  sigma_loglambda -> loglambda;\n  loglambda -> lambda;\n  lambda -> Y;\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>Note that there currently is not a widely accepted standard for drawing these
kinds of graphs, but as long as you have a reasonable and consistent way to
represent them it should be fine. The double bordered node is used to show a
deterministic node (i.e., a transformation of some variables).</p>
</div>
<div id="getting-the-posterior" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Getting the posterior</h3>
<p>With a non-conjugate prior for a Poisson model, we will need to use MCMC. Below
is the STAN code and the R code for running the model with our prior. We’ll talk
more about STAN in later weeks; for now, just use the code to run the example.</p>
<pre class="stan"><code>data {
  int&lt;lower=0&gt; N;  // number of observations
  int&lt;lower=0&gt; y[N];  // data array (counts);
}
parameters {
  real log_lambda;  // log of rate parameter
}
model {
  y ~ poisson_log(log_lambda);
  // prior
  log_lambda ~ normal(0, 5);
}
generated quantities {
  real lambda = exp(log_lambda);
  int yrep[N];
  for (i in 1:N) {
    yrep[i] = poisson_log_rng(log_lambda);
  }
}</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1"><span class="kw">library</span>(rstan)</a>
<a class="sourceLine" id="cb48-2" data-line-number="2"><span class="kw">rstan_options</span>(<span class="dt">auto_write =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb48-3" data-line-number="3">m2 &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&quot;../codes/poisson_model.stan&quot;</span>, </a>
<a class="sourceLine" id="cb48-4" data-line-number="4">           <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> <span class="dv">564</span>, <span class="dt">y =</span> redcard_dat<span class="op">$</span>redCards), </a>
<a class="sourceLine" id="cb48-5" data-line-number="5">           <span class="dt">iter =</span> <span class="dv">800</span>, <span class="dt">chains =</span> <span class="dv">2</span>, <span class="dt">cores =</span> <span class="dv">2</span>)</a></code></pre></div>
<p>Below is a summary of the posterior distributions</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1"><span class="kw">print</span>(m2, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;log_lambda&quot;</span>, <span class="st">&quot;lambda&quot;</span>))</a></code></pre></div>
<pre><code>&gt;# Inference for Stan model: poisson_model.
&gt;# 2 chains, each with iter=800; warmup=400; thin=1; 
&gt;# post-warmup draws per chain=400, total post-warmup draws=800.
&gt;# 
&gt;#             mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
&gt;# log_lambda -0.11       0 0.04 -0.19 -0.14 -0.11 -0.07 -0.02   282    1
&gt;# lambda      0.90       0 0.04  0.82  0.87  0.90  0.93  0.98   287    1
&gt;# 
&gt;# Samples were drawn using NUTS(diag_e) at Sat Dec 14 16:06:00 2019.
&gt;# For each parameter, n_eff is a crude measure of effective sample size,
&gt;# and Rhat is the potential scale reduction factor on split chains (at 
&gt;# convergence, Rhat=1).</code></pre>
<p>The posterior density and the 95% CI (credible interval) for <span class="math inline">\(\lambda\)</span> is</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1">bayesplot<span class="op">::</span><span class="kw">mcmc_areas</span>(m2, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;log_lambda&quot;</span>, <span class="st">&quot;lambda&quot;</span>), <span class="dt">prob =</span> <span class="fl">0.95</span>)</a></code></pre></div>
<p><img src="03_one_parameter_models_files/figure-html/m2_plot_areas-1.png" width="672" /></p>
<p>The frequentist maximum likelihood estimate for <span class="math inline">\(\lambda\)</span> is the mean of the
data, which is <span class="math inline">\(\hat \lambda = 0.897\)</span>, and
the <em>SE</em> is the square root of the sample mean, 0.04, with a 90% confidence interval [0.819, 0.975]. So the Bayesian results are
similar, with a slightly more precise inference (slightly smaller <em>SE</em>).</p>
</div>
<div id="posterior-predictive-check-2" class="section level3">
<h3><span class="header-section-number">3.2.6</span> Posterior Predictive Check</h3>
<p>As now you should know, always generate some simulated predictions from the
model to check your results.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb52-1" data-line-number="1">bayesplot<span class="op">::</span><span class="kw">ppc_bars</span>(redcard_dat<span class="op">$</span>redCards, </a>
<a class="sourceLine" id="cb52-2" data-line-number="2">                    <span class="dt">yrep =</span> <span class="kw">as.matrix</span>(m2, <span class="dt">pars =</span> <span class="st">&quot;yrep&quot;</span>))</a></code></pre></div>
<p><img src="03_one_parameter_models_files/figure-html/ppc-redcard-1-1.png" width="432" /></p>
<p>There’s also a useful graphical tool, rootogram, for diagnosing count models</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1">bayesplot<span class="op">::</span><span class="kw">ppc_rootogram</span>(redcard_dat<span class="op">$</span>redCards, </a>
<a class="sourceLine" id="cb53-2" data-line-number="2">                         <span class="dt">yrep =</span> <span class="kw">as.matrix</span>(m2, <span class="dt">pars =</span> <span class="st">&quot;yrep&quot;</span>), </a>
<a class="sourceLine" id="cb53-3" data-line-number="3">                         <span class="dt">style =</span> <span class="st">&quot;hanging&quot;</span>)</a></code></pre></div>
<p><img src="03_one_parameter_models_files/figure-html/rootogram-redcard-1.png" width="432" /></p>
<p>As can be seen, the predicted counts were off a little bit. Things that can
improve the model includes:</p>
<ol style="list-style-type: decimal">
<li>Using a different distribution than Poisson;</li>
<li>Using a zero-inflated Poisson as there were more zeros in the data;</li>
<li>Including predictors in a Poisson regression;</li>
<li>Adjusting for the number of games each player played.</li>
</ol>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-silberzahn2018many">
<p>Silberzahn, Raphael, Eric L Uhlmann, Daniel P Martin, Pasquale Anselmi, Frederik Aust, Eli Awtrey, Štěpán Bahnı'k, et al. 2018. “Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results.” <em>Advances in Methods and Practices in Psychological Science</em> 1 (3). Sage Publications Sage CA: Los Angeles, CA: 337–56.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="brief-introduction-to-stan.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["notes_bookdown.pdf", "notes_bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | Course Handouts for Bayesian Data Analysis Class</title>
  <meta name="description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Fall semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | Course Handouts for Bayesian Data Analysis Class" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Fall semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | Course Handouts for Bayesian Data Analysis Class" />
  
  <meta name="twitter:description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Fall semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  

<meta name="author" content="Mark Lai" />


<meta name="date" content="2020-06-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="bayesian-inference.html"/>
<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PSYC 621 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#history-of-bayesian-statistics"><i class="fa fa-check"></i><b>1.1</b> History of Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#thomas-bayes-17011762"><i class="fa fa-check"></i><b>1.1.1</b> Thomas Bayes (1701–1762)</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#pierre-simon-laplace-17491827"><i class="fa fa-check"></i><b>1.1.2</b> Pierre-Simon Laplace (1749–1827)</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction.html"><a href="introduction.html#th-century"><i class="fa fa-check"></i><b>1.1.3</b> 20th Century</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#motivations-for-using-bayesian-methods"><i class="fa fa-check"></i><b>1.2</b> Motivations for Using Bayesian Methods</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#problem-with-classical-frequentist-statistics"><i class="fa fa-check"></i><b>1.2.1</b> Problem with classical (frequentist) statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#probability"><i class="fa fa-check"></i><b>1.3</b> Probability</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#classical-interpretation"><i class="fa fa-check"></i><b>1.3.1</b> Classical Interpretation</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#frequentist-interpretation"><i class="fa fa-check"></i><b>1.3.2</b> Frequentist Interpretation</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#problem-of-the-single-case"><i class="fa fa-check"></i><b>1.3.3</b> Problem of the single case</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#subjectivist-interpretation"><i class="fa fa-check"></i><b>1.3.4</b> Subjectivist Interpretation</a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#basics-of-probability"><i class="fa fa-check"></i><b>1.3.5</b> Basics of Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#bayess-theorem"><i class="fa fa-check"></i><b>1.4</b> Bayes’s Theorem</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#example-1-base-rate-fallacy-from-wikipedia"><i class="fa fa-check"></i><b>1.4.1</b> Example 1: Base rate fallacy (From Wikipedia)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#bayesian-statistics"><i class="fa fa-check"></i><b>1.5</b> Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#example-2-locating-a-plane"><i class="fa fa-check"></i><b>1.5.1</b> Example 2: Locating a Plane</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#comparing-bayesian-and-frequentist-statistics"><i class="fa fa-check"></i><b>1.6</b> Comparing Bayesian and Frequentist Statistics</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#software-for-bayesian-statistics"><i class="fa fa-check"></i><b>1.7</b> Software for Bayesian Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>2</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Steps of Bayesian Data Analysis</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#real-data-example"><i class="fa fa-check"></i><b>2.2</b> Real Data Example</a></li>
<li class="chapter" data-level="2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#choosing-a-model"><i class="fa fa-check"></i><b>2.3</b> Choosing a Model</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#exchangeability"><i class="fa fa-check"></i><b>2.3.1</b> Exchangeability*</a></li>
<li class="chapter" data-level="2.3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#probability-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="2.3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-likelihood"><i class="fa fa-check"></i><b>2.3.3</b> The Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#specifying-priors"><i class="fa fa-check"></i><b>2.4</b> Specifying Priors</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#beta-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#obtain-the-posterior-distributions"><i class="fa fa-check"></i><b>2.5</b> Obtain the Posterior Distributions</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#grid-approximation"><i class="fa fa-check"></i><b>2.5.1</b> Grid Approximation</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#using-conjugate-priors"><i class="fa fa-check"></i><b>2.5.2</b> Using Conjugate Priors</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#laplace-approximation-with-maximum-a-posteriori-estimation"><i class="fa fa-check"></i><b>2.5.3</b> Laplace Approximation with Maximum A Posteriori Estimation</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#markov-chain-monte-carlo-mcmc"><i class="fa fa-check"></i><b>2.5.4</b> Markov Chain Monte Carlo (MCMC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summarizing-the-posterior-distribution"><i class="fa fa-check"></i><b>2.6</b> Summarizing the Posterior Distribution</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-mean-median-and-mode"><i class="fa fa-check"></i><b>2.6.1</b> Posterior Mean, Median, and Mode</a></li>
<li class="chapter" data-level="2.6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#uncertainty-estimates"><i class="fa fa-check"></i><b>2.6.2</b> Uncertainty Estimates</a></li>
<li class="chapter" data-level="2.6.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>2.6.3</b> Credible Intervals</a></li>
<li class="chapter" data-level="2.6.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#probability-of-theta-higherlower-than-a-certain-value"><i class="fa fa-check"></i><b>2.6.4</b> Probability of <span class="math inline">\(\theta\)</span> Higher/Lower Than a Certain Value</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#model-checking"><i class="fa fa-check"></i><b>2.7</b> Model Checking</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>2.7.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.8</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="2.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summary"><i class="fa fa-check"></i><b>2.9</b> Summary</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#key-concepts"><i class="fa fa-check"></i><b>2.9.1</b> Key Concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="one-parameter-models.html"><a href="one-parameter-models.html"><i class="fa fa-check"></i><b>3</b> One-Parameter Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#binomialbernoulli-data"><i class="fa fa-check"></i><b>3.1</b> Binomial/Bernoulli data</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#reparameterization"><i class="fa fa-check"></i><b>3.1.1</b> Reparameterization*</a></li>
<li class="chapter" data-level="3.1.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#posterior-predictive-check-1"><i class="fa fa-check"></i><b>3.1.2</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="3.1.3" data-path="one-parameter-models.html"><a href="one-parameter-models.html#comparison-to-frequentist-results"><i class="fa fa-check"></i><b>3.1.3</b> Comparison to frequentist results</a></li>
<li class="chapter" data-level="3.1.4" data-path="one-parameter-models.html"><a href="one-parameter-models.html#sensitivity-to-different-priors"><i class="fa fa-check"></i><b>3.1.4</b> Sensitivity to different priors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#poisson-data"><i class="fa fa-check"></i><b>3.2</b> Poisson Data</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#example-2"><i class="fa fa-check"></i><b>3.2.1</b> Example 2</a></li>
<li class="chapter" data-level="3.2.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#choosing-a-model-1"><i class="fa fa-check"></i><b>3.2.2</b> Choosing a model</a></li>
<li class="chapter" data-level="3.2.3" data-path="one-parameter-models.html"><a href="one-parameter-models.html#choosing-a-prior"><i class="fa fa-check"></i><b>3.2.3</b> Choosing a prior</a></li>
<li class="chapter" data-level="3.2.4" data-path="one-parameter-models.html"><a href="one-parameter-models.html#model-equations-and-diagram"><i class="fa fa-check"></i><b>3.2.4</b> Model Equations and Diagram</a></li>
<li class="chapter" data-level="3.2.5" data-path="one-parameter-models.html"><a href="one-parameter-models.html#getting-the-posterior"><i class="fa fa-check"></i><b>3.2.5</b> Getting the posterior</a></li>
<li class="chapter" data-level="3.2.6" data-path="one-parameter-models.html"><a href="one-parameter-models.html#posterior-predictive-check-2"><i class="fa fa-check"></i><b>3.2.6</b> Posterior Predictive Check</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html"><i class="fa fa-check"></i><b>4</b> Brief Introduction to STAN</a>
<ul>
<li class="chapter" data-level="4.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#stan"><i class="fa fa-check"></i><b>4.1</b> <code>STAN</code></a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#stan-code"><i class="fa fa-check"></i><b>4.1.1</b> <code>STAN</code> code</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#rstan"><i class="fa fa-check"></i><b>4.2</b> <code>RStan</code></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#assembling-data-list-in-r"><i class="fa fa-check"></i><b>4.2.1</b> Assembling data list in R</a></li>
<li class="chapter" data-level="4.2.2" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#call-rstan"><i class="fa fa-check"></i><b>4.2.2</b> Call <code>rstan</code></a></li>
<li class="chapter" data-level="4.2.3" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#summarize-the-results"><i class="fa fa-check"></i><b>4.2.3</b> Summarize the results</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#resources"><i class="fa fa-check"></i><b>4.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="group-comparisons.html"><a href="group-comparisons.html"><i class="fa fa-check"></i><b>5</b> Group Comparisons</a>
<ul>
<li class="chapter" data-level="5.1" data-path="group-comparisons.html"><a href="group-comparisons.html#data"><i class="fa fa-check"></i><b>5.1</b> Data</a></li>
<li class="chapter" data-level="5.2" data-path="group-comparisons.html"><a href="group-comparisons.html#between-subject-comparisons"><i class="fa fa-check"></i><b>5.2</b> Between-Subject Comparisons</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="group-comparisons.html"><a href="group-comparisons.html#plots"><i class="fa fa-check"></i><b>5.2.1</b> Plots</a></li>
<li class="chapter" data-level="5.2.2" data-path="group-comparisons.html"><a href="group-comparisons.html#independent-sample-t-test"><i class="fa fa-check"></i><b>5.2.2</b> Independent sample t-test</a></li>
<li class="chapter" data-level="5.2.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesian-normal-model"><i class="fa fa-check"></i><b>5.2.3</b> Bayesian Normal Model</a></li>
<li class="chapter" data-level="5.2.4" data-path="group-comparisons.html"><a href="group-comparisons.html#robust-model"><i class="fa fa-check"></i><b>5.2.4</b> Robust Model</a></li>
<li class="chapter" data-level="5.2.5" data-path="group-comparisons.html"><a href="group-comparisons.html#shifted-lognormal-model"><i class="fa fa-check"></i><b>5.2.5</b> Shifted Lognormal Model*</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="group-comparisons.html"><a href="group-comparisons.html#notes-on-model-comparison"><i class="fa fa-check"></i><b>5.3</b> Notes on Model Comparison</a></li>
<li class="chapter" data-level="5.4" data-path="group-comparisons.html"><a href="group-comparisons.html#within-subject-comparisons"><i class="fa fa-check"></i><b>5.4</b> Within-Subject Comparisons</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="group-comparisons.html"><a href="group-comparisons.html#plots-1"><i class="fa fa-check"></i><b>5.4.1</b> Plots</a></li>
<li class="chapter" data-level="5.4.2" data-path="group-comparisons.html"><a href="group-comparisons.html#independent-sample-t-test-1"><i class="fa fa-check"></i><b>5.4.2</b> Independent sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="5.4.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesian-normal-model-1"><i class="fa fa-check"></i><b>5.4.3</b> Bayesian Normal Model</a></li>
<li class="chapter" data-level="5.4.4" data-path="group-comparisons.html"><a href="group-comparisons.html#using-brms"><i class="fa fa-check"></i><b>5.4.4</b> Using <code>brms</code>*</a></li>
<li class="chapter" data-level="5.4.5" data-path="group-comparisons.html"><a href="group-comparisons.html#region-of-practical-equivalence-rope"><i class="fa fa-check"></i><b>5.4.5</b> Region of Practical Equivalence (ROPE)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="6.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#monte-carlo-simulation-with-one-unknown"><i class="fa fa-check"></i><b>6.1</b> Monte Carlo Simulation With One Unknown</a></li>
<li class="chapter" data-level="6.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo-mcmc-with-one-parameter"><i class="fa fa-check"></i><b>6.2</b> Markov Chain Monte Carlo (MCMC) With One Parameter</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> The Metropolis algorithm</a></li>
<li class="chapter" data-level="6.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-metropolis-hastings-algorithm"><i class="fa fa-check"></i><b>6.2.2</b> The Metropolis-Hastings Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain"><i class="fa fa-check"></i><b>6.3</b> Markov Chain</a></li>
<li class="chapter" data-level="6.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#effective-sample-size-n_texteff"><i class="fa fa-check"></i><b>6.4</b> Effective Sample Size (<span class="math inline">\(n_\text{eff}\)</span>)</a></li>
<li class="chapter" data-level="6.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mc-error"><i class="fa fa-check"></i><b>6.5</b> MC Error</a></li>
<li class="chapter" data-level="6.6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#burn-inwarmup"><i class="fa fa-check"></i><b>6.6</b> Burn-in/Warmup</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#thinning"><i class="fa fa-check"></i><b>6.6.1</b> Thinning</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#diagnostics-of-mcmc"><i class="fa fa-check"></i><b>6.7</b> Diagnostics of MCMC</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mixing"><i class="fa fa-check"></i><b>6.7.1</b> Mixing</a></li>
<li class="chapter" data-level="6.7.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#acceptance-rate"><i class="fa fa-check"></i><b>6.7.2</b> Acceptance Rate</a></li>
<li class="chapter" data-level="6.7.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#diagnostics-using-multiple-chains"><i class="fa fa-check"></i><b>6.7.3</b> Diagnostics Using Multiple Chains</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#multiple-parameters"><i class="fa fa-check"></i><b>6.8</b> Multiple Parameters</a></li>
<li class="chapter" data-level="6.9" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>6.9</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>7</b> Linear Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="linear-models.html"><a href="linear-models.html#what-is-regression"><i class="fa fa-check"></i><b>7.1</b> What is Regression?</a></li>
<li class="chapter" data-level="7.2" data-path="linear-models.html"><a href="linear-models.html#one-predictor"><i class="fa fa-check"></i><b>7.2</b> One Predictor</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="linear-models.html"><a href="linear-models.html#a-continuous-predictor"><i class="fa fa-check"></i><b>7.2.1</b> A continuous predictor</a></li>
<li class="chapter" data-level="7.2.2" data-path="linear-models.html"><a href="linear-models.html#centering"><i class="fa fa-check"></i><b>7.2.2</b> Centering</a></li>
<li class="chapter" data-level="7.2.3" data-path="linear-models.html"><a href="linear-models.html#a-categorical-predictor"><i class="fa fa-check"></i><b>7.2.3</b> A categorical predictor</a></li>
<li class="chapter" data-level="7.2.4" data-path="linear-models.html"><a href="linear-models.html#predictors-with-multiple-categories"><i class="fa fa-check"></i><b>7.2.4</b> Predictors with multiple categories</a></li>
<li class="chapter" data-level="7.2.5" data-path="linear-models.html"><a href="linear-models.html#stan-4"><i class="fa fa-check"></i><b>7.2.5</b> STAN</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="linear-models.html"><a href="linear-models.html#multiple-regression"><i class="fa fa-check"></i><b>7.3</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="linear-models.html"><a href="linear-models.html#two-predictor-example"><i class="fa fa-check"></i><b>7.3.1</b> Two Predictor Example</a></li>
<li class="chapter" data-level="7.3.2" data-path="linear-models.html"><a href="linear-models.html#interactions"><i class="fa fa-check"></i><b>7.3.2</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="linear-models.html"><a href="linear-models.html#tabulating-the-models"><i class="fa fa-check"></i><b>7.4</b> Tabulating the Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Model Diagnostics</a>
<ul>
<li class="chapter" data-level="8.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#assumptions-of-linear-models"><i class="fa fa-check"></i><b>8.1</b> Assumptions of Linear Models</a></li>
<li class="chapter" data-level="8.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#diagnostic-tools"><i class="fa fa-check"></i><b>8.2</b> Diagnostic Tools</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#posterior-predictive-check-7"><i class="fa fa-check"></i><b>8.2.1</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="8.2.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#marginal-model-plots"><i class="fa fa-check"></i><b>8.2.2</b> Marginal model plots</a></li>
<li class="chapter" data-level="8.2.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#residual-plots"><i class="fa fa-check"></i><b>8.2.3</b> Residual plots</a></li>
<li class="chapter" data-level="8.2.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#multicollinearity"><i class="fa fa-check"></i><b>8.2.4</b> Multicollinearity</a></li>
<li class="chapter" data-level="8.2.5" data-path="model-diagnostics.html"><a href="model-diagnostics.html#robust-models"><i class="fa fa-check"></i><b>8.2.5</b> Robust Models</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#other-topics"><i class="fa fa-check"></i><b>8.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html"><i class="fa fa-check"></i><b>9</b> Model Comparison and Regularization</a>
<ul>
<li class="chapter" data-level="9.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>9.1</b> Overfitting and Underfitting</a></li>
<li class="chapter" data-level="9.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>9.2</b> Kullback-Leibler Divergence</a></li>
<li class="chapter" data-level="9.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#information-criteria"><i class="fa fa-check"></i><b>9.3</b> Information Criteria</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#experiment-on-deviance"><i class="fa fa-check"></i><b>9.3.1</b> Experiment on Deviance</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#information-criteria-1"><i class="fa fa-check"></i><b>9.4</b> Information Criteria</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#akaike-information-criteria-aic"><i class="fa fa-check"></i><b>9.4.1</b> Akaike Information Criteria (AIC)</a></li>
<li class="chapter" data-level="9.4.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#deviance-information-criteria-dic"><i class="fa fa-check"></i><b>9.4.2</b> Deviance Information Criteria (DIC)</a></li>
<li class="chapter" data-level="9.4.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#watanabe-akaike-information-criteria-waic"><i class="fa fa-check"></i><b>9.4.3</b> Watanabe-Akaike Information Criteria (WAIC)</a></li>
<li class="chapter" data-level="9.4.4" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>9.4.4</b> Leave-One-Out Cross Validation</a></li>
<li class="chapter" data-level="9.4.5" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#example"><i class="fa fa-check"></i><b>9.4.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#stackingmodel-averaging"><i class="fa fa-check"></i><b>9.5</b> Stacking/Model Averaging</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#model-weights"><i class="fa fa-check"></i><b>9.5.1</b> Model Weights</a></li>
<li class="chapter" data-level="9.5.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#model-averaging"><i class="fa fa-check"></i><b>9.5.2</b> Model Averaging</a></li>
<li class="chapter" data-level="9.5.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#stacking"><i class="fa fa-check"></i><b>9.5.3</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#shrinkage-priors"><i class="fa fa-check"></i><b>9.6</b> Shrinkage Priors</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#number-of-parameters"><i class="fa fa-check"></i><b>9.6.1</b> Number of parameters</a></li>
<li class="chapter" data-level="9.6.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#sparsity-inducing-priors"><i class="fa fa-check"></i><b>9.6.2</b> Sparsity-Inducing Priors</a></li>
<li class="chapter" data-level="9.6.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#finnish-horseshoe"><i class="fa fa-check"></i><b>9.6.3</b> Finnish Horseshoe</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#variable-selection"><i class="fa fa-check"></i><b>9.7</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#projection-based-method"><i class="fa fa-check"></i><b>9.7.1</b> Projection-Based Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html"><i class="fa fa-check"></i><b>10</b> Hierarchical &amp; Multilevel Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#anova"><i class="fa fa-check"></i><b>10.1</b> ANOVA</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#frequentist-anova"><i class="fa fa-check"></i><b>10.1.1</b> “Frequentist” ANOVA</a></li>
<li class="chapter" data-level="10.1.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#bayesian-anova"><i class="fa fa-check"></i><b>10.1.2</b> Bayesian ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#multilevel-modeling-mlm"><i class="fa fa-check"></i><b>10.2</b> Multilevel Modeling (MLM)</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#examples-of-clustering"><i class="fa fa-check"></i><b>10.2.1</b> Examples of clustering</a></li>
<li class="chapter" data-level="10.2.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#data-1"><i class="fa fa-check"></i><b>10.2.2</b> Data</a></li>
<li class="chapter" data-level="10.2.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#intraclass-correlation"><i class="fa fa-check"></i><b>10.2.3</b> Intraclass correlation</a></li>
<li class="chapter" data-level="10.2.4" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#is-mlm-needed"><i class="fa fa-check"></i><b>10.2.4</b> Is MLM needed?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-coefficients"><i class="fa fa-check"></i><b>10.3</b> Varying Coefficients</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-intercepts"><i class="fa fa-check"></i><b>10.3.1</b> Varying Intercepts</a></li>
<li class="chapter" data-level="10.3.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-slopes"><i class="fa fa-check"></i><b>10.3.2</b> Varying Slopes</a></li>
<li class="chapter" data-level="10.3.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-sigma"><i class="fa fa-check"></i><b>10.3.3</b> Varying <span class="math inline">\(\sigma\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#model-comparisons"><i class="fa fa-check"></i><b>10.4</b> Model Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>11</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#basics-of-generalized-linear-models"><i class="fa fa-check"></i><b>11.1</b> Basics of Generalized Linear Models</a></li>
<li class="chapter" data-level="11.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>11.2</b> Binary Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#the-logit-link"><i class="fa fa-check"></i><b>11.2.1</b> The logit link</a></li>
<li class="chapter" data-level="11.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#choice-of-priors"><i class="fa fa-check"></i><b>11.2.2</b> Choice of Priors</a></li>
<li class="chapter" data-level="11.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interpreting-the-coefficients"><i class="fa fa-check"></i><b>11.2.3</b> Interpreting the coefficients</a></li>
<li class="chapter" data-level="11.2.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#model-checking-1"><i class="fa fa-check"></i><b>11.2.4</b> Model Checking</a></li>
<li class="chapter" data-level="11.2.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#complete-separation"><i class="fa fa-check"></i><b>11.2.5</b> Complete Separation</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binomial-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Binomial Logistic Regression</a></li>
<li class="chapter" data-level="11.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#probit-regression"><i class="fa fa-check"></i><b>11.4</b> Probit Regression</a></li>
<li class="chapter" data-level="11.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>11.5</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interpretations-2"><i class="fa fa-check"></i><b>11.5.1</b> Interpretations</a></li>
<li class="chapter" data-level="11.5.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#model-checking-2"><i class="fa fa-check"></i><b>11.5.2</b> Model Checking</a></li>
<li class="chapter" data-level="11.5.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#other-models-in-glm"><i class="fa fa-check"></i><b>11.5.3</b> Other models in GLM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="missing-data.html"><a href="missing-data.html"><i class="fa fa-check"></i><b>12</b> Missing Data</a>
<ul>
<li class="chapter" data-level="12.1" data-path="missing-data.html"><a href="missing-data.html#missing-data-mechanisms"><i class="fa fa-check"></i><b>12.1</b> Missing Data Mechanisms</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="missing-data.html"><a href="missing-data.html#mcar-missing-completely-at-random"><i class="fa fa-check"></i><b>12.1.1</b> MCAR (Missing Completely at Random)</a></li>
<li class="chapter" data-level="12.1.2" data-path="missing-data.html"><a href="missing-data.html#mar-missing-at-random"><i class="fa fa-check"></i><b>12.1.2</b> MAR (Missing At Random)</a></li>
<li class="chapter" data-level="12.1.3" data-path="missing-data.html"><a href="missing-data.html#nmar-not-missing-at-random"><i class="fa fa-check"></i><b>12.1.3</b> NMAR (Not Missing At Random)</a></li>
<li class="chapter" data-level="12.1.4" data-path="missing-data.html"><a href="missing-data.html#ignorable-missingness"><i class="fa fa-check"></i><b>12.1.4</b> Ignorable Missingness*</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="missing-data.html"><a href="missing-data.html#bayesian-approaches-for-missing-data"><i class="fa fa-check"></i><b>12.2</b> Bayesian Approaches for Missing Data</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="missing-data.html"><a href="missing-data.html#complete-case-analysislistwise-deletion"><i class="fa fa-check"></i><b>12.2.1</b> Complete Case Analysis/Listwise Deletion</a></li>
<li class="chapter" data-level="12.2.2" data-path="missing-data.html"><a href="missing-data.html#treat-missing-data-as-parameters"><i class="fa fa-check"></i><b>12.2.2</b> Treat Missing Data as Parameters</a></li>
<li class="chapter" data-level="12.2.3" data-path="missing-data.html"><a href="missing-data.html#multiple-imputation"><i class="fa fa-check"></i><b>12.2.3</b> Multiple Imputation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Handouts for Bayesian Data Analysis Class</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<p>There will be some math in this notes. Don’t worry if you feel the math is
challenging; for applied focused students, it is much more important to
understand the concepts of Bayesian methods than to understand the mathematical
symbols, as they usually can be handled by the software.</p>
<div id="history-of-bayesian-statistics" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> History of Bayesian Statistics</h2>
<p>Here is a nice brief video that covers some of the 250+ years of history of
Bayesian statistics: <a href="https://www.youtube.com/watch?v=BcvLAw-JRss" class="uri">https://www.youtube.com/watch?v=BcvLAw-JRss</a>. If you are
interested in learning more about the story, check out the nice popular science
book, <a href="https://yalebooks.yale.edu/book/9780300188226/theory-would-not-die">“The theory that would not die,” by Sharon Bertsch McGrayne</a></p>
<div id="thomas-bayes-17011762" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Thomas Bayes (1701–1762)</h3>
<p>You may find a biography of Bayes from
<a href="https://www.britannica.com/biography/Thomas-Bayes" class="uri">https://www.britannica.com/biography/Thomas-Bayes</a>. There is also a nice story in
the book by <span class="citation">Lambert (<a href="#ref-Lambert2018" role="doc-biblioref">2018</a>)</span>. He was an English Presbyterian minister. The important
work he wrote that founded Bayesian statistics was “An Essay towards solving a
Problem in the Doctrine of Chances”, which he did not publish and was later
discovered and edited by his friend, Richard Price, after Bayes’s death<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
</div>
<div id="pierre-simon-laplace-17491827" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Pierre-Simon Laplace (1749–1827)</h3>
<p>Laplace, a French Mathematician, was an important figure in not just Bayesian
statistics, but also in other areas of mathematics, astronomy, and physics. We
actually know much more the work by Laplace than by Bayes, and Laplace has
worked independently on the inverse probability problem (i.e.,
<span class="math inline">\(P[\text{Parameter} | \text{Data}]\)</span>). Indeed, he was credited for largely
formalizing Bayesian interpretation of probability and most of the machinery for
Bayesian statistics, and making it a useful technique to be applied to different
problems, despite the discipline being called “Bayesian.” His other
contributions include the methods of least square and the central limit theorem.
See a short biography of him at
<a href="https://www.britannica.com/biography/Pierre-Simon-marquis-de-Laplace" class="uri">https://www.britannica.com/biography/Pierre-Simon-marquis-de-Laplace</a>.</p>
</div>
<div id="th-century" class="section level3" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> 20th Century</h3>
<p>Until early 1920s, the <em>inverse probability</em> method, which is based on what is
now called Bayes’s Theorem, is pretty much the predominant point of view of
statistics. Then a point of view later known as <em>frequentist</em> statistics
arrived, and quickly became the mainstream school of thinking for statistical
inferences, and is still the major framework for quantitative research. In the
early 1920s, frequentist scholar, most notably R. A. Fisher and Jerzy Neyman,
criticized Bayesian inference for the use of subjective elements in an objective
discipline. In Fisher’s word,</p>
<blockquote>
<p>The theory of inverse probability is founded upon an error, and must be wholly
rejected—Fisher, 1925</p>
</blockquote>
<p>Ironically, the term <em>Bayesian</em> was first used in one of Fisher’s work. And
interestingly, Fisher actually thought he “have been doing almost exactly
what Bayes had done in the 18th century.”<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>Despite criticisms from frequentist scholars, Bayesian methods has been used by
scholars in the Allies in World War II, such as Alan Turing, in an algorithm to
break coded messages in the Enigma machine that the German Navy used to
communicate. However, because of the more complex mathematics involved in
Bayesian statistics, Bayesian statistics is limited to straight-forward problems
and theoretical discussions until the early 1980s, when computing speed
increases tremendously and makes <em>Markov Chain Monte Carlo</em>—the major
algorithm for Bayesian estimation in modern Bayesian statistics—feasible. With
the help of increased computing speed, Bayesian statistics has come back and
been used as an alternative way of thinking, especially given growing
dissatisfaction towards the misuse of frequentist statistics by some scholars
across disciplines. Bayesian estimation methods have also been applied to many
new research questions where frequentist approaches work less well, as well as
in big data analytics and machine learning.</p>
</div>
</div>
<div id="motivations-for-using-bayesian-methods" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Motivations for Using Bayesian Methods</h2>
<p>Based on my personal experience, Bayesian methods is used quite often in
statistics and related departments, as it is <span style="color:red">consistent
and coherent</span>, as contrast to frequentist where a new and probably ad hoc
procedure needed to be developed to handle a new problem. For Bayesian, as long
as you can formulate a model, you just run the analysis the same way as you
would for simpler problems, or in Bayesian people’s word “turning the Bayesian
crank,” and likely the difficulties would be more technical than theoretical,
which is usually solved with better computational speed.</p>
<p>Social and behavioral scientists are relatively slow to adopt the Bayesian
method, but things have been changing. In a recently accepted paper by
<span class="citation">van de Schoot et al. (<a href="#ref-VandeSchoot2017" role="doc-biblioref">2017</a>)</span>, the authors reviewed papers in psychology between 1990 to 2015
and found that whereas less than 10% of the papers in 1990 to 1996 mentioned
“Bayesian”, the proportion increased steadily and was found in <span style="color:red">close to 45% of the psychology papers in 2015</span>. Among
studies using Bayesian methods, more than 1/4 cited <span style="color:red">computational problems (e.g., nonconvergence) in frequentist
methods</span> as a reason, and about 13% cited the need to <span style="color:red">incorporate prior knowledge</span> into the estimation
process. The other reasons included the <span style="color:red">flexibility</span> of Bayesian methods for complex and
nonstandard problems, and the use of techniques traditionally attached to
Bayesian such as <span style="color:red">missing data</span> and <span style="color:red">model comparisons</span>.</p>
<div id="problem-with-classical-frequentist-statistics" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Problem with classical (frequentist) statistics</h3>
<p>The rise of Bayesian methods is also related to the statistical reform movement
in the past two decades. The problem is that applied researchers are <span style="color:red">obsessed with <span class="math inline">\(p &lt; .05\)</span></span> and often misinterpreted a
small <span class="math inline">\(p\)</span>-value as something that it isn’t <span class="citation">(read Gigerenzer <a href="#ref-Gigerenzer2004" role="doc-biblioref">2004</a>)</span>. Some scholars
coined the term
<a href="https://www.nimh.nih.gov/about/directors/thomas-insel/blog/2014/p-hacking.shtml"><span class="math inline">\(p\)</span>-hacking</a>
to refer to the practice of obtaining statistical significance by choosing to
test the data in a certain way, either consciously or subconsciously (e.g.,
dichotomizing using mean or median, trying the same hypothesis using different
measures of the same variable, etc). This is closely related to the recent
“replication crisis” in scientific research, <a href="https://nobaproject.com/modules/the-replication-crisis-in-psychology">with psychology being in the
center under close
scrutiny</a>.</p>
<p>Bayesian is no panacea to the problem. Indeed, if misused it can give rise to
the same problems as statistical significance. My goal in this class is to help
you appreciate the Bayesian tradition of <span style="color:red">embracing the
uncertainty in your results</span>, and adopt rigorous <span style="color:red">model checking</span> and <span style="color:red">comprehensive reporting</span> rather than relying merely on a
<span class="math inline">\(p\)</span>-value. I see this as the most important mission for someone teaching
statistics.</p>
</div>
</div>
<div id="probability" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Probability</h2>
<p>There are multiple perspectives for understanding probability.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> What
you’ve learned in your statistics training are based on the <em>frequentist</em>
interpretation of probability (and thus frequentist statistics), whereas what
you will learn in this class have the foundation on the <em>subjectivist</em>
interpretation of probability. Although, in my opinions, the impact of these
differences in interpretations of probability on statistical practices is
usually overstated, understanding the different perspectives on probability is
helpful for understanding the Bayesian framework.</p>
<blockquote>
<p>You don’t need to commit to one interpretation of probability in order to
conduct Bayesian data analysis.</p>
</blockquote>
<div id="classical-interpretation" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Classical Interpretation</h3>
<p>This is an earlier perspective, and is based on counting rules. The idea is that
probability is equally distributed among all “indifferent” outcomes.
“Indifferent” outcomes are those where a person does not have any evidence to
say that one outcome is more likely than another. For example, when one throws a
die, one does not think that a certain number is more likely than another,
unless one knows that the die is biased. In this case, there are six equally
likely outcome, and so the probability of each outcome is 1 / 6.</p>
<p><img src="../notes/figures/dice.png" /></p>
</div>
<div id="frequentist-interpretation" class="section level3" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Frequentist Interpretation</h3>
<p>The frequentist interpretation states that probability is essentially the
long-term relative frequency of an outcome. For example, to find the probability
of getting a “1” when throwing a die, one can repeat the experiment many times,
as illustrated below:</p>
<table>
<thead>
<tr class="header">
<th align="center">Trial</th>
<th align="center">Outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">10</td>
<td align="center">3</td>
</tr>
</tbody>
</table>
<p>And we can plot the relative frequency of “1”s in the trials:</p>
<p><img src="01_intro_files/figure-html/rel-freq-1.png" width="672" /></p>
<p>As you can see, with more trials, the relative frequency approaches 1 / 6. It’s
the reason why in introductory statistics, many of the concepts require you to
think in terms of repeated sampling (e.g., sampling distribution, <span class="math inline">\(p\)</span>-values,
standard errors, confidence intervals), because probability in this framework
is only possible when the outcome can be repeated. It’s also the reason why
we don’t talk about something like:</p>
<ul>
<li>the probability of the null hypothesis being true, or</li>
<li>the probability that the population mean is in the interval [75.5, 80.5],</li>
</ul>
<p>because the population is fixed and cannot be repeated. Only the samples can
be repeated, so probability in frequentist statistics is only about samples.</p>
</div>
<div id="problem-of-the-single-case" class="section level3" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Problem of the single case</h3>
<p>Because of the frequentist’s reference to long-term frequency, under this
framework it does not make sense to talk about probability of an event that
cannot be repeated. For example, it does not make sense to talk about the
probability that the Democrats will win the 2020 US Presidential Election, or
the probability that the LA Rams winning the 2019 Super Bowl (they didn’t), or
the probability that it rained on Christmas Day in LA in 2018, because all these
are specific events that cannot be repeated. The problem is that it is common
for lay people to talk about probabilities or chances for these events, and so
the frequentist interpretation is limited for these problems.</p>
</div>
<div id="subjectivist-interpretation" class="section level3" number="1.3.4">
<h3><span class="header-section-number">1.3.4</span> Subjectivist Interpretation</h3>
<p>The frequentist interpretation is sometimes called the “objectivist view”, as
the reference of probability is based on empirical evidence of long-term
relative frequency (albeit hypothetical in many cases). In contrast, the
<em>subjectivist</em> view of probability is based on one’s belief. For example, when I
say that the probability of getting a “1” from rolling a die is 1 / 6, it
reflects the state of my mind about the die. My belief can arise from different
sources: Maybe I make the die and know it is a fair one; maybe I saw someone
throwing the die 1,000 times and the number of “1”s was close to 1,000 / 6, or
maybe someone I trust and with authority says that the die has a 1-in-6 chance
of showing a “1”.</p>
<p>The “subjective” component has been criticized a lot by frequentist scholars,
sometimes unfairly. To be clear, what “subjective” here means is that
probability reflects the state of one’s mind instead of the state of the world,
and so it is totally fine that two people can have different beliefs about the
same event. However, it does not mean that probability is arbitrary, as the
beliefs are subjected to the constraints of the axioms of probability as well as
the condition that the person possessing such beliefs are
<em>rational</em>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> Therefore, if two persons are exposed to the same
information, they should form similar, though likely not identical, beliefs
about the event.</p>
<p>The subjective interpretation works perfectly fine with single events, as one
can have a belief about whether it rains on a particular day or a belief
about a particular election result.</p>
<!-- + About gambling -->
<!--     * Betting: Give $p$ units of utility in return of 1 unit of utility -->
<!--     * Activity: Have two people negotiate a bet? -->
</div>
<div id="basics-of-probability" class="section level3" number="1.3.5">
<h3><span class="header-section-number">1.3.5</span> Basics of Probability</h3>
<p>Kolmogorov axioms:</p>
<blockquote>
<ul>
<li>For an event <span class="math inline">\(A_i\)</span> (e.g., getting a “1” from throwing a die)
<ul>
<li><span class="math inline">\(P(A_i) \geq 0\)</span> [All probabilities are non-negative]</li>
<li><span class="math inline">\(P(A_1 \cup A_2 \cup \cdots) = 1\)</span> [Union of all possibilities is 1]</li>
<li><span class="math inline">\(P(A_1) + P(A_2) = P(A_1 \text{ or } A_2)\)</span> [Mutually exclusive events]</li>
</ul></li>
</ul>
</blockquote>
<p>Consider two events, for example, on throwing a die,</p>
<ul>
<li><span class="math inline">\(A\)</span>: The number is odd</li>
<li><span class="math inline">\(B\)</span>: The number is larger than or equal to 4</li>
</ul>
<p>Assuming that die is (believed to be) fair, you can easily verify that the
probability of <span class="math inline">\(A\)</span> is <span class="math inline">\(P(A)\)</span> = 3 / 6 = 1 / 2, and the probability of <span class="math inline">\(B\)</span> is
also <span class="math inline">\(P(B)\)</span> = 3 / 6 = 1 / 2.</p>
<div id="conditional-probability" class="section level4" number="1.3.5.1">
<h4><span class="header-section-number">1.3.5.1</span> Conditional Probability</h4>
<p>Conditional probability is the probability of an event given some other
information. In the real world, you can say that everything is conditional.
For example, the probability of getting an odd number on throwing a die is 1/2
is conditional on the die being fair. We use <span class="math inline">\(P(A | B)\)</span> to represent the
<span style="color:red">the conditional probability of event <span class="math inline">\(A\)</span> given event
<span class="math inline">\(B\)</span>.</span>.</p>
<p>Continuing from the previous example, <span class="math inline">\(P(A | B)\)</span> is the conditional probability
of getting an odd number, <em>knowing that the number is at least 4</em>. By definition,
the conditional probability is the probability that both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> happen (
written as <span class="math inline">\(A \cap B\)</span> and pronounced as A–cap–B or A–intersection–B),
divided by the probability that <span class="math inline">\(B\)</span> happen.</p>
<blockquote>
<p><span class="math inline">\(P(A | B) = \frac{P(A \cap B)}{P(B)}\)</span></p>
</blockquote>
<p>In the example, <span class="math inline">\(P(A \cap B)\)</span> = 1/6,
because 5 is the only even number <span class="math inline">\(\geq\)</span> 4 when throwing a die. Thus,
<span class="math display">\[\begin{align}
    P(A | B) &amp; = 1 / 3 \\
             &amp; = \frac{P(A \cap B)}{P(B)} \\
             &amp; = \frac{1 / 6}{1 / 2}
\end{align}\]</span></p>
<p>This picture should make it clear:
<img src="../notes/figures/cond_prob2.png" /></p>
</div>
<div id="independence" class="section level4" number="1.3.5.2">
<h4><span class="header-section-number">1.3.5.2</span> Independence</h4>
<p>Two events, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, are independent if</p>
<blockquote>
<p><span class="math inline">\(P(A | B) = P(A)\)</span></p>
</blockquote>
<p>This means that any knowledge of <span class="math inline">\(B\)</span> does not (or should not) affect one’s
belief about <span class="math inline">\(A\)</span>. In the example, obviously <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not independent,
because once we know that the number if 4 or above, it changes the probability
of whether it is an odd number or not.</p>
<p>It can also be expressed as</p>
<blockquote>
<p>With independence, <span class="math inline">\(P(A \cap B) = P(A) P(B)\)</span></p>
</blockquote>
</div>
<div id="law-of-total-probability" class="section level4" number="1.3.5.3">
<h4><span class="header-section-number">1.3.5.3</span> Law of Total Probability</h4>
<p>When we talk about conditional probability, like <span class="math inline">\(B_1\)</span> = 4 or above and <span class="math inline">\(B_2\)</span> =
3 or below, we can get <span class="math inline">\(P(A | B_1)\)</span> and <span class="math inline">\(P(A | B_2)\)</span> (see the figure below), we
refer <span class="math inline">\(P(A)\)</span> as the <em>marginal probability</em>, meaning that the probability of <span class="math inline">\(A\)</span>
<span style="color:red"> without knowledge of <span class="math inline">\(B\)</span></span>.</p>
<p><img src="../notes/figures/total_prob.png" /></p>
<p>If <span class="math inline">\(B_1, B_2, \cdots, B_n\)</span> are all mutually exclusive possibilities for an event
(so they add up to a probability of 1), then by the <span style="color:red">
law of total probability</span>,</p>
<blockquote>
<p><span class="math display">\[\begin{align}
P(A) &amp; = P(A \cap B_1) + P(A \cap B_2) + \cdots + P(A \cap B_n)  \\
&amp; = P(A | B_1)P(B_1) + P(A | B_2)P(B_2) + \cdots + P(A | B_n) P(B_n)  \\
&amp; = \sum_{k = 1}^n P(A | B_k) P(B_k)
\end{align}\]</span></p>
</blockquote>
</div>
</div>
</div>
<div id="bayess-theorem" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Bayes’s Theorem</h2>
<p>The Bayes’s theorem is, surprisingly (or unsurprisingly), very simple:</p>
<blockquote>
<p><span class="math display">\[P(B | A) = \frac{P(A | B) P(B)}{P(A)}\]</span></p>
</blockquote>
<p>More generally, we can expand it to incorporate the law of total probability tomake it more applicable to data analysis. Consider <span class="math inline">\(B_i\)</span> as one of the <span class="math inline">\(n\)</span> many possible mutually exclusive events, then
<span class="math display">\[\begin{align}
  P(B_i | A) &amp; = \frac{P(A | B_i) P(B_i)}{P(A)}  \\
             &amp; = \frac{P(A | B_i) P(B_i)}
                      {P(A | B_1)P(B_1) + P(A | B_2)P(B_2) + \cdots + 
                       P(A | B_n)P(B_n)} \\
             &amp; = \frac{P(A | B_i) P(B_i)}{\sum_{k = 1}^n P(A | B_k)P(B_k)}
\end{align}\]</span></p>
<p>If <span class="math inline">\(B_i\)</span> is a continuous variable, we will replace the sum by an integral,
<span class="math display">\[P(B_i | A) = \frac{P(A | B_i) P(B_i)}{\int_k P(A | B_k)P(B_k)}\]</span>
The denominator is not important for practical Bayesian analysis, therefore, it
is sufficient to write the above equality as</p>
<blockquote>
<p><span class="math display">\[P(B_i | A) \propto P(A | B_i) P(B_i)\]</span></p>
</blockquote>
<hr />
<div id="example-1-base-rate-fallacy-from-wikipedia" class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Example 1: Base rate fallacy (From Wikipedia)</h3>
<p>A police officer stops a driver <em>at random</em> and do a breathalyzer test for the
driver. The breathalyzer is known to detect true drunkenness 100% of the time,
but in 1% of the cases it gives a false positive when the driver is sober. We
also know that in general, for every 1,000 drivers passing through that spot,
one is driving drunk. Suppose that the breathalyzer shows positive for the
driver. What is the probability that the driver is truly drunk?</p>
<p><span class="math inline">\(P(\text{positive} | \text{drunk}) = 1\)</span><br />
<span class="math inline">\(P(\text{positive} | \text{sober}) = 0.01\)</span><br />
<span class="math inline">\(P(\text{drunk}) = 1 / 1000\)</span><br />
<span class="math inline">\(P(\text{sober}) = 999 / 1000\)</span></p>
<p>Using Bayes’ Theorem,</p>
<p><span class="math display">\[\begin{align}
  P(\text{drunk} | \text{positive}) 
  &amp; = \frac{P(\text{positive} | \text{drunk}) P(\text{drunk})}
           {P(\text{positive} | \text{drunk}) P(\text{drunk}) + 
            P(\text{positive} | \text{sober}) P(\text{sober})}  \\
  &amp; = \frac{1 \times 0.001}{1 \times 0.001 + 0.01 \times 0.999} \\
  &amp; = 100 / 1099 \approx 0.091
\end{align}\]</span></p>
<p>So there is less than 10% chance that the driver is drunk even when the
breathalyzer shows positive.</p>
<p>You can verify that with a simulation using R:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="introduction.html#cb3-1"></a><span class="kw">set.seed</span>(<span class="dv">4</span>)</span>
<span id="cb3-2"><a href="introduction.html#cb3-2"></a>truly_drunk &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;drunk&quot;</span>, <span class="dv">100</span>), <span class="kw">rep</span>(<span class="st">&quot;sober&quot;</span>, <span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="dv">999</span>))</span>
<span id="cb3-3"><a href="introduction.html#cb3-3"></a><span class="kw">table</span>(truly_drunk)</span></code></pre></div>
<pre><code>&gt;# truly_drunk
&gt;# drunk sober 
&gt;#   100 99900</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="introduction.html#cb5-1"></a>breathalyzer_test &lt;-<span class="st"> </span><span class="kw">ifelse</span>(truly_drunk <span class="op">==</span><span class="st"> &quot;drunk&quot;</span>, </span>
<span id="cb5-2"><a href="introduction.html#cb5-2"></a>                            <span class="co"># If drunk, 100% chance of showing positive</span></span>
<span id="cb5-3"><a href="introduction.html#cb5-3"></a>                            <span class="st">&quot;positive&quot;</span>, </span>
<span id="cb5-4"><a href="introduction.html#cb5-4"></a>                            <span class="co"># If not drunk, 1% chance of showing positive</span></span>
<span id="cb5-5"><a href="introduction.html#cb5-5"></a>                            <span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;positive&quot;</span>, <span class="st">&quot;negative&quot;</span>), <span class="dv">999000</span>, </span>
<span id="cb5-6"><a href="introduction.html#cb5-6"></a>                                   <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(.<span class="dv">01</span>, <span class="fl">.99</span>)))</span>
<span id="cb5-7"><a href="introduction.html#cb5-7"></a><span class="co"># Check the probability p(positive | sober)</span></span>
<span id="cb5-8"><a href="introduction.html#cb5-8"></a><span class="kw">table</span>(breathalyzer_test[truly_drunk <span class="op">==</span><span class="st"> &quot;sober&quot;</span>])</span></code></pre></div>
<pre><code>&gt;# 
&gt;# negative positive 
&gt;#    98903      997</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="introduction.html#cb7-1"></a><span class="co"># 997 / 99900 = 0.00997998, so the error rate is less than 1%</span></span>
<span id="cb7-2"><a href="introduction.html#cb7-2"></a><span class="co"># Now, Check the probability p(drunk | positive)</span></span>
<span id="cb7-3"><a href="introduction.html#cb7-3"></a><span class="kw">table</span>(truly_drunk[breathalyzer_test <span class="op">==</span><span class="st"> &quot;positive&quot;</span>])</span></code></pre></div>
<pre><code>&gt;# 
&gt;# drunk sober 
&gt;#   100   997</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="introduction.html#cb9-1"></a><span class="co"># 100 / (100 + 997) = 0.0911577, which is only 9.1%!</span></span></code></pre></div>
<hr />
</div>
</div>
<div id="bayesian-statistics" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Bayesian Statistics</h2>
<p><code>Bayesian statistics</code> is a way to estimate some parameter <span class="math inline">\(\theta\)</span> (i.e., some
quantities of interest, such as population mean, regression coefficient, etc) by
applying the Bayes’ Theorem.
&gt; <span class="math display">\[P(\theta = t | y) \propto P(y | \theta = t) P(\theta = t)\]</span></p>
<p>There are three components in the above equality:</p>
<ul>
<li><span class="math inline">\(P(y | \theta = t)\)</span>, the probability that you observe the datum <span class="math inline">\(y\)</span>, assuming
that the parameter <span class="math inline">\(\theta\)</span> has a value <span class="math inline">\(t\)</span>; this is called the <code>likelihood</code>
(Note: It is the likelihood of <span class="math inline">\(\theta\)</span>, but probability about <span class="math inline">\(y\)</span>)</li>
<li><span class="math inline">\(P(\theta = t)\)</span>, the probability that <span class="math inline">\(\theta\)</span> has a value of <span class="math inline">\(t\)</span>, without
referring to the datum <span class="math inline">\(y\)</span>. This usually requires appeals to one’s degree of
belief, and so is called the <code>prior</code></li>
<li><span class="math inline">\(P(\theta = t | y)\)</span>, the probability that <span class="math inline">\(\theta\)</span> has a value of <span class="math inline">\(t\)</span>, after
observing the datum <span class="math inline">\(y\)</span>; this is called the <code>posterior</code></li>
</ul>
<p>This is different from the classical/frequentist statistics, which focuses
solely on the likelihood function.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> In Bayesian statistics, the goal is to
update one’s belief about <span class="math inline">\(\theta\)</span> based on the observed datum <span class="math inline">\(y\)</span>.</p>
<hr />
<div id="example-2-locating-a-plane" class="section level3" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> Example 2: Locating a Plane</h3>
<p>(reference: <a href="http://87.106.45.173:3838/felix/BayesLessons/BayesianLesson1.Rmd" class="uri">http://87.106.45.173:3838/felix/BayesLessons/BayesianLesson1.Rmd</a>)</p>
<p>Consider a highly simplified scenario of locating a missing plane in the sea.
Assume that we know the plane, before missing, happened to be flying on the same
latitude, heading west across the Pacific, so we only need to find the longitude
of it. We want to go out to collect debris (data) so that we can narrow the
location (<span class="math inline">\(\theta\)</span>) of the plane down.</p>
<p>We start with our prior. Assume that we have some rough idea that the plane
should be, so we express our belief in a probability distribution like the
following:</p>
<p><img src="01_intro_files/figure-html/plane-prob-1.png" width="336" /></p>
<p>which says that our belief is that the plane is about twice more likely to be
towards the east than towards the west. Below are two other options for priors
(out of infinitely many), one providing virtually no information and the other
encoding stronger information:</p>
<p><img src="01_intro_files/figure-html/plane-priors-1.png" width="624" /></p>
<p>The prior is chosen to reflect the researcher’s belief, so it is likely that
different researchers will formulate a different prior for the same problem, and
that’s okay as long as the prior is reasonable and justified. Later we will
learn that in regular Bayesian analyses, with a moderate sample size different
priors generally make only a negligible differences.</p>
<p>Now, assume that we have collected debris in the locations shown in the graph,</p>
<p><img src="01_intro_files/figure-html/plane-data-1.png" width="336" /></p>
<p>Now, from Bayes’s Theorem,</p>
<p><span class="math display">\[\text{Posterior Probability} \propto \text{Prior Probability} \times
                                       \text{Likelihood}\]</span></p>
<p>So we can simply multiply the prior probabilities and the likelihood to get the
posterior probability for every location. A rescaling step is needed to make
sure that the area under the curve will be 1, which is usually performed by the
software.</p>
<pre><code>&gt;# Warning: `mapping` is not used by stat_function()</code></pre>
<p><img src="01_intro_files/figure-html/plane-post-1.png" width="456" /></p>
<p>The following shows what happen with a stronger prior:</p>
<pre><code>&gt;# Warning: `mapping` is not used by stat_function()</code></pre>
<p><img src="01_intro_files/figure-html/plane-strong-1.png" width="456" /></p>
<hr />
</div>
</div>
<div id="comparing-bayesian-and-frequentist-statistics" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Comparing Bayesian and Frequentist Statistics</h2>
<table>
<colgroup>
<col width="34%" />
<col width="37%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th>Attributes</th>
<th>Frequentist</th>
<th>Bayesian</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Interpretation of probability</td>
<td>Frequentist</td>
<td>Subjectivist</td>
</tr>
<tr class="even">
<td>Uncertainty</td>
<td>How estimates vary in repeated sampling from the same population</td>
<td>How much prior beliefs about parameters change in light of data</td>
</tr>
<tr class="odd">
<td>What’s relevant?</td>
<td>Current data set + all that might have been observed</td>
<td>Only the data set that is actually observed</td>
</tr>
<tr class="even">
<td>How to proceed with analyses</td>
<td>MLE; ad hoc and depends on problems</td>
<td>“Turning the Bayesian crank”</td>
</tr>
</tbody>
</table>
</div>
<div id="software-for-bayesian-statistics" class="section level2" number="1.7">
<h2><span class="header-section-number">1.7</span> Software for Bayesian Statistics</h2>
<ul>
<li><a href="http://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/">WinBUGS</a>
<ul>
<li>Bayesian inference Using Gibbs Sampling</li>
<li>Free, and most popular until late 2000s. Many Bayesian scholars still use
WinBUGS</li>
<li>No further development</li>
<li>One can communicate from R to WinBUGS using the package <code>R2WinBUGS</code></li>
</ul></li>
<li><a href="http://mcmc-jags.sourceforge.net/">JAGS</a>
<ul>
<li>Just Another Gibbs Sampler</li>
<li>Very similar to WinBUGS, but written in C++, and support user-defined
functionality</li>
<li>Cross-platform compatibility</li>
<li>One can communicate from R to JAGS using the package <code>rjags</code> or <code>runjags</code></li>
</ul></li>
<li><a href="http://mc-stan.org/">STAN</a>
<ul>
<li>Named in honour of Stanislaw Ulam, who invented the Markov Chain Monte
Carlo method</li>
<li>Uses new algorithms that are different from Gibbs sampling</li>
<li>Under very active development</li>
<li>Can interface with R throught the package <code>rstan</code>, and the R packages
<code>rstanarm</code> and <code>brms</code> automates the procedure for fitting models in STAN for
many commonly used models</li>
</ul></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Gigerenzer2004">
<p>Gigerenzer, Gerd. 2004. “Mindless statistics.” <em>The Journal of Socio-Economics</em> 33 (5): 587–606. <a href="https://doi.org/10.1016/j.socec.2004.09.033">https://doi.org/10.1016/j.socec.2004.09.033</a>.</p>
</div>
<div id="ref-Lambert2018">
<p>Lambert, Ben. 2018. <em>A student’s guide to Bayesian statistics</em>. <a href="https://bookshelf.vitalsource.com">https://bookshelf.vitalsource.com</a>.</p>
</div>
<div id="ref-VandeSchoot2017">
<p>van de Schoot, Rens, Sonja D. Winter, Oisín Ryan, Mariëlle Zondervan-Zwijnenburg, and Sarah Depaoli. 2017. “A systematic review of Bayesian articles in psychology: The last 25 years.” <em>Psychological Methods</em> 22 (2): 217–39. <a href="https://doi.org/10.1037/met0000100">https://doi.org/10.1037/met0000100</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Price is another important figure in mathematics and philosopher, and
have taken Bayes’ theorem and applied it to insurance and moral philosophy.<a href="introduction.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>See the <a href="https://projecteuclid.org/download/pdf_1/euclid.ba/1340370565">paper by John
Aldrich</a> on this.<a href="introduction.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>See <a href="http://plato.stanford.edu/entries/probability-interpret/" class="uri">http://plato.stanford.edu/entries/probability-interpret/</a> for
more information<a href="introduction.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>In a purely subjectivist view of probability, assigning a
probability <span class="math inline">\(P\)</span> to an event does not require any justifications, as long as it
follows the axioms of probability. For example, I can say that the probability
of me winning the lottery and thus becoming the richest person on earth tomorrow
is 95%, which by definition would make the probability of me not winning the
lottery 5%. Most Bayesian scholars, however, do not endorse this version of
subjectivist probability, and require justifications of one’s beliefs (that has
some correspondence to the world).<a href="introduction.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The likelihood function in classical/frequentist statistics is usually
written as <span class="math inline">\(P(y; \theta)\)</span>. You will notice that here I write the likelihood for
classical/frequentist statistics to be different than the one used in Bayesian
statistics. This is intentional: In frequentist conceptualization, <span class="math inline">\(\theta\)</span> is
fixed and it does not make sense to talk about probability of <span class="math inline">\(\theta\)</span>. This
implies that we cannot condition on <span class="math inline">\(\theta\)</span>, because conditional probability is
defined only when <span class="math inline">\(P(\theta)\)</span> is defined.<a href="introduction.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["notes_bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

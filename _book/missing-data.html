<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Missing Data | Course Handouts for Bayesian Data Analysis Class</title>
  <meta name="description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Spring semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Missing Data | Course Handouts for Bayesian Data Analysis Class" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Spring semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Missing Data | Course Handouts for Bayesian Data Analysis Class" />
  
  <meta name="twitter:description" content="This is a collection of my course handouts for PSYC 621 class in the 2019 Spring semester. Please contact me [mailto:hokchiol@usc.edu] for any errors (as I’m sure there are plenty of them)." />
  

<meta name="author" content="Mark Lai" />


<meta name="date" content="2019-12-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="generalized-linear-models.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.1/grViz.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PSYC 621 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#history-of-bayesian-statistics"><i class="fa fa-check"></i><b>1.1</b> History of Bayesian Statistics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#thomas-bayes-17011762"><i class="fa fa-check"></i><b>1.1.1</b> Thomas Bayes (1701–1762)</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#pierre-simon-laplace-17491827"><i class="fa fa-check"></i><b>1.1.2</b> Pierre-Simon Laplace (1749–1827)</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction.html"><a href="introduction.html#th-century"><i class="fa fa-check"></i><b>1.1.3</b> 20th Century</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#motivations-for-using-bayesian-methods"><i class="fa fa-check"></i><b>1.2</b> Motivations for Using Bayesian Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#problem-with-classical-frequentist-statistics"><i class="fa fa-check"></i><b>1.2.1</b> Problem with classical (frequentist) statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#probability"><i class="fa fa-check"></i><b>1.3</b> Probability</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#classical-interpretation"><i class="fa fa-check"></i><b>1.3.1</b> Classical Interpretation</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#frequentist-interpretation"><i class="fa fa-check"></i><b>1.3.2</b> Frequentist Interpretation</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#problem-of-the-single-case"><i class="fa fa-check"></i><b>1.3.3</b> Problem of the single case</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#subjectivist-interpretation"><i class="fa fa-check"></i><b>1.3.4</b> Subjectivist Interpretation</a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#basics-of-probability"><i class="fa fa-check"></i><b>1.3.5</b> Basics of Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#bayess-theorem"><i class="fa fa-check"></i><b>1.4</b> Bayes’s Theorem</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#example-1-base-rate-fallacy-from-wikipedia"><i class="fa fa-check"></i><b>1.4.1</b> Example 1: Base rate fallacy (From Wikipedia)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#bayesian-statistics"><i class="fa fa-check"></i><b>1.5</b> Bayesian Statistics</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#example-2-locating-a-plane"><i class="fa fa-check"></i><b>1.5.1</b> Example 2: Locating a Plane</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#comparing-bayesian-and-frequentist-statistics"><i class="fa fa-check"></i><b>1.6</b> Comparing Bayesian and Frequentist Statistics</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#software-for-bayesian-statistics"><i class="fa fa-check"></i><b>1.7</b> Software for Bayesian Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>2</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Steps of Bayesian Data Analysis</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#real-data-example"><i class="fa fa-check"></i><b>2.2</b> Real Data Example</a></li>
<li class="chapter" data-level="2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#choosing-a-model"><i class="fa fa-check"></i><b>2.3</b> Choosing a Model</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#exchangeability"><i class="fa fa-check"></i><b>2.3.1</b> Exchangeability*</a></li>
<li class="chapter" data-level="2.3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#probability-distributions"><i class="fa fa-check"></i><b>2.3.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="2.3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-likelihood"><i class="fa fa-check"></i><b>2.3.3</b> The Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#specifying-priors"><i class="fa fa-check"></i><b>2.4</b> Specifying Priors</a><ul>
<li class="chapter" data-level="2.4.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#beta-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#obtain-the-posterior-distributions"><i class="fa fa-check"></i><b>2.5</b> Obtain the Posterior Distributions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#grid-approximation"><i class="fa fa-check"></i><b>2.5.1</b> Grid Approximation</a></li>
<li class="chapter" data-level="2.5.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#using-conjugate-priors"><i class="fa fa-check"></i><b>2.5.2</b> Using Conjugate Priors</a></li>
<li class="chapter" data-level="2.5.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#laplace-approximation-with-maximum-a-posteriori-estimation"><i class="fa fa-check"></i><b>2.5.3</b> Laplace Approximation with Maximum A Posteriori Estimation</a></li>
<li class="chapter" data-level="2.5.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#markov-chain-monte-carlo-mcmc"><i class="fa fa-check"></i><b>2.5.4</b> Markov Chain Monte Carlo (MCMC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summarizing-the-posterior-distribution"><i class="fa fa-check"></i><b>2.6</b> Summarizing the Posterior Distribution</a><ul>
<li class="chapter" data-level="2.6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-mean-median-and-mode"><i class="fa fa-check"></i><b>2.6.1</b> Posterior Mean, Median, and Mode</a></li>
<li class="chapter" data-level="2.6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#uncertainty-estimates"><i class="fa fa-check"></i><b>2.6.2</b> Uncertainty Estimates</a></li>
<li class="chapter" data-level="2.6.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>2.6.3</b> Credible Intervals</a></li>
<li class="chapter" data-level="2.6.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#probability-of-theta-higherlower-than-a-certain-value"><i class="fa fa-check"></i><b>2.6.4</b> Probability of <span class="math inline">\(\theta\)</span> Higher/Lower Than a Certain Value</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#model-checking"><i class="fa fa-check"></i><b>2.7</b> Model Checking</a><ul>
<li class="chapter" data-level="2.7.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>2.7.1</b> Posterior Predictive Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-predictive-check"><i class="fa fa-check"></i><b>2.8</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="2.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summary"><i class="fa fa-check"></i><b>2.9</b> Summary</a><ul>
<li class="chapter" data-level="2.9.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#key-concepts"><i class="fa fa-check"></i><b>2.9.1</b> Key Concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="one-parameter-models.html"><a href="one-parameter-models.html"><i class="fa fa-check"></i><b>3</b> One-Parameter Models</a><ul>
<li class="chapter" data-level="3.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#binomialbernoulli-data"><i class="fa fa-check"></i><b>3.1</b> Binomial/Bernoulli data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#reparameterization"><i class="fa fa-check"></i><b>3.1.1</b> Reparameterization*</a></li>
<li class="chapter" data-level="3.1.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#posterior-predictive-check-1"><i class="fa fa-check"></i><b>3.1.2</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="3.1.3" data-path="one-parameter-models.html"><a href="one-parameter-models.html#comparison-to-frequentist-results"><i class="fa fa-check"></i><b>3.1.3</b> Comparison to frequentist results</a></li>
<li class="chapter" data-level="3.1.4" data-path="one-parameter-models.html"><a href="one-parameter-models.html#sensitivity-to-different-priors"><i class="fa fa-check"></i><b>3.1.4</b> Sensitivity to different priors</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#poisson-data"><i class="fa fa-check"></i><b>3.2</b> Poisson Data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="one-parameter-models.html"><a href="one-parameter-models.html#example-2"><i class="fa fa-check"></i><b>3.2.1</b> Example 2</a></li>
<li class="chapter" data-level="3.2.2" data-path="one-parameter-models.html"><a href="one-parameter-models.html#choosing-a-model-1"><i class="fa fa-check"></i><b>3.2.2</b> Choosing a model</a></li>
<li class="chapter" data-level="3.2.3" data-path="one-parameter-models.html"><a href="one-parameter-models.html#choosing-a-prior"><i class="fa fa-check"></i><b>3.2.3</b> Choosing a prior</a></li>
<li class="chapter" data-level="3.2.4" data-path="one-parameter-models.html"><a href="one-parameter-models.html#model-equations-and-diagram"><i class="fa fa-check"></i><b>3.2.4</b> Model Equations and Diagram</a></li>
<li class="chapter" data-level="3.2.5" data-path="one-parameter-models.html"><a href="one-parameter-models.html#getting-the-posterior"><i class="fa fa-check"></i><b>3.2.5</b> Getting the posterior</a></li>
<li class="chapter" data-level="3.2.6" data-path="one-parameter-models.html"><a href="one-parameter-models.html#posterior-predictive-check-2"><i class="fa fa-check"></i><b>3.2.6</b> Posterior Predictive Check</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html"><i class="fa fa-check"></i><b>4</b> Brief Introduction to STAN</a><ul>
<li class="chapter" data-level="4.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#stan"><i class="fa fa-check"></i><b>4.1</b> <code>STAN</code></a><ul>
<li class="chapter" data-level="4.1.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#stan-code"><i class="fa fa-check"></i><b>4.1.1</b> <code>STAN</code> code</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#rstan"><i class="fa fa-check"></i><b>4.2</b> <code>RStan</code></a><ul>
<li class="chapter" data-level="4.2.1" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#assembling-data-list-in-r"><i class="fa fa-check"></i><b>4.2.1</b> Assembling data list in R</a></li>
<li class="chapter" data-level="4.2.2" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#call-rstan"><i class="fa fa-check"></i><b>4.2.2</b> Call <code>rstan</code></a></li>
<li class="chapter" data-level="4.2.3" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#summarize-the-results"><i class="fa fa-check"></i><b>4.2.3</b> Summarize the results</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="brief-introduction-to-stan.html"><a href="brief-introduction-to-stan.html#resources"><i class="fa fa-check"></i><b>4.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="group-comparisons.html"><a href="group-comparisons.html"><i class="fa fa-check"></i><b>5</b> Group Comparisons</a><ul>
<li class="chapter" data-level="5.1" data-path="group-comparisons.html"><a href="group-comparisons.html#data"><i class="fa fa-check"></i><b>5.1</b> Data</a></li>
<li class="chapter" data-level="5.2" data-path="group-comparisons.html"><a href="group-comparisons.html#between-subject-comparisons"><i class="fa fa-check"></i><b>5.2</b> Between-Subject Comparisons</a><ul>
<li class="chapter" data-level="5.2.1" data-path="group-comparisons.html"><a href="group-comparisons.html#plots"><i class="fa fa-check"></i><b>5.2.1</b> Plots</a></li>
<li class="chapter" data-level="5.2.2" data-path="group-comparisons.html"><a href="group-comparisons.html#independent-sample-t-test"><i class="fa fa-check"></i><b>5.2.2</b> Independent sample t-test</a></li>
<li class="chapter" data-level="5.2.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesian-normal-model"><i class="fa fa-check"></i><b>5.2.3</b> Bayesian Normal Model</a></li>
<li class="chapter" data-level="5.2.4" data-path="group-comparisons.html"><a href="group-comparisons.html#robust-model"><i class="fa fa-check"></i><b>5.2.4</b> Robust Model</a></li>
<li class="chapter" data-level="5.2.5" data-path="group-comparisons.html"><a href="group-comparisons.html#shifted-lognormal-model"><i class="fa fa-check"></i><b>5.2.5</b> Shifted Lognormal Model*</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="group-comparisons.html"><a href="group-comparisons.html#notes-on-model-comparison"><i class="fa fa-check"></i><b>5.3</b> Notes on Model Comparison</a></li>
<li class="chapter" data-level="5.4" data-path="group-comparisons.html"><a href="group-comparisons.html#within-subject-comparisons"><i class="fa fa-check"></i><b>5.4</b> Within-Subject Comparisons</a><ul>
<li class="chapter" data-level="5.4.1" data-path="group-comparisons.html"><a href="group-comparisons.html#plots-1"><i class="fa fa-check"></i><b>5.4.1</b> Plots</a></li>
<li class="chapter" data-level="5.4.2" data-path="group-comparisons.html"><a href="group-comparisons.html#independent-sample-t-test-1"><i class="fa fa-check"></i><b>5.4.2</b> Independent sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="5.4.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesian-normal-model-1"><i class="fa fa-check"></i><b>5.4.3</b> Bayesian Normal Model</a></li>
<li class="chapter" data-level="5.4.4" data-path="group-comparisons.html"><a href="group-comparisons.html#using-brms"><i class="fa fa-check"></i><b>5.4.4</b> Using <code>brms</code>*</a></li>
<li class="chapter" data-level="5.4.5" data-path="group-comparisons.html"><a href="group-comparisons.html#region-of-practical-equivalence-rope"><i class="fa fa-check"></i><b>5.4.5</b> Region of Practical Equivalence (ROPE)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="6.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#monte-carlo-simulation-with-one-unknown"><i class="fa fa-check"></i><b>6.1</b> Monte Carlo Simulation With One Unknown</a></li>
<li class="chapter" data-level="6.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo-mcmc-with-one-parameter"><i class="fa fa-check"></i><b>6.2</b> Markov Chain Monte Carlo (MCMC) With One Parameter</a><ul>
<li class="chapter" data-level="6.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> The Metropolis algorithm</a></li>
<li class="chapter" data-level="6.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-metropolis-hastings-algorithm"><i class="fa fa-check"></i><b>6.2.2</b> The Metropolis-Hastings Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain"><i class="fa fa-check"></i><b>6.3</b> Markov Chain</a></li>
<li class="chapter" data-level="6.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#effective-sample-size-n_texteff"><i class="fa fa-check"></i><b>6.4</b> Effective Sample Size (<span class="math inline">\(n_\text{eff}\)</span>)</a></li>
<li class="chapter" data-level="6.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mc-error"><i class="fa fa-check"></i><b>6.5</b> MC Error</a></li>
<li class="chapter" data-level="6.6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#burn-inwarmup"><i class="fa fa-check"></i><b>6.6</b> Burn-in/Warmup</a><ul>
<li class="chapter" data-level="6.6.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#thinning"><i class="fa fa-check"></i><b>6.6.1</b> Thinning</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#diagnostics-of-mcmc"><i class="fa fa-check"></i><b>6.7</b> Diagnostics of MCMC</a><ul>
<li class="chapter" data-level="6.7.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mixing"><i class="fa fa-check"></i><b>6.7.1</b> Mixing</a></li>
<li class="chapter" data-level="6.7.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#acceptance-rate"><i class="fa fa-check"></i><b>6.7.2</b> Acceptance Rate</a></li>
<li class="chapter" data-level="6.7.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#diagnostics-using-multiple-chains"><i class="fa fa-check"></i><b>6.7.3</b> Diagnostics Using Multiple Chains</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#multiple-parameters"><i class="fa fa-check"></i><b>6.8</b> Multiple Parameters</a></li>
<li class="chapter" data-level="6.9" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>6.9</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>7</b> Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="linear-models.html"><a href="linear-models.html#what-is-regression"><i class="fa fa-check"></i><b>7.1</b> What is Regression?</a></li>
<li class="chapter" data-level="7.2" data-path="linear-models.html"><a href="linear-models.html#one-predictor"><i class="fa fa-check"></i><b>7.2</b> One Predictor</a><ul>
<li class="chapter" data-level="7.2.1" data-path="linear-models.html"><a href="linear-models.html#a-continuous-predictor"><i class="fa fa-check"></i><b>7.2.1</b> A continuous predictor</a></li>
<li class="chapter" data-level="7.2.2" data-path="linear-models.html"><a href="linear-models.html#centering"><i class="fa fa-check"></i><b>7.2.2</b> Centering</a></li>
<li class="chapter" data-level="7.2.3" data-path="linear-models.html"><a href="linear-models.html#a-categorical-predictor"><i class="fa fa-check"></i><b>7.2.3</b> A categorical predictor</a></li>
<li class="chapter" data-level="7.2.4" data-path="linear-models.html"><a href="linear-models.html#predictors-with-multiple-categories"><i class="fa fa-check"></i><b>7.2.4</b> Predictors with multiple categories</a></li>
<li class="chapter" data-level="7.2.5" data-path="linear-models.html"><a href="linear-models.html#stan-4"><i class="fa fa-check"></i><b>7.2.5</b> STAN</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="linear-models.html"><a href="linear-models.html#multiple-regression"><i class="fa fa-check"></i><b>7.3</b> Multiple Regression</a><ul>
<li class="chapter" data-level="7.3.1" data-path="linear-models.html"><a href="linear-models.html#two-predictor-example"><i class="fa fa-check"></i><b>7.3.1</b> Two Predictor Example</a></li>
<li class="chapter" data-level="7.3.2" data-path="linear-models.html"><a href="linear-models.html#interactions"><i class="fa fa-check"></i><b>7.3.2</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="linear-models.html"><a href="linear-models.html#tabulating-the-models"><i class="fa fa-check"></i><b>7.4</b> Tabulating the Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Model Diagnostics</a><ul>
<li class="chapter" data-level="8.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#assumptions-of-linear-models"><i class="fa fa-check"></i><b>8.1</b> Assumptions of Linear Models</a></li>
<li class="chapter" data-level="8.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#diagnostic-tools"><i class="fa fa-check"></i><b>8.2</b> Diagnostic Tools</a><ul>
<li class="chapter" data-level="8.2.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#posterior-predictive-check-7"><i class="fa fa-check"></i><b>8.2.1</b> Posterior Predictive Check</a></li>
<li class="chapter" data-level="8.2.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#marginal-model-plots"><i class="fa fa-check"></i><b>8.2.2</b> Marginal model plots</a></li>
<li class="chapter" data-level="8.2.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#residual-plots"><i class="fa fa-check"></i><b>8.2.3</b> Residual plots</a></li>
<li class="chapter" data-level="8.2.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#multicollinearity"><i class="fa fa-check"></i><b>8.2.4</b> Multicollinearity</a></li>
<li class="chapter" data-level="8.2.5" data-path="model-diagnostics.html"><a href="model-diagnostics.html#robust-models"><i class="fa fa-check"></i><b>8.2.5</b> Robust Models</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#other-topics"><i class="fa fa-check"></i><b>8.3</b> Other Topics</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html"><i class="fa fa-check"></i><b>9</b> Model Comparison and Regularization</a><ul>
<li class="chapter" data-level="9.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>9.1</b> Overfitting and Underfitting</a></li>
<li class="chapter" data-level="9.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>9.2</b> Kullback-Leibler Divergence</a></li>
<li class="chapter" data-level="9.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#information-criteria"><i class="fa fa-check"></i><b>9.3</b> Information Criteria</a><ul>
<li class="chapter" data-level="9.3.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#experiment-on-deviance"><i class="fa fa-check"></i><b>9.3.1</b> Experiment on Deviance</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#information-criteria-1"><i class="fa fa-check"></i><b>9.4</b> Information Criteria</a><ul>
<li class="chapter" data-level="9.4.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#akaike-information-criteria-aic"><i class="fa fa-check"></i><b>9.4.1</b> Akaike Information Criteria (AIC)</a></li>
<li class="chapter" data-level="9.4.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#deviance-information-criteria-dic"><i class="fa fa-check"></i><b>9.4.2</b> Deviance Information Criteria (DIC)</a></li>
<li class="chapter" data-level="9.4.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#watanabe-akaike-information-criteria-waic"><i class="fa fa-check"></i><b>9.4.3</b> Watanabe-Akaike Information Criteria (WAIC)</a></li>
<li class="chapter" data-level="9.4.4" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>9.4.4</b> Leave-One-Out Cross Validation</a></li>
<li class="chapter" data-level="9.4.5" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#example"><i class="fa fa-check"></i><b>9.4.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#stackingmodel-averaging"><i class="fa fa-check"></i><b>9.5</b> Stacking/Model Averaging</a><ul>
<li class="chapter" data-level="9.5.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#model-weights"><i class="fa fa-check"></i><b>9.5.1</b> Model Weights</a></li>
<li class="chapter" data-level="9.5.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#model-averaging"><i class="fa fa-check"></i><b>9.5.2</b> Model Averaging</a></li>
<li class="chapter" data-level="9.5.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#stacking"><i class="fa fa-check"></i><b>9.5.3</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#shrinkage-priors"><i class="fa fa-check"></i><b>9.6</b> Shrinkage Priors</a><ul>
<li class="chapter" data-level="9.6.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#number-of-parameters"><i class="fa fa-check"></i><b>9.6.1</b> Number of parameters</a></li>
<li class="chapter" data-level="9.6.2" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#sparsity-inducing-priors"><i class="fa fa-check"></i><b>9.6.2</b> Sparsity-Inducing Priors</a></li>
<li class="chapter" data-level="9.6.3" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#finnish-horseshoe"><i class="fa fa-check"></i><b>9.6.3</b> Finnish Horseshoe</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#variable-selection"><i class="fa fa-check"></i><b>9.7</b> Variable Selection</a><ul>
<li class="chapter" data-level="9.7.1" data-path="model-comparison-and-regularization.html"><a href="model-comparison-and-regularization.html#projection-based-method"><i class="fa fa-check"></i><b>9.7.1</b> Projection-Based Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html"><i class="fa fa-check"></i><b>10</b> Hierarchical &amp; Multilevel Models</a><ul>
<li class="chapter" data-level="10.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#anova"><i class="fa fa-check"></i><b>10.1</b> ANOVA</a><ul>
<li class="chapter" data-level="10.1.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#frequentist-anova"><i class="fa fa-check"></i><b>10.1.1</b> “Frequentist” ANOVA</a></li>
<li class="chapter" data-level="10.1.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#bayesian-anova"><i class="fa fa-check"></i><b>10.1.2</b> Bayesian ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#multilevel-modeling-mlm"><i class="fa fa-check"></i><b>10.2</b> Multilevel Modeling (MLM)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#examples-of-clustering"><i class="fa fa-check"></i><b>10.2.1</b> Examples of clustering</a></li>
<li class="chapter" data-level="10.2.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#data-1"><i class="fa fa-check"></i><b>10.2.2</b> Data</a></li>
<li class="chapter" data-level="10.2.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#intraclass-correlation"><i class="fa fa-check"></i><b>10.2.3</b> Intraclass correlation</a></li>
<li class="chapter" data-level="10.2.4" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#is-mlm-needed"><i class="fa fa-check"></i><b>10.2.4</b> Is MLM needed?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-coefficients"><i class="fa fa-check"></i><b>10.3</b> Varying Coefficients</a><ul>
<li class="chapter" data-level="10.3.1" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-intercepts"><i class="fa fa-check"></i><b>10.3.1</b> Varying Intercepts</a></li>
<li class="chapter" data-level="10.3.2" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-slopes"><i class="fa fa-check"></i><b>10.3.2</b> Varying Slopes</a></li>
<li class="chapter" data-level="10.3.3" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#varying-sigma"><i class="fa fa-check"></i><b>10.3.3</b> Varying <span class="math inline">\(\sigma\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="hierarchical-multilevel-models.html"><a href="hierarchical-multilevel-models.html#model-comparisons"><i class="fa fa-check"></i><b>10.4</b> Model Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>11</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="11.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#basics-of-generalized-linear-models"><i class="fa fa-check"></i><b>11.1</b> Basics of Generalized Linear Models</a></li>
<li class="chapter" data-level="11.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>11.2</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#the-logit-link"><i class="fa fa-check"></i><b>11.2.1</b> The logit link</a></li>
<li class="chapter" data-level="11.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#choice-of-priors"><i class="fa fa-check"></i><b>11.2.2</b> Choice of Priors</a></li>
<li class="chapter" data-level="11.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interpreting-the-coefficients"><i class="fa fa-check"></i><b>11.2.3</b> Interpreting the coefficients</a></li>
<li class="chapter" data-level="11.2.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#model-checking-1"><i class="fa fa-check"></i><b>11.2.4</b> Model Checking</a></li>
<li class="chapter" data-level="11.2.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#complete-separation"><i class="fa fa-check"></i><b>11.2.5</b> Complete Separation</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binomial-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Binomial Logistic Regression</a></li>
<li class="chapter" data-level="11.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#probit-regression"><i class="fa fa-check"></i><b>11.4</b> Probit Regression</a></li>
<li class="chapter" data-level="11.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>11.5</b> Poisson Regression</a><ul>
<li class="chapter" data-level="11.5.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interpretations-2"><i class="fa fa-check"></i><b>11.5.1</b> Interpretations</a></li>
<li class="chapter" data-level="11.5.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#model-checking-2"><i class="fa fa-check"></i><b>11.5.2</b> Model Checking</a></li>
<li class="chapter" data-level="11.5.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#other-models-in-glm"><i class="fa fa-check"></i><b>11.5.3</b> Other models in GLM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="missing-data.html"><a href="missing-data.html"><i class="fa fa-check"></i><b>12</b> Missing Data</a><ul>
<li class="chapter" data-level="12.1" data-path="missing-data.html"><a href="missing-data.html#missing-data-mechanisms"><i class="fa fa-check"></i><b>12.1</b> Missing Data Mechanisms</a><ul>
<li class="chapter" data-level="12.1.1" data-path="missing-data.html"><a href="missing-data.html#mcar-missing-completely-at-random"><i class="fa fa-check"></i><b>12.1.1</b> MCAR (Missing Completely at Random)</a></li>
<li class="chapter" data-level="12.1.2" data-path="missing-data.html"><a href="missing-data.html#mar-missing-at-random"><i class="fa fa-check"></i><b>12.1.2</b> MAR (Missing At Random)</a></li>
<li class="chapter" data-level="12.1.3" data-path="missing-data.html"><a href="missing-data.html#nmar-not-missing-at-random"><i class="fa fa-check"></i><b>12.1.3</b> NMAR (Not Missing At Random)</a></li>
<li class="chapter" data-level="12.1.4" data-path="missing-data.html"><a href="missing-data.html#ignorable-missingness"><i class="fa fa-check"></i><b>12.1.4</b> Ignorable Missingness*</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="missing-data.html"><a href="missing-data.html#bayesian-approaches-for-missing-data"><i class="fa fa-check"></i><b>12.2</b> Bayesian Approaches for Missing Data</a><ul>
<li class="chapter" data-level="12.2.1" data-path="missing-data.html"><a href="missing-data.html#complete-case-analysislistwise-deletion"><i class="fa fa-check"></i><b>12.2.1</b> Complete Case Analysis/Listwise Deletion</a></li>
<li class="chapter" data-level="12.2.2" data-path="missing-data.html"><a href="missing-data.html#treat-missing-data-as-parameters"><i class="fa fa-check"></i><b>12.2.2</b> Treat Missing Data as Parameters</a></li>
<li class="chapter" data-level="12.2.3" data-path="missing-data.html"><a href="missing-data.html#multiple-imputation"><i class="fa fa-check"></i><b>12.2.3</b> Multiple Imputation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Handouts for Bayesian Data Analysis Class</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="missing-data" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Missing Data</h1>
<p>Missing data are common in many research problems. Sometimes missing data arise
from design, but more often data are missing for reasons that are beyond
researchers’ control. I will first provide some conceptual discussion on
the types of missing data, and then talk about the Bayesian approach for
handling missing data by treating missing data as parameters with some prior
information. I will then give a brief introduction of <em>multiple imputation</em> and
its Bayesian origin. A regression with missing data problem will be used to
illustrate two Bayesian approaches to handle missing data.</p>
<div id="missing-data-mechanisms" class="section level2">
<h2><span class="header-section-number">12.1</span> Missing Data Mechanisms</h2>
<p>To simplify the discussion, assume that missing values are only present in the
outcome <span class="math inline">\(Y\)</span> in a hypothetical regression problem of using people’s age (<span class="math inline">\(X\)</span>) to
predict their voting intention (<span class="math inline">\(Y\)</span>).</p>
<p>Let <span class="math inline">\(R\)</span> be an indicator variable that denotes whether
<span class="math inline">\(Y\)</span> is missing (<span class="math inline">\(R = 0\)</span>) or not (<span class="math inline">\(R = 1\)</span>). For example, if <span class="math inline">\(Y\)</span> looks like</p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb438-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb438-2" data-line-number="2">N &lt;-<span class="st"> </span><span class="dv">30</span></a>
<a class="sourceLine" id="cb438-3" data-line-number="3">x &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw">sample</span>(<span class="dv">18</span><span class="op">:</span><span class="dv">80</span>, N, <span class="dt">replace =</span> <span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb438-4" data-line-number="4">y &lt;-<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>x <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">sd =</span> <span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb438-5" data-line-number="5">y &lt;-<span class="st"> </span><span class="kw">round</span>(y <span class="op">*</span><span class="st"> </span><span class="fl">1.4</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb438-6" data-line-number="6"><span class="kw">ifelse</span>(y[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span>, <span class="ot">NA</span>, y[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</a></code></pre></div>
<pre><code>&gt;#  [1] 2.4 2.7 2.4 2.0 2.0 3.9 3.2 2.3 1.7  NA</code></pre>
<p>then <span class="math inline">\(R\)</span> will be</p>
<pre><code>&gt;#  [1] 1 1 1 1 1 1 1 1 1 0</code></pre>
<p>Assume our data look like the first scatter plot below if there are no missing
data:</p>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb441-1" data-line-number="1">demo_data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">y =</span> y, <span class="dt">x =</span> x, </a>
<a class="sourceLine" id="cb441-2" data-line-number="2">                    <span class="dt">r_mcar =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="ot">TRUE</span>, <span class="ot">FALSE</span>, <span class="ot">TRUE</span>), <span class="dv">10</span>), </a>
<a class="sourceLine" id="cb441-3" data-line-number="3">                    <span class="dt">r_mar =</span> <span class="dv">1</span><span class="op">:</span>N <span class="op">&gt;=</span><span class="st"> </span><span class="dv">11</span>, </a>
<a class="sourceLine" id="cb441-4" data-line-number="4">                    <span class="dt">r_nmar =</span> <span class="kw">rank</span>(<span class="fl">0.2</span> <span class="op">*</span><span class="st"> </span>x <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(x) <span class="op">+</span><span class="st"> </span><span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span>y <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(y)) <span class="op">&gt;=</span><span class="st"> </span><span class="dv">11</span>)</a></code></pre></div>
<div class="figure"><span id="fig:mechanism-plots"></span>
<img src="12_missing_data_files/figure-html/mechanism-plots-1.png" alt="Scatter plots for different types of missing data" width="624" />
<p class="caption">
Figure 12.1: Scatter plots for different types of missing data
</p>
</div>
<p>Missing data can be related to the predictor <span class="math inline">\(X\)</span> (e.g., older people are more
likely to give a missing response), the outcome <span class="math inline">\(Y\)</span> itself (e.g., people with
lower voting intention are less likely to respond), and some other unmeasured
factors that relate to neither <span class="math inline">\(X\)</span> nor <span class="math inline">\(Y\)</span>, which I summarize as <span class="math inline">\(Z\)</span>. Depending
on what causes missing data, the three missing data algorithms are <code>MCAR</code>
(missing completely at random), <code>MAR</code> (missing at random), and <code>NMAR</code> (not
missing at random), as summarized in the figures below, which I will further
explain.</p>
<!-- ![Three missing data mechanisms (left: `MCAR`; middle: `MAR`; right: -->
<!-- `NMAR`) ](figures/missing_data_dag.png) -->
<div id="mcar-missing-completely-at-random" class="section level3">
<h3><span class="header-section-number">12.1.1</span> MCAR (Missing Completely at Random)</h3>
<div id="htmlwidget-748157f64609652d122e" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-748157f64609652d122e">{"x":{"diagram":"\ndigraph mcar {\n\n  # a \"graph\" statement\n  graph [overlap = true, fontsize = 10]\n\n  node [shape = plaintext]\n  Y; X; \n  Ystar [label = \"Y*\"]\n  RY [label = <R<FONT POINT-SIZE=\"8\"><SUB>Y<\/SUB><\/FONT>>]\n\n  # paths\n  X -> Ystar;\n  Ystar -> Y;\n  RY -> Y;\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p><code>MCAR</code> means that the probability of a missing response (denoted as <span class="math inline">\(R\)</span>) is
unrelated to anything of interest in the research question. For example, for the
left graph in Figure 2, <span class="math inline">\(Z\)</span> maybe some haphazard events such as interviewers
accidentally erase responses for some people, which we believe to be unrelated
to participants’ ages or voting intentions. The plot on the top right panel of
Figure 1 is an example, with the missing cases being grayed out.</p>
<p>One quick-and-dirty method to check for MCAR is to check whether the
distribution of <span class="math inline">\(X\)</span> is similar for cases with or without missing data on <span class="math inline">\(Y\)</span>,
and as you can see in the above graph the means and variances of <span class="math inline">\(X\)</span> for the
group with missing data and for the group without are highly similar. This
method can be generalized to data with missing data on multiple variables, and
one can check whether missing data on every variable affect all other variables.</p>
<p>As you can see, the regression line barely changes with or without the missing
data.</p>
<blockquote>
<p>In general, under <code>MCAR</code>, using only cases with no missing value still give
valid inferences and unbiased estimations. However, for more complex models
complete case analyses (also called listwise deletion) can greatly reduce the
sample size for analysis, as it throws away information from cases with
partial information.</p>
</blockquote>
</div>
<div id="mar-missing-at-random" class="section level3">
<h3><span class="header-section-number">12.1.2</span> MAR (Missing At Random)</h3>
<div id="htmlwidget-a08941f42e8ad93d0338" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-a08941f42e8ad93d0338">{"x":{"diagram":"\ndigraph mar {\n\n  # a \"graph\" statement\n  graph [overlap = true, fontsize = 10]\n\n  node [shape = plaintext]\n  Y; X; \n  Ystar [label = \"Y*\"]\n  RY [label = <R<FONT POINT-SIZE=\"8\"><SUB>Y<\/SUB><\/FONT>>]\n\n  # paths\n  X -> Ystar;\n  X -> RY;\n  Ystar -> Y;\n  RY -> Y;\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>It’s probably not the most intuitive naming, but <code>MAR</code> refers to the condition
that the probability of a missing observation (<span class="math inline">\(R\)</span>) can be explained by the
observed data (i.e., <span class="math inline">\(X\)</span> in this case). In other words, missing data does not
relate to the values that would have been observed (which is denoted as
<span class="math inline">\(Y_\textrm{mis}\)</span>), once we considered the observed data. For example, for the
middle graph in Figure 2, some missing data on voting intentions can be
explained by some random factor <span class="math inline">\(Z\)</span>, but for some cases data are missing
because, for instance, younger people tend to be less motivated to complete the
survey. The plot on the bottom left panel of Figure 1 is an example, with the
missing cases being grayed out.</p>
<p>As can be seen, when data are MAR, the distributions of <span class="math inline">\(X\)</span> are different for
groups with and without missing <span class="math inline">\(Y\)</span> values. Also, the distributions of the
observed <span class="math inline">\(Y\)</span> values differ systematically from the complete data.</p>
<blockquote>
<p>Under <code>MAR</code>, using only the cases without missing values still produces an
unbiased estimate of the regression coefficient, if missing data is only present
in <span class="math inline">\(Y\)</span>. However, for more complex models and with missing data in <span class="math inline">\(X\)</span>, more
advanced methods generally give more accurate coefficient estimates and standard
errors.</p>
</blockquote>
</div>
<div id="nmar-not-missing-at-random" class="section level3">
<h3><span class="header-section-number">12.1.3</span> NMAR (Not Missing At Random)</h3>
<div id="htmlwidget-0f7405d6c60f62578b83" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-0f7405d6c60f62578b83">{"x":{"diagram":"\ndigraph nmar {\n\n  # a \"graph\" statement\n  graph [overlap = true, fontsize = 10]\n\n  node [shape = plaintext]\n  Y; X; \n  Ystar [label = \"Y*\"]\n  RY [label = <R<FONT POINT-SIZE=\"8\"><SUB>Y<\/SUB><\/FONT>>]\n\n  # paths\n  X -> Ystar;\n  X -> RY;\n  Ystar -> Y;\n  Ystar -> RY;\n  RY -> Y;\n  {rank = same; Ystar; RY;}\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p><code>NMAR</code> is sometimes called <em>missing not at random</em> or <em>non-ignorable
missingness</em>, and as the name suggested it refers to conditions where MAR does
not hold. In other words, NMAR happens when, after considering all the observed
data, the probability of a missing value (<span class="math inline">\(R\)</span>) still depends on the value of <span class="math inline">\(Y\)</span>
that would have been observed. For example, if we consider people in the same
age group and still find those with lower voting intentions tend not to give
their responses, the situation can be described as <code>NMAR</code>. The plot on the
bottom right panel of Figure 1, where people with lowing voting intentions are
more likely to miss.</p>
<p>The example looks very similar to the one for MAR, including the fact that the
distributions of <span class="math inline">\(X\)</span> are different for the group with and without missing <span class="math inline">\(Y\)</span>.
Indeed, there are no statistical procedures that can distinguish between MAR
in general and NMAR. If there are evidence for MCAR then one can be more
confident in ruling out NMAR, and there have been recent efforts to establish
procedures for testing some special cases of MAR. However, for many real data
problems one has to rely on reasoning, judgments, and perhaps some educated
guessing to decide whether the data is MAR or NMAR.</p>
<p>On the other hand, if one has variables that potentially relates to the
probability of missing but are not part of the model of interest (e.g., gender,
SES, etc), these can be included in the imputation model (discussed later) so
that the missing data mechanism better resembles MAR. Including these
<em>auxiliary</em> variables is equivalent to changing them from unmeasured to
measured, and generally can weaken the associations between the unobserved <span class="math inline">\(Y\)</span>
and <span class="math inline">\(R\)</span>, thus making the estimates less biased.</p>
<blockquote>
<p>With <code>NMAR</code>, valid statistical inferences can only be
obtained by correctly modeling the mechanism for the missing data. Including
variables that help explain probability of missing data makes <code>MAR</code> more
reasonable.</p>
</blockquote>
</div>
<div id="ignorable-missingness" class="section level3">
<h3><span class="header-section-number">12.1.4</span> Ignorable Missingness*</h3>
<p>Let <span class="math inline">\(Y_\textrm{obs}\)</span> be the part of the multivariate data <span class="math inline">\(Y\)</span> that is
observed (i.e., not missing), and <span class="math inline">\(Y_\textrm{mis}\)</span> be the part that would
have been observed. The likelihood now concerns both <span class="math inline">\(Y_\textrm{obs}\)</span> and
<span class="math inline">\(R\)</span>, that is, <span class="math inline">\(P(Y_\textrm{obs}, R)\)</span>. Let <span class="math inline">\(\boldsymbol{\mathbf{\phi}}\)</span> be the set of
parameters that determine the probability of missing in addition to the observed
data, which can be written as <span class="math inline">\(P(R | Y_\textrm{obs}, \boldsymbol{\mathbf{\phi}})\)</span>. Note it is
assumed that <span class="math inline">\(\boldsymbol{\mathbf{\phi}}\)</span> is distinct from the model parameters <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span>.</p>
<p>For a case <span class="math inline">\(i\)</span> with <span class="math inline">\(r_i = 1\)</span>, the joint likelihood of <span class="math inline">\((x_i, y_i, r_i = 1)\)</span> is</p>
<p><span class="math display">\[P(x_i, y_{\textrm{obs}, i}, r_i = 1; \boldsymbol{\mathbf{\theta}}, \boldsymbol{\mathbf{\phi}}) 
  = P(r_i = 1 | x_i, y_{\textrm{obs}, i}; \boldsymbol{\mathbf{\phi}}) 
    P(y_{\textrm{obs}, i} | x_i; \boldsymbol{\mathbf{\theta}}) 
    P(x_i).\]</span></p>
<p>For a case with <span class="math inline">\(r_i = 0\)</span>, <span class="math inline">\(y_i\)</span> is missing. Assume first we know the
missing value <span class="math inline">\(y_{\textrm{mis}, i}\)</span>, and the complete likelihood <span class="math inline">\((x_i, y_{\textrm{mis}, i}, r_i = 0)\)</span> is</p>
<p><span class="math display">\[P(x_i, y_{\textrm{mis}, i}, r_i = 0; \boldsymbol{\mathbf{\theta}}, \boldsymbol{\mathbf{\phi}}) 
  = P(r_i = 0 | x_i, y_{\textrm{mis}, i}; \boldsymbol{\mathbf{\phi}}) 
    P(y_{\textrm{mis}, i} | x_i; \boldsymbol{\mathbf{\theta}}) 
    P(x_i)\]</span></p>
<p>But because <span class="math inline">\(y\)</span> is missing, we need to integrate out the missing value to
obtain the observed likelihood of <span class="math inline">\((x_i, r_i = 0)\)</span></p>
<p><span class="math display">\[\begin{align*}
  P(x_i, r_i = 0; \boldsymbol{\mathbf{\theta}}, \boldsymbol{\mathbf{\phi}}) 
  &amp; = \int P(r_i = 0 | x_i, y_{\textrm{mis}, i}; \boldsymbol{\mathbf{\phi}}) 
           P(y_{\textrm{mis}, i} | x_i; \boldsymbol{\mathbf{\theta}}) 
           P(x_i) \; \mathrm{d}y_{\textrm{mis}, i} \\
  &amp; = P(x_i) \int P(r_i = 0 | x_i, y_{\textrm{mis}, i}; \boldsymbol{\mathbf{\phi}}) 
           P(y_{\textrm{mis}, i} | x_i; \boldsymbol{\mathbf{\theta}}) 
           \; \mathrm{d}y_{\textrm{mis}, i} 
\end{align*}\]</span></p>
<p>Because the likelihood depends on <span class="math inline">\(R\)</span> and cannot be separated from <span class="math inline">\(\boldsymbol{\mathbf{\phi}}\)</span>,
correct inference on <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span> can be obtained only by correct modeling the
missing data mechanism.</p>
<div id="if-mcar-holds" class="section level4">
<h4><span class="header-section-number">12.1.4.1</span> If MCAR Holds</h4>
<p>However, if the condition for MCAR is satisfied such that</p>
<p><span class="math display">\[P(r_i = 0 | x_i, y_{\textrm{mis}, i}; \boldsymbol{\mathbf{\phi}}) = P(r_i = 0; \boldsymbol{\mathbf{\phi}}),\]</span></p>
<p>that is, <span class="math inline">\(R\)</span> is related to neither <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> Then the observed likelihood is</p>
<p><span class="math display">\[\begin{align*}
  P(x_i, r_i = 0; \boldsymbol{\mathbf{\theta}}, \boldsymbol{\mathbf{\phi}}) 
  &amp; = P(x_i) \int P(r_i = 0; \boldsymbol{\mathbf{\phi}}) 
           P(y_{\textrm{mis}, i} | x_i; \boldsymbol{\mathbf{\theta}}) 
           \; \mathrm{d}y_{\textrm{mis}, i} \\
  &amp; = P(x_i) P(r_i = 0; \boldsymbol{\mathbf{\phi}}) \times
      \int P(y_{\textrm{mis}, i} | x_i; \boldsymbol{\mathbf{\theta}}) 
           \; \mathrm{d}y_{\textrm{mis}, i} \\
  &amp; = P(x_i) P(r_i = 0; \boldsymbol{\mathbf{\phi}})
\end{align*}\]</span></p>
<p>So inference of <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span> does not depend on the missing data mechanism
<span class="math inline">\(P(r_i = 0; \boldsymbol{\mathbf{\phi}})\)</span>, and missingness is <em>ignorable</em>.</p>
</div>
<div id="if-mar-holds" class="section level4">
<h4><span class="header-section-number">12.1.4.2</span> If MAR Holds</h4>
<p>Similarly, if the condition for MAR is satisfied such that</p>
<p><span class="math display">\[P(r_i = 0 | x_i, y_{\textrm{mis}, i}; \boldsymbol{\mathbf{\phi}}) 
  = P(r_i = 0 | x_i, ; \boldsymbol{\mathbf{\phi}}),\]</span></p>
<p>that is, <span class="math inline">\(R\)</span> is not related to <span class="math inline">\(Y\)</span> after taking into account <span class="math inline">\(X\)</span>. Then the
observed likelihood is</p>
<p><span class="math display">\[\begin{align*}
  P(x_i, r_i = 0; \boldsymbol{\mathbf{\theta}}, \boldsymbol{\mathbf{\phi}}) 
  &amp; = P(x_i) \int P(r_i = 0 | x_i; \boldsymbol{\mathbf{\phi}}) 
           P(y_{\textrm{mis}, i} | x_i; \boldsymbol{\mathbf{\theta}}) 
           \; \mathrm{d}y_{\textrm{mis}, i}  \\
  &amp; = P(x_i) P(r_i = 0 | x_i; \boldsymbol{\mathbf{\phi}}) \times
      \int P(y_{\textrm{mis}, i} | x_i; \boldsymbol{\mathbf{\theta}}) 
           \; \mathrm{d}y_{\textrm{mis}, i} \\
  &amp; = P(x_i) P(r_i = 0 | x_i; \boldsymbol{\mathbf{\phi}})
\end{align*}\]</span></p>
<p>So inference of <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span> does not depend on the missing data mechanism
<span class="math inline">\(P(r_i = 0 | x_i; \boldsymbol{\mathbf{\phi}})\)</span>, and missingness is ignorable.</p>
<p>On the other hand, if <span class="math inline">\(r_i\)</span> depends on <span class="math inline">\(y_\textrm{mis}\)</span> (i.e., NMAR) so that
<span class="math inline">\(P(r_i = 0 | x_i, y_{\textrm{mis}, i}; \boldsymbol{\mathbf{\phi}})\)</span> cannot be written outside of
the integral, inference of <span class="math inline">\(\boldsymbol{\mathbf{\theta}}\)</span> depends on the missing data mechanism,
so missingness is non-ignorable.</p>
<p>The discussion generalizes to missing data on multiple variables.</p>
<hr />
</div>
</div>
</div>
<div id="bayesian-approaches-for-missing-data" class="section level2">
<h2><span class="header-section-number">12.2</span> Bayesian Approaches for Missing Data</h2>
<p>We will be using the <code>kidiq</code> data set we discussed in Chapter 7. I’ll do the
same rescaling and coding <code>mom_hs</code> as a factor variable:</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" data-line-number="1">kidiq &lt;-<span class="st"> </span>haven<span class="op">::</span><span class="kw">read_dta</span>(<span class="st">&quot;../data/kidiq.dta&quot;</span>)</a>
<a class="sourceLine" id="cb442-2" data-line-number="2">kidiq100 &lt;-<span class="st"> </span>kidiq <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb442-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mom_iq =</span> mom_iq <span class="op">/</span><span class="st"> </span><span class="dv">100</span>,  <span class="co"># divid mom_iq by 100</span></a>
<a class="sourceLine" id="cb442-4" data-line-number="4">         <span class="dt">kid_score =</span> kid_score <span class="op">/</span><span class="st"> </span><span class="dv">100</span>,   <span class="co"># divide kid_score by 100</span></a>
<a class="sourceLine" id="cb442-5" data-line-number="5">         <span class="dt">mom_iq_c =</span> mom_iq <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb442-6" data-line-number="6">         <span class="dt">mom_hs =</span> <span class="kw">factor</span>(mom_hs, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>))) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb442-7" data-line-number="7"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="st"> </span>mom_iq)</a></code></pre></div>
<p>In R, the package <code>mice</code> can be used to perform <em>multiple imputation</em> (to be
discussed soon), as well as to create missing data. First, let’s generate some
missing completely at random (MCAR) data by randomly removing up to 50% of
the data:</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb443-1" data-line-number="1"><span class="kw">library</span>(mice)</a>
<a class="sourceLine" id="cb443-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1955</span>)</a>
<a class="sourceLine" id="cb443-3" data-line-number="3">kidiq100_mcar &lt;-<span class="st"> </span><span class="kw">ampute</span>(kidiq100, <span class="dt">prop =</span> <span class="fl">0.5</span>, </a>
<a class="sourceLine" id="cb443-4" data-line-number="4">                        <span class="dt">pattern =</span> <span class="kw">data.frame</span>(<span class="dt">kid_score =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), </a>
<a class="sourceLine" id="cb443-5" data-line-number="5">                                             <span class="dt">mom_hs =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>), </a>
<a class="sourceLine" id="cb443-6" data-line-number="6">                                             <span class="dt">mom_work =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), </a>
<a class="sourceLine" id="cb443-7" data-line-number="7">                                             <span class="dt">mom_age =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), </a>
<a class="sourceLine" id="cb443-8" data-line-number="8">                                             <span class="dt">mom_iq_c =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)), </a>
<a class="sourceLine" id="cb443-9" data-line-number="9">                        <span class="dt">freq =</span> <span class="kw">c</span>(.<span class="dv">2</span>, <span class="fl">.4</span>, <span class="fl">.4</span>), </a>
<a class="sourceLine" id="cb443-10" data-line-number="10">                        <span class="dt">mech =</span> <span class="st">&quot;MCAR&quot;</span>)</a>
<a class="sourceLine" id="cb443-11" data-line-number="11">kidiq100_mcar &lt;-<span class="st"> </span>kidiq100_mcar<span class="op">$</span>amp</a></code></pre></div>
<p>The second time, I’ll generate some missing at random (MAR) data:</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1955</span>)</a>
<a class="sourceLine" id="cb444-2" data-line-number="2">kidiq100_mar &lt;-<span class="st"> </span><span class="kw">ampute</span>(kidiq100, <span class="dt">prop =</span> <span class="fl">0.5</span>, </a>
<a class="sourceLine" id="cb444-3" data-line-number="3">                       <span class="dt">pattern =</span> <span class="kw">data.frame</span>(<span class="dt">kid_score =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), </a>
<a class="sourceLine" id="cb444-4" data-line-number="4">                                            <span class="dt">mom_hs =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>), </a>
<a class="sourceLine" id="cb444-5" data-line-number="5">                                            <span class="dt">mom_work =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), </a>
<a class="sourceLine" id="cb444-6" data-line-number="6">                                            <span class="dt">mom_age =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), </a>
<a class="sourceLine" id="cb444-7" data-line-number="7">                                            <span class="dt">mom_iq_c =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)), </a>
<a class="sourceLine" id="cb444-8" data-line-number="8">                       <span class="dt">freq =</span> <span class="kw">c</span>(.<span class="dv">2</span>, <span class="fl">.4</span>, <span class="fl">.4</span>), </a>
<a class="sourceLine" id="cb444-9" data-line-number="9">                       <span class="dt">mech =</span> <span class="st">&quot;MAR&quot;</span>)</a></code></pre></div>
<pre><code>&gt;# Warning: Data is made numeric because the calculation of weights requires
&gt;# numeric data</code></pre>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb446-1" data-line-number="1">kidiq100_mar &lt;-<span class="st"> </span>kidiq100_mar<span class="op">$</span>amp</a></code></pre></div>
<p>And finally, some not missing at random (NMAR) data:</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb447-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1955</span>)</a>
<a class="sourceLine" id="cb447-2" data-line-number="2">kidiq100_nmar &lt;-<span class="st"> </span><span class="kw">ampute</span>(kidiq100, <span class="dt">prop =</span> <span class="fl">0.5</span>, </a>
<a class="sourceLine" id="cb447-3" data-line-number="3">                        <span class="dt">pattern =</span> <span class="kw">data.frame</span>(<span class="dt">kid_score =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), </a>
<a class="sourceLine" id="cb447-4" data-line-number="4">                                             <span class="dt">mom_hs =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>), </a>
<a class="sourceLine" id="cb447-5" data-line-number="5">                                             <span class="dt">mom_work =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), </a>
<a class="sourceLine" id="cb447-6" data-line-number="6">                                             <span class="dt">mom_age =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), </a>
<a class="sourceLine" id="cb447-7" data-line-number="7">                                             <span class="dt">mom_iq_c =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)), </a>
<a class="sourceLine" id="cb447-8" data-line-number="8">                        <span class="dt">freq =</span> <span class="kw">c</span>(.<span class="dv">2</span>, <span class="fl">.4</span>, <span class="fl">.4</span>), </a>
<a class="sourceLine" id="cb447-9" data-line-number="9">                        <span class="co"># mice call it MNAR</span></a>
<a class="sourceLine" id="cb447-10" data-line-number="10">                        <span class="dt">mech =</span> <span class="st">&quot;MNAR&quot;</span>)</a></code></pre></div>
<pre><code>&gt;# Warning: Data is made numeric because the calculation of weights requires
&gt;# numeric data</code></pre>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb449-1" data-line-number="1">kidiq100_nmar &lt;-<span class="st"> </span>kidiq100_nmar<span class="op">$</span>amp</a></code></pre></div>
<p>Let’s check the distributions of the resulting data:</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb450-1" data-line-number="1">p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(kidiq100_mcar, <span class="kw">aes</span>(<span class="dt">x =</span> mom_iq_c, <span class="dt">y =</span> kid_score)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb450-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> kidiq100, <span class="dt">color =</span> <span class="st">&quot;grey92&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb450-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb450-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_smooth</span>()</a>
<a class="sourceLine" id="cb450-5" data-line-number="5">p2 &lt;-<span class="st"> </span>p1 <span class="op">%+%</span><span class="st"> </span>kidiq100_mar</a>
<a class="sourceLine" id="cb450-6" data-line-number="6">p3 &lt;-<span class="st"> </span>p1 <span class="op">%+%</span><span class="st"> </span>kidiq100_nmar</a>
<a class="sourceLine" id="cb450-7" data-line-number="7"><span class="kw">grid.arrange</span>(p1, p2, p3, <span class="dt">nrow =</span> <span class="dv">2</span>)</a></code></pre></div>
<pre><code>&gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<pre><code>&gt;# Warning: Removed 230 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>&gt;# Warning: Removed 230 rows containing missing values (geom_point).</code></pre>
<pre><code>&gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<pre><code>&gt;# Warning: Removed 215 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>&gt;# Warning: Removed 215 rows containing missing values (geom_point).</code></pre>
<pre><code>&gt;# `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<pre><code>&gt;# Warning: Removed 239 rows containing non-finite values (stat_smooth).</code></pre>
<pre><code>&gt;# Warning: Removed 239 rows containing missing values (geom_point).</code></pre>
<p><img src="12_missing_data_files/figure-html/kidiq100-missing-scatter-1.png" width="720" /></p>
<p>When eyeballing it doesn’t appear that the data are very different, but the
regression slopes are affected by the different missing data mechanisms. We’ll
look at the simple regression model of using <code>mom_iq_c</code> to predict <code>kid_score</code>,
using the MAR data set. In that data set, the missingness of <code>kid_score</code>
actually depends on both <code>mom_iq_c</code> and <code>mom_hs</code>, but when the regression does
not include <code>mom_hs</code> in the model, the resulting situation will actually be
NMAR.</p>
<p>The missing data pattern of the <code>kidiq100_mar</code> data set is:</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb460-1" data-line-number="1"><span class="co"># Recode mom_hs to factor</span></a>
<a class="sourceLine" id="cb460-2" data-line-number="2">kidiq100_mar<span class="op">$</span>mom_hs &lt;-<span class="st"> </span><span class="kw">factor</span>(kidiq100_mar<span class="op">$</span>mom_hs, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>))</a>
<a class="sourceLine" id="cb460-3" data-line-number="3"><span class="kw">md.pattern</span>(kidiq100_mar, <span class="dt">rotate.names =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p><img src="12_missing_data_files/figure-html/md-pattern-mar-1.png" width="480" /></p>
<pre><code>&gt;#     mom_work mom_age mom_hs mom_iq_c kid_score    
&gt;# 219        1       1      1        1         1   0
&gt;# 49         1       1      1        1         0   1
&gt;# 94         1       1      1        0         0   2
&gt;# 72         1       1      0        1         0   2
&gt;#            0       0     72       94       215 381</code></pre>
<p>Which shows that only 219 observations had full data, and most were missing the
<code>kid_score</code> variable.</p>
<div id="complete-case-analysislistwise-deletion" class="section level3">
<h3><span class="header-section-number">12.2.1</span> Complete Case Analysis/Listwise Deletion</h3>
<p>By default, <code>brms</code> uses only cases with no missing data. For example,</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" data-line-number="1">m3_ld &lt;-<span class="st"> </span><span class="kw">brm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_iq_c <span class="op">+</span><span class="st"> </span>mom_hs, <span class="dt">data =</span> kidiq100_mar, </a>
<a class="sourceLine" id="cb462-2" data-line-number="2">             <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>), </a>
<a class="sourceLine" id="cb462-3" data-line-number="3">                       <span class="co"># set for all &quot;b&quot; coefficients</span></a>
<a class="sourceLine" id="cb462-4" data-line-number="4">                       <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</a>
<a class="sourceLine" id="cb462-5" data-line-number="5">                       <span class="kw">prior</span>(<span class="kw">student_t</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;sigma&quot;</span>)), </a>
<a class="sourceLine" id="cb462-6" data-line-number="6">             <span class="dt">seed =</span> <span class="dv">2302</span></a>
<a class="sourceLine" id="cb462-7" data-line-number="7">)</a></code></pre></div>
<pre><code>&gt;# Warning: Rows containing NAs were excluded from the model.</code></pre>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" data-line-number="1">m3_ld</a></code></pre></div>
<pre><code>&gt;#  Family: gaussian 
&gt;#   Links: mu = identity; sigma = identity 
&gt;# Formula: kid_score ~ mom_iq_c + mom_hs 
&gt;#    Data: kidiq100_mar (Number of observations: 219) 
&gt;# Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
&gt;#          total post-warmup samples = 4000
&gt;# 
&gt;# Population-Level Effects: 
&gt;#           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
&gt;# Intercept     0.81      0.03     0.76     0.86 1.00     3395     3055
&gt;# mom_iq_c      0.71      0.11     0.50     0.92 1.00     3518     2641
&gt;# mom_hsyes     0.07      0.03     0.01     0.13 1.00     3583     3274
&gt;# 
&gt;# Family Specific Parameters: 
&gt;#       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
&gt;# sigma     0.20      0.01     0.18     0.22 1.00     3632     2916
&gt;# 
&gt;# Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
&gt;# is a crude measure of effective sample size, and Rhat is the potential 
&gt;# scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Notice that the number of observations is only 219. As previously
explained, this analysis is only valid when data are missing completely at
random or missing at random (i.e., missingness of the outcome only depends
on <code>mom_iq_c</code> and factors unrelated to <code>Ozone</code>).</p>
<p>If you recall in Chapter 7, the coefficient using the full data should be:</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" data-line-number="1">m3 &lt;-<span class="st"> </span><span class="kw">brm</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_iq_c <span class="op">+</span><span class="st"> </span>mom_hs, <span class="dt">data =</span> kidiq100, </a>
<a class="sourceLine" id="cb466-2" data-line-number="2">          <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>), </a>
<a class="sourceLine" id="cb466-3" data-line-number="3">                    <span class="co"># set for all &quot;b&quot; coefficients</span></a>
<a class="sourceLine" id="cb466-4" data-line-number="4">                    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</a>
<a class="sourceLine" id="cb466-5" data-line-number="5">                    <span class="kw">prior</span>(<span class="kw">student_t</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;sigma&quot;</span>)), </a>
<a class="sourceLine" id="cb466-6" data-line-number="6">          <span class="dt">seed =</span> <span class="dv">1955</span></a>
<a class="sourceLine" id="cb466-7" data-line-number="7">)</a></code></pre></div>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" data-line-number="1">m3</a></code></pre></div>
<pre><code>&gt;#  Family: gaussian 
&gt;#   Links: mu = identity; sigma = identity 
&gt;# Formula: kid_score ~ mom_iq_c + mom_hs 
&gt;#    Data: kidiq100 (Number of observations: 434) 
&gt;# Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
&gt;#          total post-warmup samples = 4000
&gt;# 
&gt;# Population-Level Effects: 
&gt;#           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
&gt;# Intercept     0.82      0.02     0.78     0.86 1.00     3840     3093
&gt;# mom_iq_c      0.56      0.06     0.44     0.68 1.00     3672     2883
&gt;# mom_hsyes     0.06      0.02     0.02     0.10 1.00     3761     3213
&gt;# 
&gt;# Family Specific Parameters: 
&gt;#       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
&gt;# sigma     0.18      0.01     0.17     0.19 1.00     3899     3136
&gt;# 
&gt;# Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
&gt;# is a crude measure of effective sample size, and Rhat is the potential 
&gt;# scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>So the listwise approach overestimated the regression coefficient. We can do
better.</p>
<p>Now, take a look on whether missingness in <code>kid_score</code> is related to other
variables.</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb469-1" data-line-number="1"><span class="co"># Compute the missingness indicator (you can use the `within` function too)</span></a>
<a class="sourceLine" id="cb469-2" data-line-number="2">kidiq100_mar_R &lt;-<span class="st"> </span><span class="kw">transform</span>(kidiq100_mar, </a>
<a class="sourceLine" id="cb469-3" data-line-number="3">                            <span class="dt">kid_score_R =</span> <span class="kw">factor</span>(<span class="kw">as.numeric</span>(<span class="op">!</span><span class="kw">is.na</span>(kid_score)), </a>
<a class="sourceLine" id="cb469-4" data-line-number="4">                                                 <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Missing&quot;</span>, </a>
<a class="sourceLine" id="cb469-5" data-line-number="5">                                                            <span class="st">&quot;Observed&quot;</span>)))</a></code></pre></div>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" data-line-number="1"><span class="co"># Plot distributions of variables against missingness indicator</span></a>
<a class="sourceLine" id="cb470-2" data-line-number="2"><span class="kw">qplot</span>(kid_score_R, mom_iq_c, <span class="dt">data =</span> kidiq100_mar_R, <span class="dt">geom =</span> <span class="st">&quot;boxplot&quot;</span>)</a></code></pre></div>
<pre><code>&gt;# Warning: Removed 94 rows containing non-finite values (stat_boxplot).</code></pre>
<p><img src="12_missing_data_files/figure-html/missing_dist1-1.png" width="672" /></p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb472-1" data-line-number="1"><span class="kw">ggplot</span>(<span class="dt">data =</span> kidiq100_mar_R, <span class="kw">aes</span>(<span class="dt">x =</span> kid_score_R)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb472-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_bar</span>(<span class="kw">aes</span>(<span class="dt">fill =</span> <span class="kw">factor</span>(mom_hs)), <span class="dt">position =</span> <span class="kw">position_dodge</span>())</a></code></pre></div>
<p><img src="12_missing_data_files/figure-html/missing_dist1-2.png" width="672" /></p>
<p>As we already knew, missingness of <code>kid_score</code> is related to both <code>mom_iq_c</code>
and <code>mom_hs</code>, in that those with higher <code>mom_iq_c</code> and those whose mother had
high school degree were more likely to be missing.</p>
</div>
<div id="treat-missing-data-as-parameters" class="section level3">
<h3><span class="header-section-number">12.2.2</span> Treat Missing Data as Parameters</h3>
<p>A fully Bayesian approach to handle missing data is to treat the missing
<code>kid_score</code> values just as parameters, and assign priors to them. When the
missing data mechanism is ignorable (MCAR or MAR), we can assume that the
missing and observed <code>kid_score</code> values are exchangeable, conditioning on the
predictors (i.e., whether <code>kid_score</code> is missing or not does not add information
to the <code>kid_score</code> values). Therefore, if <code>kid_score</code> is missing, we use the
likelihood as the prior for the missing values:</p>

<p><span class="math display">\[\begin{align*}
  \mathtt{kid_score}_{\textrm{obs}, i}&amp; \sim \mathcal{N}(\beta_0 + \beta_1 \mathtt{mom_iq_c}_i, \sigma) \\
  \mathtt{kid_score}_{\textrm{mis}, i}&amp; \sim \mathcal{N}(\beta_0 + \beta_1 \mathtt{mom_iq_c}_i, \sigma) \\
  \beta_0 &amp; \sim \mathcal{N}(0, 1) \\
  \beta_1 &amp; \sim \mathcal{N}(0, 1) \\
  \beta_2 &amp; \sim \mathcal{N}(0, 1)
\end{align*}\]</span></p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" data-line-number="1"><span class="kw">library</span>(rstan)</a>
<a class="sourceLine" id="cb473-2" data-line-number="2"><span class="kw">rstan_options</span>(<span class="dt">auto_write =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre class="stan"><code>data {
  int&lt;lower=0&gt; N;  // number of observations
  vector[N] y;  // response variable (including missing values);
  int&lt;lower=0, upper=1&gt; y_obs[N];  // missingness indicator for Y 
  int&lt;lower=0&gt; p;  // number of predictor variables (exclude intercept)
  matrix[N, p] X;  // predictor variable matrix
}
transformed data {
  int n_obs = sum(y_obs);  // number of observed cases
  int ns[n_obs];  // indices of observed cases
  int ny = 1; 
  for (n in 1:N) {
    if (y_obs[n]) {
      ns[ny] = n;
      ny += 1;
    }
  }
}
parameters {
  real beta_0;  // intercept
  vector[2] beta;  // 2 slopes
  real&lt;lower=0&gt; sigma;  // error standard deviation
}
model {
  // likelihood for observed Y
  y[ns] ~ normal_id_glm(X[ns, ], beta_0, beta, sigma);
  // prior
  beta_0 ~ normal(0, 1);
  beta ~ normal(0, 1);
  sigma ~ student_t(4, 0, 1);
}
generated quantities {
  real yrep[N];  // simulated data based on model
  vector[N] yhat = beta_0 + X * beta;  // used to compute R-squared effect size
  for (i in 1:N) {
    yrep[i] = normal_rng(yhat[i], sigma);
  }
}</code></pre>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb475-1" data-line-number="1"><span class="co"># Data with no missing X</span></a>
<a class="sourceLine" id="cb475-2" data-line-number="2">kidiq100_mar_obsX &lt;-<span class="st"> </span><span class="kw">drop_na</span>(kidiq100_mar, mom_iq_c, mom_hs)</a></code></pre></div>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb476-1" data-line-number="1">m3_stan &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="st">&quot;../codes/normal_regression_missing.stan&quot;</span>, </a>
<a class="sourceLine" id="cb476-2" data-line-number="2">                <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> <span class="kw">nrow</span>(kidiq100_mar_obsX), </a>
<a class="sourceLine" id="cb476-3" data-line-number="3">                            <span class="dt">y =</span> <span class="kw">replace_na</span>(kidiq100_mar_obsX<span class="op">$</span>kid_score, <span class="dv">99</span>), </a>
<a class="sourceLine" id="cb476-4" data-line-number="4">                            <span class="dt">y_obs =</span> <span class="kw">as.numeric</span>(</a>
<a class="sourceLine" id="cb476-5" data-line-number="5">                              <span class="op">!</span><span class="kw">is.na</span>(kidiq100_mar_obsX<span class="op">$</span>kid_score)</a>
<a class="sourceLine" id="cb476-6" data-line-number="6">                            ), </a>
<a class="sourceLine" id="cb476-7" data-line-number="7">                            <span class="dt">p =</span> <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb476-8" data-line-number="8">                            <span class="dt">X =</span> <span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>mom_iq_c <span class="op">+</span><span class="st"> </span>mom_hs, </a>
<a class="sourceLine" id="cb476-9" data-line-number="9">                                             <span class="dt">data =</span> kidiq100_mar_obsX)[ , <span class="dv">-1</span>]), </a>
<a class="sourceLine" id="cb476-10" data-line-number="10">                <span class="dt">seed =</span> <span class="dv">1234</span>)</a></code></pre></div>
<p>Note that the results are basically identical to the complete case analyses, and
the posterior distributions of the missing <span class="math inline">\(Y\)</span> values are essentially the
predictive intervals given the <span class="math inline">\(X\)</span> values. For example, for
the first 10 observations with missing <code>kid_score</code> values,</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb477-1" data-line-number="1">draws_ymis &lt;-</a>
<a class="sourceLine" id="cb477-2" data-line-number="2"><span class="st">  </span><span class="kw">as.matrix</span>(m3_stan, <span class="dt">pars =</span> <span class="st">&quot;yrep&quot;</span>)[ , <span class="kw">is.na</span>(kidiq100_mar_obsX<span class="op">$</span>kid_score)]</a>
<a class="sourceLine" id="cb477-3" data-line-number="3"><span class="kw">mcmc_areas_ridges</span>(draws_ymis[ , <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>], <span class="dt">bw =</span> <span class="st">&quot;SJ&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:m3-stan-dens"></span>
<img src="12_missing_data_files/figure-html/m3-stan-dens-1.png" alt="Posterior density plots of the first two missing values of \texttt{kid_score}" width="672" />
<p class="caption">
Figure 12.2: Posterior density plots of the first two missing values of \texttt{kid_score}
</p>
</div>
<p>The posterior distributions of the missing values are highly related to the
missing data handling technique called <em>multiple imputation</em>, which we will
discuss next. Indeed, each posterior sample can be considered an imputed data
set. The posterior draws of the missing values are also called
<em>plausible values</em>.</p>
</div>
<div id="multiple-imputation" class="section level3">
<h3><span class="header-section-number">12.2.3</span> Multiple Imputation</h3>
<p>Multiple imputation is one of the modern techniques for missing data handling,
and is general in that it has a very broad application. It uses the observed
data and the observed associations to predict the missing values, and captures
the uncertainty involved in the predictions by imputing multiple data sets.
That’s a bit abstract, but with your Bayesian knowledge, that just means
<em>getting samples from the posterior distributions of the missing values</em>, and
then substitute them to the missing holes to form an imputed data set. The
difference is that, instead of using all posterior samples, we usually obtain 20
or 30 imputed data sets, which can be saved and used for almost any kind of
analyses, Bayesian or frequentist.</p>
<div id="multiple-imputation-has-several-advantages" class="section level4">
<h4><span class="header-section-number">12.2.3.1</span> Multiple imputation has several advantages</h4>
<ul>
<li>It provides valid results when data is MAR</li>
<li>It reduces biases when data is NMAR by incorporating covariates that help
explain the missing data mechanism (e.g., <code>mom_work</code> and <code>mom_age</code>)</li>
<li>It is very flexible and can impute continuous and categorical variables</li>
</ul>
</div>
<div id="example-of-multiple-imputation" class="section level4">
<h4><span class="header-section-number">12.2.3.2</span> Example of multiple imputation</h4>
<p>Although in theory one can use the Bayesian procedures with <code>Stan</code> to account
for missing data or to do multiple imputations, there are some limitations.
First, when the goal is to impute missing data instead of making inferences on
the model parameters, the algorithm in <code>Stan</code> may not be as efficient as
specialized programs for multiple imputation. Second, the Hamiltonian Monte
Carlo sampler in <code>Stan</code> requires the use of derivatives, so it is not (yet)
well-equipped to handle categorical parameters. Thus, it is hard or not possible
to handle categorical missing data. Third, when the number of variables with
missing data is large, it is tedious to specify the missing data mechanism for
all variables.</p>
<p>Instead, as <span class="citation">Gelman et al. (<a href="#ref-Gelman2013">2013</a>)</span> recommended, we can handle missing data using a
<em>two-step process</em>:</p>
<ol style="list-style-type: decimal">
<li>Do multiple imputation using a specialized program</li>
<li>Use <code>brms</code> or <code>rstan</code> (or other Bayesian methods) to analyze each imputed
data set</li>
</ol>
</div>
<div id="r-packages-for-multiple-imputation" class="section level4">
<h4><span class="header-section-number">12.2.3.3</span> R packages for multiple imputation</h4>
<p>There are several packages in R for multiple imputation (e.g., <code>Amelia</code>, <code>jomo</code>,
<code>mi</code>, <code>mice</code>, <code>missForest</code>, <code>norm</code>, <code>pan</code>). Although these packages differ in
terms of their underlying algorithms, my experience and also evidence from the
literature suggested that they usually gave similar performance for continuous
missing data, but several packages have specialized functionality for specific
models and data types (e.g., categorical missing data, multilevel data).</p>
<p>I will illustrate the use of <code>mice</code> below. I strongly encourage you to take a
look on the vignettes found on the website of the package:
<a href="https://github.com/stefvanbuuren/mice" class="uri">https://github.com/stefvanbuuren/mice</a>. Also, the author of the package has a
nice book on multiple imputation <span class="citation">(Van Buuren <a href="#ref-vanburren2018">2018</a>)</span>, which is freely available at
<a href="https://stefvanbuuren.name/fimd/" class="uri">https://stefvanbuuren.name/fimd/</a> and I encourage you to read if you are
interested. Note that the example discussed here is simple so not much fine
tuning for the imputation is needed. For your own analyses multiple imputation
can be complex, and you should consult statisticians or other resources to set
up a reasonable imputation model.</p>
<p>Let’s continue with the <code>kidiq</code> example. We can use the whole data set for
imputation.</p>
<blockquote>
<p>In general it’s recommended to include covariates that have even minor
associations with the probability of missing. The bias introduced by ignoring an
important covariate usually is higher than the bias introduced by including a
inappropriate covariate. However, see <span class="citation">Thoemmes and Rose (<a href="#ref-thoemmes2014cautious">2014</a>)</span> for a cautionary
note.</p>
</blockquote>
<blockquote>
<p>In planning a study, if high missing rate on a variable is anticipated, one
can collect covariates that can help explain the missing data mechanism. This
helps recover missing information in the analyses.</p>
</blockquote>
<div id="setting-up-and-run-the-imputation" class="section level5">
<h5><span class="header-section-number">12.2.3.3.1</span> 1. Setting up and run the imputation</h5>
<p>With binary and continuous missing variables, it can be as simple as running
the following:</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb478-1" data-line-number="1"><span class="kw">library</span>(mice)</a>
<a class="sourceLine" id="cb478-2" data-line-number="2"><span class="co"># Using mice to impute 20 data sets</span></a>
<a class="sourceLine" id="cb478-3" data-line-number="3">kidiq100_imp &lt;-<span class="st"> </span><span class="kw">mice</span>(kidiq100_mar, <span class="dt">m =</span> <span class="dv">20</span>, <span class="dt">maxit =</span> <span class="dv">35</span>,</a>
<a class="sourceLine" id="cb478-4" data-line-number="4">                     <span class="dt">printFlag =</span> <span class="ot">FALSE</span>)  <span class="co"># set to false only for knitting to Rmd</span></a></code></pre></div>
<p>Of course this oversimplifies the complexity of multiple imputation. By default
it uses the method called “predictive mean matching” to replace missing data
with a randomly chosen value from several similar cases (see <a href="https://stefvanbuuren.name/fimd/sec-pmm.html" class="uri">https://stefvanbuuren.name/fimd/sec-pmm.html</a>). Things will get more complicated
when you have more variables and complex data types.</p>
<p>Typing <code>kidiq100_imp$imp</code> will show the imputed missing values. Check <code>?mice</code>
for more information. The <code>complete</code> function fills the missing values to the
missing holes to form data sets with no missing data.</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb479-1" data-line-number="1">kidiq100_imp1 &lt;-<span class="st"> </span><span class="kw">complete</span>(kidiq100_imp, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb479-2" data-line-number="2"><span class="kw">head</span>(kidiq100_imp1, <span class="dv">10</span>)</a></code></pre></div>
<pre><code>&gt;#    kid_score mom_hs mom_work mom_age mom_iq_c
&gt;# 1       1.21    yes        4      27  -0.1883
&gt;# 2       1.26    yes        4      25   0.2754
&gt;# 3       0.85    yes        4      27   0.1544
&gt;# 4       0.41    yes        3      25  -0.1785
&gt;# 5       0.42    yes        4      27  -0.0725
&gt;# 6       0.98     no        1      18   0.0790
&gt;# 7       0.69    yes        4      20   0.3889
&gt;# 8       1.16    yes        3      23   0.2515
&gt;# 9       1.02    yes        1      24  -0.1838
&gt;# 10      0.95    yes        1      19  -0.0493</code></pre>
<p>Compared to the original data:</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb481-1" data-line-number="1"><span class="kw">head</span>(kidiq100, <span class="dv">10</span>)</a></code></pre></div>
<pre><code>&gt;# # A tibble: 10 x 5
&gt;#    kid_score mom_hs mom_work mom_age mom_iq_c
&gt;#        &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
&gt;#  1      0.65 yes           4      27  0.211  
&gt;#  2      0.98 yes           4      25 -0.106  
&gt;#  3      0.85 yes           4      27  0.154  
&gt;#  4      0.83 yes           3      25 -0.00550
&gt;#  5      1.15 yes           4      27 -0.0725 
&gt;#  6      0.98 no            1      18  0.0790 
&gt;#  7      0.69 yes           4      20  0.389  
&gt;#  8      1.06 yes           3      23  0.251  
&gt;#  9      1.02 yes           1      24 -0.184  
&gt;# 10      0.95 yes           1      19 -0.0493</code></pre>
</div>
<div id="check-for-convergence" class="section level5">
<h5><span class="header-section-number">12.2.3.3.2</span> 2. Check for Convergence</h5>
<p>We should also look at convergence:</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb483-1" data-line-number="1"><span class="kw">plot</span>(kidiq100_imp)</a></code></pre></div>
<p><img src="12_missing_data_files/figure-html/plot-kidiq100_imp-1.png" width="624" /></p>
<p>These are basically Markov chains in regular Bayesian analyses. So if you see
some chains are constantly above or below others then it’s problematic.</p>
<p>See <a href="https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html" class="uri">https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html</a>
for additional steps to check for convergence.</p>
</div>
<div id="run-brm_multiple-on-imputed-data-sets" class="section level5">
<h5><span class="header-section-number">12.2.3.3.3</span> 3. Run <code>brm_multiple</code> on imputed data sets</h5>
<p><code>brms</code> directly supports multiply imputed data sets. Simply use the
<code>brm_multiple</code> function and supply the multiply imputed data object to it. Also,
for computational efficiency using two chains for each imputed data set would
be faster.</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb484-1" data-line-number="1">m3_imp &lt;-<span class="st"> </span><span class="kw">brm_multiple</span>(kid_score <span class="op">~</span><span class="st"> </span>mom_iq_c <span class="op">+</span><span class="st"> </span>mom_hs, <span class="dt">data =</span> kidiq100_imp, </a>
<a class="sourceLine" id="cb484-2" data-line-number="2">                       <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;Intercept&quot;</span>), </a>
<a class="sourceLine" id="cb484-3" data-line-number="3">                                 <span class="co"># set for all &quot;b&quot; coefficients</span></a>
<a class="sourceLine" id="cb484-4" data-line-number="4">                                 <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</a>
<a class="sourceLine" id="cb484-5" data-line-number="5">                                 <span class="kw">prior</span>(<span class="kw">student_t</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> <span class="st">&quot;sigma&quot;</span>)), </a>
<a class="sourceLine" id="cb484-6" data-line-number="6">                       <span class="dt">seed =</span> <span class="dv">1955</span>, </a>
<a class="sourceLine" id="cb484-7" data-line-number="7">                       <span class="dt">chains =</span> 2L, </a>
<a class="sourceLine" id="cb484-8" data-line-number="8">                       <span class="dt">cores =</span> 2L</a>
<a class="sourceLine" id="cb484-9" data-line-number="9">)</a></code></pre></div>
<p>See this vignette:
<a href="https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html#compatibility-with-other-multiple-imputation-packages" class="uri">https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html#compatibility-with-other-multiple-imputation-packages</a> for more information. If you look at the results:</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb485-1" data-line-number="1">m3_imp</a></code></pre></div>
<pre><code>&gt;# Warning: The model has not converged (some Rhats are &gt; 1.05). Do not analyse the results! 
&gt;# We recommend running more iterations and/or setting stronger priors.</code></pre>
<pre><code>&gt;#  Family: gaussian 
&gt;#   Links: mu = identity; sigma = identity 
&gt;# Formula: kid_score ~ mom_iq_c + mom_hs 
&gt;#    Data: kidiq100_imp (Number of observations: 434) 
&gt;# Samples: 40 chains, each with iter = 2000; warmup = 1000; thin = 1;
&gt;#          total post-warmup samples = 40000
&gt;# 
&gt;# Population-Level Effects: 
&gt;#           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
&gt;# Intercept     0.80      0.02     0.76     0.85 1.15      166      768
&gt;# mom_iq_c      0.67      0.10     0.48     0.85 1.39       84      329
&gt;# mom_hsyes     0.07      0.03     0.02     0.13 1.18      143      665
&gt;# 
&gt;# Family Specific Parameters: 
&gt;#       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
&gt;# sigma     0.19      0.01     0.18     0.21 1.20      137      256
&gt;# 
&gt;# Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
&gt;# is a crude measure of effective sample size, and Rhat is the potential 
&gt;# scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>You will see that there are 40 chains in the results. The <code>Rhat</code> value will
be much higher than 1, as the chains are from different data sets and will never
converge. Instead, you should investigate the <code>Rhat</code> for each data set by</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb488-1" data-line-number="1">m3_imp<span class="op">$</span>rhats</a></code></pre></div>
<pre><code>&gt;#    b_Intercept b_mom_iq_c b_mom_hsyes sigma  lp__
&gt;# 1        1.001      1.002       1.002 0.999 1.000
&gt;# 2        1.000      1.003       1.001 1.000 1.000
&gt;# 3        0.999      1.000       0.999 1.000 1.001
&gt;# 4        1.001      1.000       1.001 1.000 1.001
&gt;# 5        0.999      0.999       1.000 0.999 0.999
&gt;# 6        0.999      1.000       0.999 1.000 1.000
&gt;# 7        0.999      1.002       1.000 1.003 1.000
&gt;# 8        0.999      0.999       0.999 1.000 0.999
&gt;# 9        1.000      1.000       0.999 1.000 1.003
&gt;# 10       0.999      0.999       0.999 0.999 1.001
&gt;# 11       1.002      1.000       1.001 1.001 1.000
&gt;# 12       1.001      1.000       1.000 1.001 1.000
&gt;# 13       0.999      1.000       0.999 0.999 1.001
&gt;# 14       1.000      1.003       1.001 1.000 0.999
&gt;# 15       1.000      1.000       0.999 1.000 1.006
&gt;# 16       0.999      0.999       0.999 1.001 1.002
&gt;# 17       1.000      1.000       0.999 1.001 1.003
&gt;# 18       1.000      1.000       1.000 1.001 1.000
&gt;# 19       1.000      1.000       0.999 1.001 1.002
&gt;# 20       1.000      1.001       1.000 1.001 1.003</code></pre>
<p>So the chains have converged for each individual data set.</p>
<p>Now, put the results together:</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb490-1" data-line-number="1"><span class="kw">source</span>(<span class="st">&quot;../codes/extract_brmsfit.R&quot;</span>)</a>
<a class="sourceLine" id="cb490-2" data-line-number="2">texreg<span class="op">::</span><span class="kw">screenreg</span>(<span class="kw">map</span>(<span class="kw">list</span>(m3, m3_ld, m3_imp), </a>
<a class="sourceLine" id="cb490-3" data-line-number="3">                      extract_brmsfit, </a>
<a class="sourceLine" id="cb490-4" data-line-number="4">                      <span class="co"># LOO-IC and WAIC not meaningful for comparing different</span></a>
<a class="sourceLine" id="cb490-5" data-line-number="5">                      <span class="co"># data</span></a>
<a class="sourceLine" id="cb490-6" data-line-number="6">                      <span class="dt">include.loo.ic =</span> <span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb490-7" data-line-number="7">                      <span class="dt">include.waic =</span> <span class="ot">FALSE</span>), </a>
<a class="sourceLine" id="cb490-8" data-line-number="8">                  <span class="dt">custom.model.names =</span> <span class="kw">c</span>(<span class="st">&quot;Full data&quot;</span>, </a>
<a class="sourceLine" id="cb490-9" data-line-number="9">                                         <span class="st">&quot;Complete case&quot;</span>, </a>
<a class="sourceLine" id="cb490-10" data-line-number="10">                                         <span class="st">&quot;MI&quot;</span>))</a></code></pre></div>
<pre><code>&gt;# Warning: The model has not converged (some Rhats are &gt; 1.05). Do not analyse the results! 
&gt;# We recommend running more iterations and/or setting stronger priors.</code></pre>
<pre><code>&gt;# Warning: Using only the first imputed data set. Please interpret the results
&gt;# with caution until a more principled approach has been implemented.</code></pre>
<pre><code>&gt;# 
&gt;# ====================================================
&gt;#            Full data     Complete case  MI          
&gt;# ----------------------------------------------------
&gt;# Intercept    0.82 *        0.81 *         0.80 *    
&gt;#            [0.78; 0.86]  [0.76; 0.86]   [0.75; 0.85]
&gt;# mom_iq_c     0.56 *        0.71 *         0.67 *    
&gt;#            [0.43; 0.67]  [0.48; 0.90]   [0.48; 0.85]
&gt;# mom_hsyes    0.06 *        0.07 *         0.07 *    
&gt;#            [0.02; 0.10]  [0.01; 0.13]   [0.02; 0.13]
&gt;# ----------------------------------------------------
&gt;# R^2          0.21          0.22           0.22      
&gt;# Num. obs.  434           219            434         
&gt;# ====================================================
&gt;# * 0 outside the confidence interval</code></pre>
<p>You can see that the coefficients for <code>mom_iq_c</code> is closer to the original data
with multiple imputation, and the credible intervals are slightly shorter than
complete case analyses.</p>
<p>For data with more variables, choices of missing data handling method can make a
substantial difference. Therefore, researchers need to be thoughtful in choosing
imputation models that best reflect the missing data mechanism.</p>
<p>Missing data is an active research area, and this note only covers a very small
fraction of the issues discussed in the literature.</p>

</div>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Gelman2013">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald Rubin. 2013. <em>Bayesian Data Analysis</em>. 3rd ed. London, UK: CRC Press.</p>
</div>
<div id="ref-thoemmes2014cautious">
<p>Thoemmes, Felix, and Norman Rose. 2014. “A Cautious Note on Auxiliary Variables That Can Increase Bias in Missing Data Problems.” <em>Multivariate Behavioral Research</em> 49 (5): 443–59.</p>
</div>
<div id="ref-vanburren2018">
<p>Van Buuren, Stef. 2018. <em>Flexible Imputation of Missing Data</em>. 2nd ed. Boca Raton, FL: CRC Press. <a href="https://stefvanbuuren.name/fimd/">https://stefvanbuuren.name/fimd/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="generalized-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["notes_bookdown.pdf", "notes_bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
